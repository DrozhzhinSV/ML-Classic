{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Краткий обзор ансамблевых методов\n",
    "\n",
    "На сегодняшнем занятии, мы продолжим изучать [*ансамблевые методы*](https://en.wikipedia.org/wiki/Ensemble_learning) ([здесь](https://neurohive.io/ru/osnovy-data-science/ansamblevye-metody-begging-busting-i-steking/) можно ознакомиться с хорошим обзором). Напомню, что данные методы основаны на гипотезе о том, что объединение нескольких алгоритмов может привести к созданию одной более сильной модели. Алгоритмы, из которых будет состоять итоговая модель, принято называть *базовыми моделями (алгоритмами)*. \n",
    "\n",
    "В большинстве случаев, базовые алгоритмы входящие в композицию являются одинаковыми (например, каждый базовый алгоритм [*случайного леса*](https://ru.wikipedia.org/wiki/Random_forest) - это [*решающее дерево*](https://en.wikipedia.org/wiki/Decision_tree)). Но это не всегда так. Существуют методы, базовые алгоритмы которых имеют абсолютно разную структуру.\n",
    "\n",
    "Можно выделить 3 основных типа ансамблевых методов:\n",
    "\n",
    "1. [*Бэггинг*](https://en.wikipedia.org/wiki/Bootstrap_aggregating). Частным случаем *бэггинга* является *случайный лес*. *Бэггинг*, как правило, состоит из одинаковых базовых алгоритмов, которые строятся параллельно и независимо друг от друга, а итоговый результат является усреднением результатов всех алгоритмов\n",
    "\n",
    "$$\n",
    "a(\\mathbf{x}) = \\frac{1}{N}\\sum\\limits_{i = 1}^{N}a_{i}(\\mathbf{x}) \\text{ - в случае задачи регрессии},\n",
    "$$\n",
    "\n",
    "$$\n",
    "a(\\mathbf{x}) = \\text{sgn}\\Bigg(\\frac{1}{N}\\sum\\limits_{i = 1}^{N}a_{i}(\\mathbf{x})\\Bigg) \\text{ - в случае задачи классификации}.\n",
    "$$ \n",
    "\n",
    "Такие подходы в основном направлены на уменьшение *разброса* модели:\n",
    "\n",
    "$$\n",
    "\\mathbf{\\varepsilon}^{*} = \\underbrace{\\Big(a - \\mathbb{E}a^{*}\\Big)^{2}}_{\\text{Квадрат смещения}} + \\underbrace{\\mathbb{D}a^{*}}_{\\text{Разброс}} + \\underbrace{\\sigma^{2}}_{\\text{Шум}}, \n",
    "$$\n",
    "\n",
    "где *смещение* - это отклонение, усредненного прогноза рассматриваемой модели (по различным обучающим выборкам) от прогноза идеальной модели;  *разброс* - показывает насколько разными могут получаться предсказания, если обучать алгоритм на разных обучающих выборках; *шум* - характеристика данных, которая будет наблюдаться даже на идеальной модели. \n",
    "\n",
    "\n",
    "2. [*Бустинг*](https://en.wikipedia.org/wiki/Boosting_(machine_learning)). Данный подход так же характеризуется одинаковыми базовыми алгоритмами. Но, в отличие от *бэггинга*, *бустинг* представляет собой направленное построение, в котором модели строятся последовательно и каждая следующая направлена на уменьшение ошибки уже построенной композиции. Наиболее распространенной реализацией *бустинга* является [*градиентный бустинг*](https://en.wikipedia.org/wiki/Gradient_boosting), который на каждой итерации шагает в сторону антиградиента ошибки, полученной по уже построенной композиции алгоритмов. \n",
    "\n",
    "Если основная цель *бэггинга* уменьшить *разброс*, то *бустинг* направлен на получение модели с более низким *смещением*.   \n",
    "\n",
    "\n",
    "3. [*Стекинг*](https://dyakonov.org/2017/03/10/c%D1%82%D0%B5%D0%BA%D0%B8%D0%BD%D0%B3-stacking-%D0%B8-%D0%B1%D0%BB%D0%B5%D0%BD%D0%B4%D0%B8%D0%BD%D0%B3-blending/). В данном случае, базовые алгоритмы, как правило, различны. Самая простая реализация *стэкинга* следующая:\n",
    "\n",
    "\n",
    "   - Выборка делится на 3 части;\n",
    "   - На первой части обучаются базовые алгоритмы;\n",
    "   - На второй части делается прогноз каждого обученного на первой части базового алгоритма;\n",
    "   - На полученных прогнозах обучается финальная модель (данная модель не является композицией базовых алгоритмов. Это независимый от них алгоритм, который использует их предсказания в качестве фичей.);\n",
    "   - На третьей части тестируется качество финальной модели.\n",
    "\n",
    "Такой подход к реализации стекинга называется *блендингом*. Очевидный недостаток *блендинга* - разделение выборки на части для обучения (имееются ввиду первые 2 части). Для повышения качества финальной модели можно построить несколько *блендингов* на разных разбиениях и усреднить результаты базовых алгоритмов, полученные на этих разбиениях. \n",
    "\n",
    "*Блендинг* - это одноуровневая реализация *стэкинга*. Существуют более сложные подходы к реализации - многоуровневые. Например, в случае двухуровнего *стэкинга*, можно взять *блэндинг* от *блэндинга*:\n",
    "\n",
    "   - Выборка делится на 4 части;\n",
    "   - На первой части обучаются базовые алгоритмы;\n",
    "   - На второй части делается прогноз каждого обученного на первой части базового алгоритма;\n",
    "   - На полученных прогнозах обучаются другие базовые алгоритмы;\n",
    "   - На третьей части делается прогноз каждого обученного на второй части базового алгоритма;\n",
    "   - На полученных на третьей части прогнозах обучается финальная модель;\n",
    "   - На четвертой части тестируется качество финальной модели.\n",
    "   \n",
    "   \n",
    "   \n",
    "На прошлом занятии Вам уже рассказывали о *бэггинге* и его частном случае - *случайном лесе*. Большая часть этого занятия будет посвящена *градиентному бустингу*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Градиентный бустинг\n",
    "\n",
    "### 1. Постановка задачи в общем виде\n",
    "\n",
    "Пусть у нас имеется обучающая выборка \n",
    "\n",
    "$$\\Big\\{\\mathbf{x}_{j}, y_{j}\\Big\\}_{j = 1}^{n}$$ \n",
    "\n",
    "где $\\mathbf{x}_{j} = \\Big(x_{j}^{1}, \\ldots, x_{j}^{m}\\Big)$ - вектор значений признаков на $j$-ом объекте, а $y_{j}$ - ответ на данном объекте. Через $\\mathbf{x}$ обозначим матрицу, строками которой являются вектора $\\mathbf{x}_{j}$.\n",
    "\n",
    "Будем строить композицию в виде\n",
    "\n",
    "$$\n",
    "a(\\mathbf{x}) = a_{0}(\\mathbf{x}) + \\sum\\limits_{i = 1}^{N}a_{i}(\\mathbf{x}).\n",
    "$$\n",
    "\n",
    "Перед началом построения необходимо задать начальный алгоритм $a_{0}(\\mathbf{x})$. Поскольку построение направленное, алгоритм $a_{0}(\\mathbf{x})$ можно взять сколько угодно простым. Например,\n",
    "\n",
    "$$\n",
    "a_{0}(\\mathbf{x}) = \\frac{1}{n}\\sum\\limits_{j = 1}^{n}y_{j} \\text{ - в случае задачи регрессии}\n",
    "$$\n",
    "\n",
    "или\n",
    "\n",
    "$$\n",
    "a_{0}(\\mathbf{x}) = \\mathbf{I}\\Bigg[\\frac{1}{n}\\sum\\limits_{j = 1}^{n}y_{j} \\ge 0.5\\Bigg] \\text{ - в случае задачи классификации},\n",
    "$$\n",
    "\n",
    "где $\\mathbf{I}$ - индикаторная функция.\n",
    "\n",
    "Пусть построено $(N - 1)$ алгоритмов:\n",
    "\n",
    "$$\n",
    "a^{(N - 1)}(\\mathbf{x}) = a_{0}(\\mathbf{x}) + \\sum\\limits_{i = 1}^{N - 1}a_{i}(\\mathbf{x})\n",
    "$$\n",
    "\n",
    "и функционал ошибки имеет вид:\n",
    "\n",
    "$$\n",
    "Q\\Big(a^{(N - 1)}(\\mathbf{x}), \\mathbf{x}\\Big) = \\frac{1}{n}\\sum\\limits_{j = 1}^{n}L\\Big(y_{j}, a^{(N - 1)}(\\mathbf{x}_{j})\\Big) = \\frac{1}{n}\\sum\\limits_{j = 1}^{n}L\\Bigg(y_{j}, a_{0}(\\mathbf{x}_{j}) + \\sum\\limits_{i = 1}^{N - 1}a_{i}(\\mathbf{x}_{j})\\Bigg).\n",
    "$$\n",
    "\n",
    "Необходимо выбрать алгоритм $a_{N}(\\mathbf{x})$ таким образом, чтобы ошибка была минимальной:\n",
    "\n",
    "\n",
    "$$\n",
    "Q\\Big(a^{(N)}(\\mathbf{x}), \\mathbf{x}\\Big)  = \\frac{1}{n}\\sum\\limits_{j = 1}^{n}L\\Bigg(y_{j}, a_{0}(\\mathbf{x}_{j}) + \\sum\\limits_{i = 1}^{N}a_{i}(\\mathbf{x}_{j})\\Bigg) = \\\\ = \\frac{1}{n}\\sum\\limits_{j = 1}^{n}L\\Bigg(y_{j}, \\Big[a_{0}(\\mathbf{x}_{j}) + \\sum\\limits_{i = 1}^{N - 1}a_{i}(\\mathbf{x}_{j})\\Big] + a_{N}(\\mathbf{x}_{j})\\Bigg) \\to \\min_{a_{N}}.\n",
    "$$\n",
    "\n",
    "\n",
    "Из предыдущих занятий мы помним, что направление наискорейшего убывания функции определяется ее *антиградиентом*:\n",
    "\n",
    "$$\n",
    "\\mathbf{w_{N}} = \\mathbf{w_{N - 1}} - \\nabla Q(\\mathbf{w}_{N - 1}, \\mathbf{x}).\n",
    "$$\n",
    "\n",
    "В нашем случае,\n",
    "\n",
    "$$\n",
    "\\mathbf{w_{N}} = \n",
    "\\begin{bmatrix}\n",
    "    \\Big(a_{0}(\\mathbf{x}_{1}) + \\sum\\limits_{i = 1}^{N - 1}a_{i}(\\mathbf{x}_{1})\\Big) + a_{N}(\\mathbf{x}_{1})\\\\\n",
    "    \\vdots \\\\\n",
    "    \\Big(a_{0}(\\mathbf{x}_{n}) + \\sum\\limits_{i = 1}^{N - 1}a_{i}(\\mathbf{x}_{n})\\Big) + a_{N}(\\mathbf{x}_{n})\\\\\n",
    "\\end{bmatrix} \\quad \\mathbf{w_{N - 1}} = \n",
    "\\begin{bmatrix}\n",
    "    a_{0}(\\mathbf{x}_{1}) + \\sum\\limits_{i = 1}^{N - 1}a_{i}(\\mathbf{x}_{1})\\\\\n",
    "    \\vdots \\\\\n",
    "    a_{0}(\\mathbf{x}_{n}) + \\sum\\limits_{i = 1}^{N - 1}a_{i}(\\mathbf{x}_{n})\\\\\n",
    "\\end{bmatrix}, \\quad \\nabla Q(\\mathbf{w}_{N - 1}, \\mathbf{x}) = \n",
    "\\begin{bmatrix}\n",
    "    L_{z}^{'}(y_{1}, z)\\Bigg|_{z = a_{0}(\\mathbf{x}_{1}) + \\sum\\limits_{i = 1}^{N - 1}a_{i}(\\mathbf{x}_{1})}\\\\\n",
    "    \\vdots \\\\\n",
    "    L_{z}^{'}(y_{n}, z)\\Bigg|_{z = a_{0}(\\mathbf{x}_{n}) + \\sum\\limits_{i = 1}^{N - 1}a_{i}(\\mathbf{x}_{n})}\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "Из этого следует, что\n",
    "\n",
    "$$\n",
    "a_{N}(\\mathbf{x}_{j}) = -L_{z}^{'}(y_{j}, z)\\Bigg|_{z = a_{0}(\\mathbf{x}_{j}) + \\sum\\limits_{i = 1}^{N - 1}a_{i}(\\mathbf{x}_{j})}\n",
    "$$\n",
    "\n",
    "Таким образом, мы получили, что новый алгоритм $a_{N}$ должен принимать на каждом объекте $\\mathbf{x}_{j}$ свое значение. Соответственно, нам необходимо найти функцию $\\tilde{a}(\\mathbf{x})$, которая является наилучшей аппроксимацией функции $a_{N}(\\mathbf{x})$ относительно некоторой функции потерь $\\tilde{L}(y, z)$. А это задача обучения с учителем, в которой в качестве обучающей выборки выступает множество $\\Bigg\\{\\mathbf{x}_{j}, -L_{z}^{'}(y_{j}, z)\\Bigg|_{z = a_{0}(\\mathbf{x}_{j}) + \\sum\\limits_{i = 1}^{N - 1}a_{i}(\\mathbf{x}_{j})}\\Bigg\\}$.\n",
    "\n",
    "Заметим, что выбор функции потерь $\\tilde{L}(y, z)$ не зависит от выбора исходной функции потерь $L(y, z)$, вся информация о которой содержится в векторе антиградиента.    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Градиентный бустинг для задач регрессии и классификации\n",
    "\n",
    "1. *Задача регрессии.*\n",
    "\n",
    "В задаче регрессии в качестве функции потерь, как правило, используется *квадратичная функция* потерь:\n",
    "\n",
    "$$\n",
    "L\\Big(y_{i}, a(\\mathbf{x}_{i})\\Big) = \\Big(y_{i} - a(\\mathbf{x}_{i})\\Big)^{2}.\n",
    "$$\n",
    "\n",
    "В этом случае, вектор антиградиента на $N$ - ой итерации примет вид:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    -L_{z}^{'}(y_{1}, z)\\Bigg|_{z = a_{0}(\\mathbf{x}_{1}) + \\sum\\limits_{i = 1}^{N - 1}a_{i}(\\mathbf{x}_{1})}\\\\\n",
    "    \\vdots \\\\\n",
    "    -L_{z}^{'}(y_{n}, z)\\Bigg|_{z = a_{0}(\\mathbf{x}_{n}) + \\sum\\limits_{i = 1}^{N - 1}a_{i}(\\mathbf{x}_{n})}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "    2\\Bigg(y_{1} - a_{0}(\\mathbf{x}_{1}) - \\sum\\limits_{i = 1}^{N - 1}a_{i}(\\mathbf{x}_{1})\\Bigg)\\\\\n",
    "    \\vdots \\\\\n",
    "    2\\Bigg(y_{n} - a_{0}(\\mathbf{x}_{n}) - \\sum\\limits_{i = 1}^{N - 1}a_{i}(\\mathbf{x}_{n})\\Bigg)\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "    2\\Big(y_{1} - a^{(N - 1)}(\\mathbf{x}_{1})\\Big)\\\\\n",
    "    \\vdots \\\\\n",
    "    2\\Big(y_{n} - a^{(N - 1)}(\\mathbf{x}_{n})\\Big)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "Если же мы имеем дело с *абсолютной функцией* потерь:\n",
    "\n",
    "$$\n",
    "L\\Big(y_{i}, a(\\mathbf{x}_{i})\\Big) = \\Big|y_{i} - a(\\mathbf{x}_{i})\\Big|,\n",
    "$$\n",
    "\n",
    "то вектор антиградиента на $N$ - ой итерации примет вид:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    \\frac{\\Big(y_{1} - a^{(N - 1)}(\\mathbf{x}_{1})\\Big)}{\\Big|y_{1} - a^{(N - 1)}(\\mathbf{x}_{1})\\Big|}\\\\\n",
    "    \\vdots \\\\\n",
    "    \\frac{\\Big(y_{n} - a^{(N - 1)}(\\mathbf{x}_{n})\\Big)}{\\Big|y_{n} - a^{(N - 1)}(\\mathbf{x}_{n})\\Big|}\n",
    "\\end{bmatrix} = \n",
    "\\begin{bmatrix}\n",
    "    \\text{sgn}\\Big(y_{1} - a^{(N - 1)}(\\mathbf{x}_{1})\\Big)\\\\\n",
    "    \\vdots \\\\\n",
    "    \\text{sgn}\\Big(y_{n} - a^{(N - 1)}(\\mathbf{x}_{n})\\Big)\n",
    "\\end{bmatrix}\n",
    "$$\n",
    "\n",
    "\n",
    "2. *Задача классификации.*\n",
    "\n",
    "При решении задачи классификации, в качестве функции потерь обычно используется логистическая функция \n",
    "\n",
    "$$\n",
    "L\\Big(y_{i}, a(\\mathbf{x}_{i})\\Big) = \\ln{\\Big(1 + e^{-y_{i}a(\\mathbf{x}_{i})}\\Big)}.\n",
    "$$\n",
    "\n",
    "В данном случае, вектор антиградиента на $N$ - ой итерации примет вид:\n",
    "\n",
    "$$\n",
    "\\begin{bmatrix}\n",
    "    -L_{z}^{'}(y_{1}, z)\\Bigg|_{z = a^{(N - 1)}(\\mathbf{x}_{1})\\Big)}\\\\\n",
    "    \\vdots \\\\\n",
    "    -L_{z}^{'}(y_{n}, z)\\Bigg|_{z = a^{(N - 1)}(\\mathbf{x}_{n})\\Big)}\n",
    "\\end{bmatrix} =\n",
    "\\begin{bmatrix}\n",
    "    \\frac{y_{1}}{1 + \\text{exp}\\Big(-y_{1}a^{(N - 1)}(\\mathbf{x}_{1})\\Big)}\\\\\n",
    "    \\vdots \\\\\n",
    "    \\frac{y_{n}}{1 + \\text{exp}\\Big(-y_{n}a^{(N - 1)}(\\mathbf{x}_{n})\\Big)}\n",
    "\\end{bmatrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Решающие деревья в качестве базовых алгоритмов градиентного бустинга\n",
    "\n",
    "В градиентном бустинге в качестве базовых алгоритмов обычно используются неглубокие *решающие деревья*. \n",
    "\n",
    "Как мы помним, дерево разбивает пространство объектов на $K$ частей, где $K$ - количество листовых вершин. Все объекты, попавшие в один и тот же лист, получают одинаковый прогноз. Таким образом, математическая формализация *решающего дерева* может выглядеть следующим образом:\n",
    "\n",
    "$$\n",
    "a(\\mathbf{x}_{j}) = \\sum\\limits_{i = 1}^{K}\\mathbf{I}\\Big[\\mathbf{x}_{j} \\in \\text{Leaf}_{i}\\Big] \\cdot a_{i}, \n",
    "$$\n",
    "\n",
    "где $a_{i}$ - значение в $i$-ом листе и\n",
    "\n",
    "$$\n",
    "\\mathbf{I}\\Big[\\mathbf{x}_{j} \\in \\text{Leaf}_{i}\\Big] = \\begin{cases}\n",
    "1, \\text{ если объект } \\mathbf{x}_{j} \\text{ попал в i-ый лист}\\\\\n",
    "0, \\text{ иначе}.\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Тогда очередной базовый алгоритм $a_{N}$ и вся композиция могут быть переписаны в следующем виде:\n",
    "\n",
    "$$\n",
    "a_{N}(\\mathbf{x}_{j}) = \\sum\\limits_{i = 1}^{K_{N}}\\mathbf{I}\\Big[\\mathbf{x}_{j} \\in \\text{Leaf}_{i}\\Big] \\cdot a_{N,i}, \n",
    "$$\n",
    "\n",
    "$$\n",
    "a^{(N)}(\\mathbf{x}_{j}) = a_{0}(\\mathbf{x}_{j}) + \\sum\\limits_{i = 1}^{N - 1}a_{i}(\\mathbf{x}_{j}) + \\sum\\limits_{i = 1}^{K_{N}}\\mathbf{I}\\Big[\\mathbf{x}_{j} \\in \\text{Leaf}_{i}\\Big] \\cdot a_{N, i}.\n",
    "$$\n",
    "\n",
    "И задача оптимизации примет вид:\n",
    "\n",
    "$$\n",
    "\\frac{1}{n}\\sum\\limits_{j = 1}^{n}L\\Bigg(y_{j}, \\Big[a_{0}(\\mathbf{x}_{j}) + \\sum\\limits_{i = 1}^{N - 1}a_{i}(\\mathbf{x}_{j})\\Big] + \\sum\\limits_{i = 1}^{K_{N}}\\mathbf{I}\\Big[\\mathbf{x}_{j} \\in \\text{Leaf}_{i}\\Big] \\cdot a_{N,i}\\Bigg) \\to \\min_{a_{N,1}\\ldots,a_{N, K_{N}}}.\n",
    "$$\n",
    "\n",
    "Поскольку \n",
    "\n",
    "$$\n",
    "\\Big\\{\\mathbf{x}_{j} \\in \\text{Leaf}_{i}\\Big\\} \\cap \\Big\\{\\mathbf{x}_{j} \\in \\text{Leaf}_{m}\\Big\\}  = \\emptyset, \\quad \\forall i, m: i\\neq m,\n",
    "$$\n",
    "\n",
    "то данная задача разбивается на $K_{N}$ независимых подзадач\n",
    "\n",
    "$$\n",
    "\\begin{cases}\n",
    "\\sum\\limits_{\\mathbf{x}_{j} \\in \\text{Leaf}_{1}}L\\Bigg(y_{j}, \\Big[a_{0}(\\mathbf{x}_{j}) + \\sum\\limits_{i = 1}^{N - 1}a_{i}(\\mathbf{x}_{j})\\Big] + a_{N, 1}\\Bigg) &\\to& \\min_{a_{N, 1}},\\\\\n",
    "\\qquad \\qquad \\qquad \\qquad \\ldots \\\\\n",
    "\\sum\\limits_{\\mathbf{x}_{j} \\in \\text{Leaf}_{K_{N}}}L\\Bigg(y_{j}, \\Big[a_{0}(\\mathbf{x}_{j}) + \\sum\\limits_{i = 1}^{N - 1}a_{i}(\\mathbf{x}_{j})\\Big] + a_{N, K_{N}}\\Bigg) &\\to& \\min_{a_{N, K_{N}}}.\\\\\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Решение данной задачи можно найти аналитически."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Задача:** Найти решение в случае задачи регресии и квадратичной функции потерь."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Переобучение градиентного бустинга\n",
    "\n",
    "Посмотрим как меняется ошибка алгоритма в зависимости от количества деревьев"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import datasets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Сгенирируем случайную выборку\n",
    "X, Y = datasets.make_regression(n_samples = 2000, n_features = 20, n_informative = 10, random_state = 911) \n",
    "\n",
    "#Разделим выборку на две части \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn import metrics\n",
    "import numpy as np\n",
    "\n",
    "gb = GradientBoostingRegressor(n_estimators = 500, max_depth = 5, learning_rate = 1, random_state = 123)\n",
    "gb.fit(X_train, Y_train)\n",
    "\n",
    "rmse_test = [np.sqrt(metrics.mean_squared_error(Y_test, predict)) for predict in gb.staged_predict(X_test)]\n",
    "rmse_train = np.sqrt(gb.train_score_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "plt.figure(figsize = (10, 6))\n",
    "\n",
    "plt.plot(rmse_train, linewidth = 2, color = 'green', label = 'GB Train')\n",
    "plt.plot(rmse_test, linewidth = 2, color = 'red', label = 'GB Test')\n",
    "\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.xlabel(\"N\", fontsize = 10)\n",
    "plt.ylabel(\"RMSE\", fontsize = 10)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из графика видно, что с увеличением числа деревьев ошибка стремится к 0. При этом ошибка на тестовой выборке заметно выше. Достигнув минимума, она начинает расти. Таким образом, произошло переобучение модели.\n",
    "\n",
    "Один из способов борьбы с переобучением - двигаться в сторону антиградиента с некоторым шагом:\n",
    "\n",
    "$$\n",
    "a(\\mathbf{x}) = a_{0}(\\mathbf{x}) + \\eta\\sum\\limits_{i = 1}^{N}a_{i}(\\mathbf{x}),\n",
    "$$\n",
    "\n",
    "где $\\eta \\in (0, 1]$ - длина шага.\n",
    "\n",
    "**Вопрос:** Каковы причины переобучения?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABakAAANqCAYAAACHFsfqAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAD8gElEQVR4nOzdd5xddZn48c9zp6Q3SAghoVcBqRFFlC5iWdC1rwUQZF27rnWtu5bVtWH5icuCgoqKdUEFFRGwLC30DqEEEtIgvU/5/v44505uJndm7iQzc+6d+bxfr/ua0+65z5yZ5JnznO95TqSUkCRJkiRJkiSpCKWiA5AkSZIkSZIkjVwWqSVJkiRJkiRJhbFILUmSJEmSJEkqjEVqSZIkSZIkSVJhLFJLkiRJkiRJkgpjkVqSJEmSJEmSVBiL1NouEXFARNwcEbdExB0RcVFEjC06ruEsIpoi4sMR8X8RcVtEvG2A9js+Is6PiEfy/d5a3ndE7BER6/Of8Z35Z+8/EJ/bCCJickS8Y5A/44CIuCEiNkbEBwfzsyQNP+bjoWc+HnpDlI8jIr4ZEXMj4q6IOKLKNhPyn0H59XREnJevOzMillasO2cw45XU2MzfQ8/8PfTqJX/n210XEQ9W5OmdBjMu9Y9Fam2vhcApKaXnpJQOA1YB7ys0ouHvM8A44KSU0hEppf8ZoP1eCCwH9k0pHQGcCuxQsf6RlNJhKaVDgUuAfxugz20Ek4FBTarAMuA9wFcG+XMkDU/m46H3GczHQ20yg5+PXwLsm7/OBc7vvkFKaXX+Mzgs//c2D/hVxSaXVay/cJDjldTYzN9D7zOYv4faZOogf1d4Y0WeXjLIcakfLFJru6SUVqaUVgBERAkYDZTn35ZfEb4zIn5ZviIcERdHxGMRcU9+hevgfPl1ETE7n/5cRKwpf05EfCQi7s739cXeto+I4yMiRcSp+fyU/KrlZ/L5wyLixvyzfx0RU/Ll+0TEn/LPuC0i9o6IS/Ora8vymO+IiLfno2S+3duxiYjREfH9PO7bI+KEfHnXeyNidkRcl083R8TTFd/Db/PpHSJiRWweXftG4IXAzRFxTUTsVnFcX51Pn1/x/VYuPyc/NlO7xbo3cBTwiZRSZ/6zXZpS+lIP395EsgTc/XuOiPhy/rO9OyJely+vehy7vbfye54aEY9XrJsfEePz6f/Nr0rfGxHnVmzTUXE19E/5sqq/g72JiA/l77krIv49X/xFYO9831+O7Cr5Nfnvyd0RcXpf++1LSmlJSukWoG179yVp5DEf98x8bD7up9OBH6TMjcDkiJjRS5z7ATsBfx2Az5Y0wpi/e2b+Nn/3U7/yt+pTc9EBqPFFxBjgBmBX4EGy0aAAvypflYyIzwFnA9/K130opfSLPLmcCNxTsb+dgJMq5l9C9h/Oc1NK6yKi8mrkVtvnbgPeAvwe+Cfgzop1PwDenVK6PiL+A/g02dXqS4EvppR+HRGjgVJK6Y35Z1wM/Dal9It8/swaDs07gZRSenZEHAD8MbITmf76GPBExfyewL+nlC6JiLcC3wReUV4ZEZ/KY/9M5U7y7+ntQLUrhQcBd5YTag/2jog7gAnAWOC5Vbb5R+Aw4FBgKnBLRPylp+O4jd6aUlqW/97dEhG/TCk9A6zPRx9Uqvo7GBGnAbNTSp+q3DgiTiG78noUEMAVEXEs8FHg4PL+I6IZeGVKaVX+B8qNEXFFSil1299lQLXbuL6WUvrBdhwDSdqK+bhH5mPzcX/y8UzgyYr5+fmyhT0ch9eTjZyu/MxX5fE+BLw/pfRk9bdKkvm7F+Zv8/dg5e/vR0QH8Evgc90/V8VxJLW2W0qp/J/ZdLLk9fF81cER8deIuJvsauVBFW/7ckQ8DJwG/LzbLj8JfKFi/mTg+ymldfnnLetje8j+IxqVJ+DTgCsAImISMDmldH2+3SXAsRExAZiZUvp1/hkbyp/Xi9flVwJviYiXV1n/AuBH+f4eILsVtF9JNSJmAs8Dfl2xuBP4cT79w/xzys4kO/6frLK7d5J9v+tr+NyP59/bUxWLy7cn7U32R8gFVd76AuAnKaWOlNJi4HrgOX19Xj+9JyLuBG4k+0Nu3162rfo7mFK6ontCzZ2Sv24n+8PsgB72H8AXIuIu4E9kyW96941SSq+ruI2o8mWBWtKAMx+bjyuYjysMcj5+PfCTivnfAHuklA4Brib7WUtSj8zf5u8K5u8Kg5S/35hSejbZaPoXAm/ejn1pgFmk1oBJKbUDP2Xzf6IXA+/K/wP4d7Jbl8o+lFLaF/iPfF3ZHmRX2H5T48f2tv2PgS+RXY3eVOP++uOy/I+JfwL+exD2D9lV6c8ClVf2Vvey/Q7A+9m6r/FEspOonuK8Dzg0slvMSCl9Pv/eJvaw/RXAsb1GPggi4niyP7KOTlkvr9vZ8vequ4vp+Xew6kcA/1mR/PZJKV1UZbs3AtOAI/PjtLjaviPistjywUrl11v6iEOStpn5eFCYjysM83y8gOykvWxWvmzrICMOBZpTSreWl6WUnkkpbcxnLwSO7ON7lSTA/D0I+wfz9xbM35BSWpB/XU32O35UH9+ThpBFam2XiNg3NvdwCrKrrDfnqycACyOihew/oWpWkd3GUvbp/FXpauCs2NyDa4c+ti/7DXA48L3ygpTSSmB5RLwwX/Rm4Pr8P6j5EfGK/DNGRe1PVV5G9dY5fyX/vvPbknYjS/C12ptsJM4fuy2/hSxBku+/sgfi11JK3wF2yW+1KXs/8K2UUtU/LlJKc4E5wOcioimPeTRZkqnmBcAjVZb/leyKeFNETCNLvDdX2W5bTQKWp+w2tQPIror3ppbfwUp/AN4am/t1zcxvf1ud76syjiUppbbIeqPtXm1njqSWNFTMx4D5uJL5uEI/8/EVwFsi8zxgZUqpp1Yfb2DLUdTElv0vTwPur+H7lTRCmb8B83cl83eFgc7fkfUtn5pPtwAvp6JVjopnT2ptr/HApRHRms9fD/xnPv1J4CZgaf618j+lL0fEJ8iuaJ5TsXx+SukvlR+QUvp9RBwGzImITcCVbH4S7lbbV7xvE1B+EMTJFavOAL6bJ81HgbPy5W8G/juyvlptwGvy9T35xzyu8cCHqqz/DnB+fmtMO3BmSmlj9rfHFu/dMyL+VuX9B1TEVuldwEUR8SGyflhvrbLNP5P1fypfhQ/yW6V6cQ7wZWBuRDxDdhvThyvWl3toBdmV9HO22kN2G9XRZLepJeDDKaVFfXxupefnx6IZ2LniuEzLv/4eeHtE3E/2B8qNfeyv6u9g9NBDK6X0x4h4FnBD/nNaA7wppfRIRPw9Iu4BriIbUfCb/Gc7B3igH99jVRGxc76viUBnRLwPODCltGp79y1pRDAfm48rmY+33ZXAS4G5wDoqfvYRcUfasl/na/NtK70n/77ayQovZw5ATJKGL/O3+buS+Xvb1ZK/RwF/yAvUTWStRv5nAD5bAySS/cElSZIkSZIkSQWx3YckSZIkSZIkqTAWqSVJkiRJkiRJhbFILUmSJEmSJEkqjEVqSZIkSZIkSVJhmosOYHtMnTo17bHHHkWHIUkaxm699danU0rT+t5SvTFnS5IGk/l6YJivJUmDqbd83dBF6j322IM5c+YUHYYkaRiLiHlFxzAcmLMlSYPJfD0wzNeSpMHUW7623YckSZIkSZIkqTAWqSVJkiRJkiRJhbFILUmSJEmSJEkqjEVqSZIkSZIkSVJhLFJLkiRJkiRJkgpjkVqSJEmSJEmSVBiL1JIkSZIkSZKkwlikliRJkiRJkiQVxiK1JEmSJEmSJKkwFqklSZIkSZIkSYWxSC1JkiRJkiRJKoxFakmSJEmSJElSYSxSS5IkSZIkSZIKY5FakiRJkiRJklQYi9QbN8Lxx8PJJxcdiSRJ6s0nPwnHHgt/+UvRkUiSpB5c//j1HHfxcXziz58oOhRJUgNpLjqAwkXA9ddDS0vRkUiSpN7cfz/89a+wZEnRkUiSpB48s/4Z/jLvL+wwZoeiQ5EkNRBHUpeL021tkFKxsUiSpJ4159fW29uLjUOSJPWoKZoA6EydBUciSWokFqkjoClLonR0FBuLJEnqmUVqSZLqXimyMkNHp+fXkqTaWaSGLUdTS5Kk+lS+qGyRWpKkutVUciS1JKn/LFKDRWpJkhpBeSS1dz5JklS3yiOpLVJLkvrDIjVYpJYkqRHY7kOSpLrX1e4jeVFZklQ7i9RgkVqSpEZgkVqSpLo36f7H+OaVcOq184sORZLUQCxSg0VqSZIagT2pJUkjWER8LyKWRMQ9Vdb9a0SkiJiaz0dEfDMi5kbEXRFxxFDFOfbJRbz7ZnjOnU8P1UdKkoYBi9RgkVqSpEbgSGpJ0sh2MXBq94URsStwCvBExeKXAPvmr3OB84cgPgBKzdn5dVOnPaklSbWzSA0WqSVJagQ+OFGSNIKllP4CLKuy6uvAh4FUsex04AcpcyMwOSJmDEGY0Jzd+RQdFqklSbWzSA2bi9SbNhUbhyRJ6tFNi28D4MlnHis4EkmS6kNEnA4sSCnd2W3VTODJivn5+bJq+zg3IuZExJylS5dud0ylpuz8utSZ+thSkqTNLFKDI6klSWoAT6zJHsC0am21QWSSJI0sETEW+DfgU9uzn5TSBSml2Sml2dOmTdv+wPI7n8IitSSpH5qLDqAuWKSWJKnudZYfnGi7D0mSAPYG9gTujAiAWcBtEXEUsADYtWLbWfmyQRd5kbrJdh+SpH5wJDVYpJYkqQEki9SSJHVJKd2dUtoppbRHSmkPspYeR6SUFgFXAG+JzPOAlSmlhUMRV/nBibb7kCT1h0VqsEgtSVIDSE35ny1t7cUGIklSASLiJ8ANwP4RMT8izu5l8yuBR4G5wP8A7xiCEAGIJtt9SJL6z3YfYJFakqQGkJqzkdTRYZFakjTypJTe0Mf6PSqmE/DOwY6pmsjPr5ssUkuS+sGR1GCRWpKkBtA1krrdIrUkSfWqPJK61GGRWpJUO4vUYJFakqQGsLkntQ9ikiSpXnX1pE4WqSVJtbNIDRapJUlqAOUite0+JEmqX+V2Hz44UZLUHxapwSK1JEkNoNyTmvaOYgORJEk9Ko+kbrLdhySpHyxSg0VqSdKIFhHfi4glEXFPxbIvR8QDEXFXRPw6IiZXrPtYRMyNiAcj4sVDFmh5JLVFakmS6lapyQcnSpL6zyI1WKSWJI10FwOndlt2NXBwSukQ4CHgYwARcSDweuCg/D3fiYimoQjSdh+SJNW/aM4fnOgjJCRJ/WCRGixSS5JGtJTSX4Bl3Zb9MaVUrgbfCMzKp08HfppS2phSegyYCxw1JHHmJ70+OFGSpPpVamnNvvrgRElSPwxakbphbh0Gi9SSJPXurcBV+fRM4MmKdfPzZVuJiHMjYk5EzFm6dOl2B1HuSW27D0mS6pc9qSVJ22IwR1JfTAPcOgxYpJYkqQcR8XGgHbi0v+9NKV2QUpqdUpo9bdq07Q+mlP3ZEh0WqSVJqlfldh9N1qglSf0waEXqRrl1GLBILUlSFRFxJvBy4I0pdd2zuwDYtWKzWfmywZef9DqSWpKk+lUeSV3ywYmSpH4osid1Xdw6DFikliSpm4g4FfgwcFpKaV3FqiuA10fEqIjYE9gXuHkoYupq99FpkVqSpHpVas56Ujf7CAlJUj8UUqSuq1uHwSK1JGlEi4ifADcA+0fE/Ig4G/g2MAG4OiLuiIjvAqSU7gV+BtwH/B54Z0ppaKrGXSOpPeuVJKleNeUPTmxyJLUkqR+ah/oDK24dPqkubh0Gi9SSpBEtpfSGKosv6mX7zwOfH7yIepAXqUv2pJYkqW6Ve1KXvKYsSeqHIR1JXY+3DgNdJ70WqSVJqmNNebsPi9SSJNWtppZR2VcHUkuS+mHQRlLntw4fD0yNiPnAp4GPAaPIbh0GuDGl9PaU0r0RUb51uJ2hvHUYHEktSVIjaMrbfXQ4NEuSpHpVfnCiPaklSf0xaEXqhrl1GOqjSP3003DZZfDII3DWWbDLLjBnTjbKe++9YdMmuOceWLQIbroJdtwRIrKY99oLpk2D+fNhwoTstWwZLFkC69ZBR0c2+qy5OfuaUra/WmUXFLZ/m23V2757WjeY8Uiqf83N8IUvFB2FBljXgxMtUkuSVLfKReom07UkqR+GvCd1vdnQvoHPXP9JvgiDV6ROCX7/e/j73+Goo2DhQrj9dnjsMdh1VxgzBi68EDZsyLb/xjeyIqu3M0vSthk1yiL1MBT5ReWSRWpJkupW14MTE9m5sAOIJEk1GPFF6iBYsmlFNjMYReqU4CMfgS9/ue9tX/zirGj9/e9nifwFL8iWP/oobNwIBx8M994LL30pHHQQlErZ6+GHs1HUe+2Vbbd6NeywA0yfDuPGZdt0dGSv9vZs362ttf2xkGpoJFbLNuXt+vsHSm/77mldrfFIGr7y3sUaZuxJLUlS3SuVmuiIvEjd2enfZZKkmoz4InVTqYm2cs4cjCL1BRdsLlBPn5593WmnrAB90EFZe481a+Dcc2H27Gz9l7+cFZYnThz4eCRJalT5g45L7Y6kliSpXpWixKa8SJ3a2giL1JKkGlikjibaStl0amtjQG9EuuQSePvbs+nvfx/OPLO2902ePJBRSJI0LETe47LUaZFakqR61lECOqGjfRPNjC46HElSAygVHUDRIoKO5qw0nfrzMMFafPe72dePfrT2ArUkSarKBydKktQYOvJKQ2f7ID33SZI07Iz4kdQAHU0loIPUNkBF6gUL4G9/gxtvhJYW+PjHB2a/kiSNYKVmH5woSVIjaC8XqQfqHFuSNOxZpAbaWrIiNRvWb//O7rsPjj8eli7N5l/4Qhg/fvv3K0nSSFfuSW2RWpKkutaR99HssEgtSarRiG/3AfDM+PxBDkuWbvtOUoKvfhWOPnpzgfpFL4IvfGH7A5QkSRU9qVPBkUiSpN50lvKWmu3tBUciSWoUjqQGlk7KDkMsXLjtO/n1r+GDH8ymTz4ZrrgCxowZgOgkSRLgSGpJkhpEuSd1R7sjqSVJtbFIDSwf30x7QPMzy2DjRhg1qvY3/+//wpNPwmc+k82///3w5S9DU9NghCpJ0shlkVqSpIZQLlInH5woSaqRRWogmptZNB5mrQYWLYLdd6/tjffcA6985eb5I4+EL33JArUkSYOg/ODEJovUkiTVtY5SAMme1JKkmtmTGmiKJp6akM889VTtb/zv/948fcop8JOfQEvLgMYmSZJy+UjqsCe1JEl1rTN/cKI9qSVJtXIkNdBU2oYidUrwi19k07fdBocfPiixSZKkTKmlFYAmi9SSJNW19qasSt3ZYbsPSVJtHEnNNo6kfvDBrDXI9Olw2GGDFZokSSqzJ7UkSQ2hs5QXqW33IUmqkUVqoLnUzMJykXrhwtredN112dfjj4eIQYhKkiRVKjXlReoEdFqoliSpXnXm58i2+5Ak1coiNdvY7qOySC1JkgZdU6mJtvJfLp70SpJUtzrzfN3ZbrsPSVJtLFKzDe0+UtpcpD7hhMEKS5IkVShFifbyXy4dHYXGIkmSetaR96RObRapJUm18cGJ9HMkdVsb3HorLF4MO+8M++036PFJkqTsonK7I6klSap7XT2pfXCiJKlGFqnpNpK6Wk/qzk447TQolWD16s2jqE8+2X7UkiQNkVKU6CinXUdmSZJUt8pFantSS5JqZZGabCT1M2Ogs6WZ0rJlsGEDjB69eYNHH4Xf/W7rN77znUMXpCRJI1wpSmxqymcsUkuSVLe6itQdFqklSbWxJzXZSOpUgrZpO2YLuo+mvu++rd901lnwvOcNfnCSJAnILiqvb8ln1q8vNBZJktSzjnK7j7ZNBUciSWoUFqnJTnoBNu2yU7bg0Ue33KBcpH7ve2Hp0uzBid/73hBGKEmSSlFiQ/kesA0bCo1FkiT1rLOUlRps9yFJqpVFarKR1ABrDtg7W3DDDVtucO+92deDDoKpU4cwMkmSVNYUTay3SC1JGqEi4nsRsSQi7qlY9uWIeCAi7oqIX0fE5Ip1H4uIuRHxYES8eChj7Wyy3YckqX8sUrN5JPWqQ/bLFnzyk3DiibB8eTZ/xx3Z1wMPHPrgJEkS0G0kte0+JEkjz8XAqd2WXQ0cnFI6BHgI+BhARBwIvB44KH/PdyKiiSHigxMlSf1lkZrNI6lXHrTv5oXXXgvvfjdceSXccw+MGwdHHllQhJIkyXYfkqSRLKX0F2BZt2V/TCmVK8E3ArPy6dOBn6aUNqaUHgPmAkcNVaxd7T580LEkqUYWqYHmUnbGu2rvWbDffptXXHopvOxl2fQpp8Do0QVEJ0mSwAcnSpLUh7cCV+XTM4EnK9bNz5dtJSLOjYg5ETFn6dKlAxJI10hq231IkmpkkZrN7T7am8gekrhxI7zgBdnKceOyr2efXUxwkiQJcCS1JEk9iYiPA+3Apf19b0rpgpTS7JTS7GnTpg1IPMme1JKkfmrue5Phr9zuo6OzA5qastdvfwsLF8L++8O6dZuL1ZIkqRA+OFGSpK1FxJnAy4GTUkopX7wA2LVis1n5siHR2ZS3+7AntSSpRo6kZvNI6o7UsXnhpElwwAEQYYFakqQ64IMTJUnaUkScCnwYOC2ltK5i1RXA6yNiVETsCewL3DxUcZV7UmORWpJUI0dS020ktSRJqktb9KR2JLUkaYSJiJ8AxwNTI2I+8GngY8Ao4OqIALgxpfT2lNK9EfEz4D6yNiDvTCkN2QlvyntSd1qkliTVyCI1PYykliRJdcWR1JKkkSyl9IYqiy/qZfvPA58fvIh6lvJ2H9iTWpJUI9t94EhqSZIagQ9OlCSpMWzuSd1WcCSSpEZhkRpHUkuS1Ah8cKIkSY2hqyd1h+fYkqTaWKTGkdSSJDUC231IktQYkkVqSVI/WaTGkdSSJDUCH5woSVJj6GjJSw2bNhUbiCSpYVikBpojG5bV3ulDHSRJqleOpJYkqTG0tWZXlWP9xoIjkSQ1CovUVIyktt2HJEl1qxQle1JLktQA2kZl59gl87UkqUYWqanoSW27D0mS6lZTNG0eSe1JryRJdas9H0ld2uBIaklSbSxS40hqSZIage0+JElqDJtGZwnbIrUkqVYWqXEktSRJjcAHJ0qS1BgcSS1J6i+L1DiSWpI0skXE9yJiSUTcU7Fsh4i4OiIezr9OyZdHRHwzIuZGxF0RccRQxelIakmSGkP76KxI3WSRWpJUI4vUOJJakjTiXQyc2m3ZR4FrUkr7Atfk8wAvAfbNX+cC5w9RjFs+ONEitSRJdauttdzuY1PBkUiSGoVFahxJLUka2VJKfwGWdVt8OnBJPn0J8IqK5T9ImRuByRExYyjibIomVo3KZ1avHoqPlCRJ26BjVCsATRstUkuSajOoRepGuX3YkdSSJG1lekppYT69CJieT88EnqzYbn6+bNCVosSK0fnMihVD8ZGSJGkbtI+y3YckqX8GeyT1xTTA7cOOpJYkqWcppQSk/r4vIs6NiDkRMWfp0qXbHUdTqWIk9cqV0Nm53fuUJEkDr2skte0+JEk1GtQidSPdPgyOpJYkqcLich7Ovy7Jly8Adq3Ybla+bCsppQtSSrNTSrOnTZu23QGVokRHE6xuJStQr1mz3fuUJEkDry1/cGKz7T4kSTUqoif1dt0+PNCjsgCaS9lDHdo72wdkf5IkDQNXAGfk02cAl1csf0veput5wMqKvD6oyheVV46JbIEtPyRJqkube1K3FRyJJKlRFPrgxG25fXigR2WB7T4kSSNbRPwEuAHYPyLmR8TZwBeBF0XEw8DJ+TzAlcCjwFzgf4B3DFWcpcj+bLEvtSRJ9a1jdFakbrbdhySpRs0FfObiiJiRUlq4rbcPDzTbfUiSRrKU0ht6WHVSlW0T8M7Bjai6riJ1uS+1RWpJkupSGpNdUXYktSSpVkWMpK6/24cdSS1JUt0r5+sVo/ObsCxSS5JUl9LorEjdbJFaklSjQR1Jnd8+fDwwNSLmA58mu134Z/mtxPOA1+abXwm8lOz24XXAWYMZWyVHUkuSVP/KI6mXl9t9rFxZXDCSJKlH0dpKR0BTRye0tUFLS9EhSZLq3KAWqRvl9mFHUkuSVP/sSS1JUmNobmphfTOMbwPWr7dILUnqU6EPTqwXjqSWJKkxlKJkkVqSpDrXFE2sK9el168vNBZJUmOwSI0jqSVJahRbFKmXLy80FkmSVF1zqZn1FqklSf1gkRpHUkuS1Ciaookl4/KZRYsKjUWSJFXXXGpmfbm5qEVqSVINLFJTMZLaIrUkSXWtFCUWTMhnFiwoNBZJklSdI6klSf1lkZqKkdS2+5Akqa41lZpYMDGfsUgtSVJdciS1JKm/LFKTJVCA9s72giORJEm9KUWJpypHUqdUaDySJGlrjqSWJPWXRWps9yFJUqMoRYk1oyBNGA8bNsCKFUWHJEmSumkqNbHOIrUkqR8sUmO7D0mSGkX57qeOGTOyBbb8kCSp7mzR7mPdukJjkSQ1BovUOJJakqRG0XVheZfp2YL58wuMRpIkVWO7D0lSf1mkxpHUkiQ1ivJI6k377JUtuPPOAqORJEnV+OBESVJ/WaTGkdSSJDWKcpF6w5GHZgtuvLHAaCRJUjWOpJYk9ZdFaqCllGXPto62giORJEm9KRep1x15SLbgxhshpQIjkiRJ3TmSWpLUXxapgZamvEjdaZFakqR6Vr77af0eM2HKFFi0yIcnSpJUZ5qiyZHUkqR+sUgNtDa1ArCpY1PBkUiSpN6UR1J3pE44JB9NfffdBUYkSZK6ay41s84itSSpHyxSY5FakqRGUS5St3e2w7OfnS20SC1JUl3Zot3HunWFxiJJagwWqdlcpLYntSRJ9a1qkfqOO4oLSJIkbcUHJ0qS+ssiNY6kliSpUTRF1pO6vbMdDjssW/iTn8Df/15cUJIkaQs+OFGS1F8WqbFILUlSo9jck7oDnvMceMlLshUXXVRgVJIkDb6I+F5ELImIeyqW7RARV0fEw/nXKfnyiIhvRsTciLgrIo4YylibSj44UZLUPxapsUgtSVKj2KLdRwS8//3ZikceKTAqSZKGxMXAqd2WfRS4JqW0L3BNPg/wEmDf/HUucP4QxQg4klqS1H8WqbFILUlSo2gqVbT7ANhnn+yrRWpJ0jCXUvoLsKzb4tOBS/LpS4BXVCz/QcrcCEyOiBlDEihZkXqdI6klSf1gkRpoKWXZ0yK1JEn1ravdR2dHtmDXXaG5GRYsgFtuKTAySZIKMT2ltDCfXgRMz6dnAk9WbDc/X7aViDg3IuZExJylS5cOSFA+OFGS1F8WqXEktSRJjWKLdh+QFajb8+mjjoKVK7Pphx+GtWsLiFCSpGKklBKQtuF9F6SUZqeUZk+bNm1AYtmi3ce6dQOyT0nS8GaRGovUkiQ1iq2K1ACnVrTn/Pvf4YILYL/9YPx4OPJIuOwyT5AlScPV4nIbj/zrknz5AmDXiu1m5cuGhCOpJUn9ZZGailuHUwedqbPgaCRJUk+aoltPaoCvfCV7iCLAP/8zvOMdm9fddhu8/vVZ0fpzn4O77hrCaCVJGnRXAGfk02cAl1csf0tkngesrGgLMuiaoskHJ0qS+qW5702Gv4igtamVTR2baOtoY1TzqKJDkiRJVVReWO5y0EHwm9/Ay18O8+dny176Uhg1Cq6/PmsBsmABfPKT8J//CfPmwdSpBUQv9UOqcsd+92V9zUuNoFTKXupTRPwEOB6YGhHzgU8DXwR+FhFnA/OA1+abXwm8FJgLrAPOGspYm0vNrGnNZ9asGcqPliQ1KIvUuXKRelPHJovUkiTVqartPgBOPhn+6Z/gkUfglFPgE5+A1vzseNMm+PWv4b/+KxtZ/alPwaxZcPXVWWEkJXj+8+E1r8lGXP/sZ9DRAbNnwyGHZPtYvhwuvTQbsb18OSxaBJMmwTHHwPTp8NRTsGIFrF6dvdasyXpil/tir1uXxfPYY7B0KRx6aLaPZ57JiuktLVt+P7UUH7sv6+yEjRuzV3v7lq/+7Ht754fiM4ZjzNJI9YtfwKteVXQUDSGl9IYeVp1UZdsEvHNwI+pZc6mZta3QXoLmdeugrW3rXCdJUgWL1Dn7UkuSVP+aSlXafUBW6L300upvam2F170OJk/O+leff/7W21x7bTbKevToLftXv//92dfvfz8rQg+Ue+4ZuH1pZCm3tql1Xqp3/s4OS82lZghYPSqYsj7BqlWw445FhyVJqmMWqXMWqSVJqn89jqSuxYtfDOedB9/6Fuy8M7znPVkB+5ln4Iorste6ddno6X33hV/+Er7+9c3vP+EE2GuvbMTyIYdkLURuvTV7/4wZWQuRCROy1/jxMG5c9rWtDcaMyUY0jx0Le+8Nd94J06Zl+9u4MdtmW4qPlcsisiJ7eWR2czM0NWWv/u57e+eH4jNGSoyS1IDK+Xr1aIvUkqTaWKTOtZSyW48sUkuSVL+aI+9J3dnRx5Y9eO97s1d3Z5+dFY6XLs2K0U1NcOGFcPHFcOCBcNZZcPTR2x54d89//sDtS5KkOlMuUq8anV98W7mywGgkSY3AInXOkdSSJNW/7RpJ3ZdDD91y/pxzspckSeqXcnuuVeXHPa1aVVwwkqSG4GOUcxapJUmqfz32pJYkSXWjfFF55aj8IbGOpJYk9cEida5cpG7rbCs4EkmS1JPySW9H2sZ2H5IkadB1tftozRc4klqS1AeL1DlHUkuSVP8Gtd2HJEkaEOV8vdyR1JKkGlmkzlmkliSp/jWF7T4kSap35Xxtuw9JUq0sUucsUkuSVP8cSS1JUv2LCJqiiZU+OFGSVCOL1DmL1JIk1b+untSd9qSWJKmeNZWaWFUuUjuSWpLUB4vUuZamFsAitSRJ9cyR1JIkNYbmUjMrR+czFqklSX2wSJ1zJLUkSfWvqWRPakmSGkFzqZkV5SL1ihVFhiJJagAWqXMWqSVJqn9d7T6S7T4kSapnzaVmlpeL1MuXFxqLJKn+WaTOWaSWJKn+2e5DkqTG0FxqZtmYfMYitSSpDxapc62lrEjd1tFWcCSSJKknFqklSWoMzaVmlpeL1MuWFRqLJKn+WaTOOZJakqStRcT7I+LeiLgnIn4SEaMjYs+IuCki5kbEZRHROlTxNIU9qSVJagRN0bRlu4+UCo1HklTfCilS19sJL1ikliSpu4iYCbwHmJ1SOhhoAl4PfAn4ekppH2A5cPZQxdTVk7rTntSSJNWz5lIzG1ugc8xoaGuDtWuLDkmSVMeGvEhdjye8sLlIvbFj41B+rCRJ9a4ZGBMRzcBYYCFwIvCLfP0lwCuGLBjbfUiS1BC6LixPnpQtsC+1JKkXRbX7qKsTXoCxLWMBWNe2big/VpKkupVSWgB8BXiCLFevBG4FVqSUylXi+cDMau+PiHMjYk5EzFm6dOmAxNRUytt9JIvUkiTVs81F6onZAovUkqReDHmRentPeAfLuNZxAKzd5C1IkiQBRMQU4HRgT2AXYBxwaq3vTyldkFKanVKaPW3atAGJyZHUkiQ1hq6cPSkvUvvwRElSL4po97FdJ7yDMSoLYFxLVqRes2nNgO1TkqQGdzLwWEppaUqpDfgVcAwwOb8bCmAWsGCoArIntSRJjaGlqQWAtknjswWOpJYk9aKIdh/bdcI7GKOyoGIkdZsjqSVJyj0BPC8ixkZEACcB9wHXAq/OtzkDuHyoAnIktSRJjaGllBepJ2bn2o6kliT1pogidd2d8AKMb82u7lqkliQpk1K6iex5EbcBd5P93XAB8BHgAxExF9gRuGioYmqKvCe1RWpJkupaa1MrABsnT8gWPP10gdFIkupdc9+bDKyU0k0RUT7hbQduJzvh/R3w04j4XL5syE54YXO7D3tSS5K0WUrp08Cnuy1+FDiqgHA2t/tItvuQJKmeldt9bJiSF6kHsF2nJGn4GfIiNdTfCS/Y7kOSpEZguw9JkhpDud3H+h3yIvWSJQVGI0mqd0W0+6hLPjhRkqT611Sy3YckSY2gPJJ63ZT8wYmOpJYk9cIida6rJ7XtPiRJqluOpJYkqTGUe1KvnTQ2W+BIaklSLyxS52z3IUlS/evqSd1pT2pJkupZud3HmskWqSVJfbNInfPBiZIk1T9HUkuS1BjK7T7WTBqTLVi6FFIqMCJJUj2zSJ2rHEmdTJySJNWlprAntSRJjaC1lLX72NBagnHjYONGWL264KgkSfXKInWuudTMqKZRdKZONrRvKDocSZJURVe7j2S7D0mS6ll5JPWmjk2w007ZwsWLC4xIklTPLFJXsC+1JEn1zXYfkiQ1hnJP6rbONth112zhvHkFRiRJqmcWqSvYl1qSpPpWLlK3dbQVHIkkSepNeSR1W0cb7LNPtnDu3AIjkiTVM4vUFRxJLUlSfWttyvpbburYVHAkkiSpN1vkbIvUkqQ+WKSuML51POBIakmS6tWo5lEAbOzYWHAkkiSpN1u0+7BILUnqg0XqCmOaxwD44ERJkurUqKa8SN1ukVqSpHpmuw9JUn9YpK5QHp1lkVqSpPrkSGpJkhrDFiOp9947W/jII9DZWWBUkqR6ZZG6wujm0YBFakmS6lW5v6UjqSVJ2iwi3h8R90bEPRHxk4gYHRF7RsRNETE3Ii6LiNahjGmLntQTJ8K0abBhAzz11FCGIUlqEBapK1ikliSpvpXbffjgREmSMhExE3gPMDuldDDQBLwe+BLw9ZTSPsBy4OyhjGuLdh9gyw9JUq8sUlewSC1JUn1rLjUTBB2pg47OjqLDkSSpXjQDYyKiGRgLLAROBH6Rr78EeMVQBrRFuw+wSC1J6pVF6gpdD2Oyz6UkSXUpIuxLLUlShZTSAuArwBNkxemVwK3AipRSe77ZfGBmtfdHxLkRMSci5ixdunTA4iq3+9hqJPUjjwzYZ0iShg+L1BUcSS1JUv3ruqhsX2pJkoiIKcDpwJ7ALsA44NRa359SuiClNDulNHvatGkDFle53cemzrxFV7lI/cADA/YZkqThwyJ1BYvUkiTVv66HJzqSWpIkgJOBx1JKS1NKbcCvgGOAyXn7D4BZwIKhDKqr3Ud5JPUhh2Rf77xzKMOQJDUIi9QVLFJLklT/yu0+fHiiJElA1ubjeRExNiICOAm4D7gWeHW+zRnA5UMZVNeDE8s9qfffH0aNgscegxUrhjIUSVIDsEhdoXz7sEVqSZLql+0+JEnaLKV0E9kDEm8D7iY7z78A+AjwgYiYC+wIXDSUcZXvfOq6qNzSAs9+djbtaGpJUjcWqSuUR1J70itJUv3ywYmSJG0ppfTplNIBKaWDU0pvTiltTCk9mlI6KqW0T0rpNSmlIU2cW7X7ADjiiOzrn/88lKFIkhqAReoKtvuQJKn+OZJakqT6t1W7D4DXvjb7evHF0NEx9EFJkuqWReoKFqklSap/jqSWJKn+VR1JfcIJsOee8MQT8JvfFBSZJKke9VqkjogTK6b37LbuHwcrqKKUT3o3dFikliQ1lpGUs8s9Lh1JLUlqNCMxX2/xoONSCd73vmz6fe+DH/0I2tuHPDZJUv3payT1Vyqmf9lt3ScGOJbC2ZNaktTARkzOLrf72OKkV5KkxjBi8nXVdh8AZ58N++8P8+bBm98M7353AdFJkupNX0Xq6GG62nzDs92HJKmBjZicbbsPSVIDGzH5umq7D4Bx4+CWW+CTn4SmJvjud+Fb34LOTli3zpHVkjRC9VWkTj1MV5tveBapJUkNbMTkbB+cKElqYCMmX/c4khpgwgT4j/+A88/P5t/zHpg0KVs+axb84AfwjW/A296WFbQlScNecx/r94qIK8iu6Janyef37Pltjal80muRWpLUgEZMznYktSSpgY2YfF21J3V355yTjZz+4hezhykCLF4MZ5yxeZsf/hD+9jeYPXvzspQghtXAc0ka8foqUp9eMf2Vbuu6zzc8R1JLkhrYiMnZPjhRktTARky+7rHdR6UI+Jd/gbe/HZYtg9Gj4dJL4V3vgrY22HFHeOYZeM5z4E1vgr32ykZWX301PPvZ8I//COvXw8teBkcfbeFakhpYr0XqlNL1lfMR0QIcDCxIKS0ZzMCK0PXgREdmSZIazEjK2V3tPszXkqQGM5Lyda/tPrqLyArSAOeeC8ceC08/nT1gcffds0L0j3605Xtuvz17AXzhCzBtGrz2tVnheuHC7MGMTz4JZ54JBx+c9cKGrPf12rVZ/+vly+HBB2H8eJgyBcaMyT6z1ENn1PZ2aM7LKClBR0dWTC+/2tuzrxs2ZPFMmtS/gyZJI1ivReqI+C7wrZTSvRExCbgB6AB2iIgPppR+MhRBDhVHUkuSGtVIytnlInWvtw9LklSHRlK+rqndR08OOGDz9K9/DbfemhWxH3ggK/y+6U1ZS5BHH4XjjoNrroGlS+H//b/sVem7382+jhuXFZhXruz9s6dMyT5j1aqsoN3aCmvWZIXrNWuy+c7O2h7wOG5ctv3o0TBq1JYjvTs7s2J2R0c23dmZFb7L09WWlZX3U+1r5atUqj5dbb78Gggpbfm1+7Tqi3cg1I96/lm89KVw0UWD+hF9tft4YUrp7fn0WcBDKaVXRMTOwFXAsEmgsLnHpUVqSVIDGjE5u6snte0+JEmNZ8Tk6647lbc3X7/4xdmru7/+NftaKmUF0L/8Bb7yFXj8cWhpyUZEP/NMNj9vXjZ6umzcOBg7Nhs5feCBWUF63bps9Pb8+dkI655sqii6l0rZZ5Vfzc3Z19ZWWLAg+8zKz5WkRrVixaB/RF9F6spLni8Cfg6QUloU9Vzd30aOpJYkNbARk7Nt9yFJamAjJl8P+vl1ZUuOiGxE9XHHVd82JbjtNti4EZ77XGhq6nm7J5/MRjdPmJB9xsaNWTuQtjaYODH72tSUFaR7agsC2UjrDRuy92/cmE1XishGWJf3Ux7dXPmqXFYe6dx9lHLl12qv8mjs7tPV5gdS91He3adVHxzhXj/q/WcxevSgf0RfReoVEfFyYAFwDHA2QEQ0A2MGObYhZ5FaktTARkzOdiS1JKmBjZh83Vxqpima6EgdtHe201zqq/wwiCLgyCNr22633XrfprnG76O5OStujx9f2/aSNML19b/rPwPfBHYG3pdSWpQvPwn43WAGVoRykXp92/qCI5Ekqd9GTM4u97h0JLUkqQGNmHwN2Tn22ra1bGjfwPhWi7WSpJ71WqROKT0EnFpl+R+APwxWUEUZ1TSKIGjrbCv+Sq8kSf0wknL2gPW4lCRpiI2kfA0WqSVJteu1ChsR3+xtfUrpPQMbTrEignGt41izaQ3r29YzYdSEokOSJKkmIylnj2nO7oZe3+6dT5KkxjKS8jVsbtFlS01JUl/6Gir8duAe4GfAU8Cw73I/tmUsazatYW3bWovUkqRGMmJy9pgWi9SSpIY1YvI1+NwnSVLt+ipSzwBeA7wOaAcuA36RUloxyHEVZmzLWADWta0rOBJJkvplxORsc7UkqYGNmHwNFqklSbUr9bYypfRMSum7KaUTgLOAycB9EfHmoQiuCJ74SpIa0UjK2V3tPnzQsSSpwYykfA0WqSVJtavpyYARcQTwBuBFwFXArYMZVJEsUkuSGtlIyNnmaklSoxsJ+RosUkuSatfXgxP/A3gZcD/wU+BjKaX2oQisKONaxgGe+EqSGstIytn2pJYkNaqRlK9hc5F6Y/vGgiORJNW7vkZSfwJ4DDg0f30hIiB7uENKKR0yuOENvfLorLWb1hYciSRJ/TIoOTsiJgMXAgcDCXgr8CBZD809gMeB16aUlm9X9P1guw9JUgMbUefYjqSWJNWqryL1noPxofV4wlvmLcSSpAY1KDkb+Abw+5TSqyOiFRgL/BtwTUrpixHxUeCjwEcG6fO3Yq6WJDWwwcrXdckitSSpVr0WqVNK86otj4gSWf+squtrUHcnvGWe+EqSGtFg5OyImAQcC5yZf8YmYFNEnA4cn292CXAdQ5izbfchSWpUg3iOXZcsUkuSalXqbWVETIyIj0XEtyPilMi8G3gUeO22fGDFCe9FkJ3wppRWAKeTneiSf33Ftux/e1mkliQ1osHI2WSjvZYC34+I2yPiwogYB0xPKS3Mt1kETO8hpnMjYk5EzFm6dOk2hrA1c7UkqVENUr6uW6OaRgEWqSVJfeu1SA38ENgfuBs4B7gWeDXwipTS6dv4mdt1wjvYfHCiJKlBDUbObgaOAM5PKR0OrCW706lLSimRte7aSkrpgpTS7JTS7GnTpm1jCFuzJ7UkqYENRr6uW46kliTVqq+e1HullJ4NEBEXAguB3VJK25Nhyie8704p3RQR36DKCW9EVD3hjYhzgXMBdtttt+0IozpHZ0mSGtRg5Oz5wPyU0k35/C/IcvbiiJiRUloYETOAJdsTeH+1NrVSihJtnW20d7bTXOrrzxlJkurGYOTrumWRWpJUq75GUreVJ1JKHWQnqtubXaqd8B5BfsIL0NsJ72CNyiorF6nXtq0d8H1LkjSIBjxnp5QWAU9GxP75opOA+4ArgDPyZWcAl2/P5/RXRDiaWpLUqAbjHLtuWaSWJNWqr6FHh0bEqnw6gDH5fJANeJ7Y3w9MKS2KiCcjYv+U0oNsPuG9j+xE94sUcMJb5khqSVKDGvCcnXs3cGn+oONHgbPILnL/LCLOJnvA05D30BzTMoa1bWtZ376eCaMmDPXHS5K0rQYrX9cli9SSpFr1WqROKTUN0ufW5QkvWKSWJDWmwcrZKaU7gNlVVp00GJ9XK/O1JKkRDeI5dl2ySC1JqlUhTRzr9YQXYFyrD06UJKne2e5DkqT6Vy5Sb+zYWHAkkqR611dP6hHHkVmSJNW/cr5e326RWpKkeuVIaklSrSxSdzNp1CQAnln/TMGRSJKknoxpyUZSe1FZkqT6ZZFaklQri9Td7DJhFwAWrl5YcCSSJKknXSOpbfchSVLdKhepvfNJktQXi9TdlIvUT61+ipRSwdFIkqRqyj2pHUktSVL98hkSkqRaWaTuZsKoCYxvHc/69vWs3Liy6HAkSVIVE0dNBGDVxlUFRyJJknoyrnUcAGvb1hYciSSp3lmkrqJyNLUkSao/k0dPBmDFhhWFxiFJknpWbs/lnU+SpL5YpK7CIrUkSfXNIrUkSfVvXEs+knqTI6klSb2zSF2FRWpJkuqbRWpJkjaLiMkR8YuIeCAi7o+IoyNih4i4OiIezr9OGeq4HEktSaqVReoqdh63MwALVy8sOBJJklSNRWpJkrbwDeD3KaUDgEOB+4GPAteklPYFrsnnh1S5SG1PaklSXyxSV1E+8fVhTJIk1aeuIvXGFYXGIUlS0SJiEnAscBFASmlTSmkFcDpwSb7ZJcArhjq28oMTHUktSeqLReoqJo2eBFikliSpXk0aleVqR1JLksSewFLg+xFxe0RcGBHjgOkppfLtwYuA6dXeHBHnRsSciJizdOnSAQ2sayT1prWklAZ035Kk4cUidRUTR00EYOXGlQVHIkmSqrHdhyRJXZqBI4DzU0qHA2vp1tojZRXiqlXilNIFKaXZKaXZ06ZNG9jASs20NrWSSGzs2Dig+5YkDS8WqasoF6kdSS1JUn0qF6lXbvCCsiRpxJsPzE8p3ZTP/4KsaL04ImYA5F+XFBFc5WhqSZJ6YpG6ivItxBapJUmqT46kliQpk1JaBDwZEfvni04C7gOuAM7Il50BXF5AeIxrsS+1JKlvzUUHUI8cSS1JUn0rPz9ixYYVpJSIiIIjkiSpUO8GLo2IVuBR4CyyQWk/i4izgXnAa4sIrGskdZsjqSVJPbNIXYU9qSVJqm+tTa2MbRnLurZ1rNm0hgmjJhQdkiRJhUkp3QHMrrLqpCEOZSvjWh1JLUnqm+0+qnAktSRJ9W/q2KkALF23tOBIJElST+xJLUmqhUXqKsq3EFukliSpfk0fNx2AJWsLeQ6UJEmqgT2pJUm1sEhdxZjmMTRFExvaN7CpY1PR4UiSpCp2GrcTAIvXLC44EkmS1BN7UkuSamGRuoqIsOWHJEl1rjySevFai9SSJNUre1JLkmphkboHtvyQJKm+lUdS2+5DkqT6NbbZntSSpL5ZpO5BeST1ig0rig1EkiRVNX18PpLadh+SJNWt8a3jAVizaU3BkUiS6plF6h7sNmk3AB5Z9kjBkUiSpGq6RlKvcyS1JEn1asKoCQCs3rS64EgkSfXMInUPDpx6IAD3Lb2v4EgkSVI1XT2pHUktSVLdmtCaF6k3WqSWJPXMInUPDtrpIADue9oitSRJ9ajc7uOp1U8VHIkkSeqJI6klSbWwSN2Dg6ZlRep7l9xbcCSSJKmacmuuJ1c9SUqp4GgkSVI1XSOpLVJLknphkboHB0w9gFKUePCZB9nQvqHocCRJUjcTR01k8ujJbGjfwNPrni46HEmSVMXEURMB231IknpnkboH41rH8aypz6K9s507F91ZdDiSJKmK8mjqJ1Y+UXAkkiSpmnK7j1UbVxUciSSpnlmk7sVzZj4HgJsX3FxwJJIkqRqL1JIk1TfbfUiSamGRuhfP2SUrUt/y1C0FRyJJkqrZbaJFakmS6lnXgxNt9yFJ6oVF6l4cNfMowCK1JEn1qjySet7KeQVHIkmSqnEktSSpFhape3HI9ENobWrlgacfYOWGlUWHI0mSutl98u6AI6klSapXjqSWJNXCInUvWptaOXT6oQDcuvDWgqORJEnd2ZNakqT6Nq5lHEGwvn097Z3tRYcjSapTFqn7MHuX2QDcvvD2giORJEndWaSWJKm+RUTXaOo1m9YUHI0kqV5ZpO7DPjvsA9jrUpKkejRj/AyaS80sXruYDe0big5HkiRV0dWX2pYfkqQeWKTuw+6Tsl6XFqklSao/TaUmZk2cBcD8VfMLjkaSJFUzcdREAFZu9FlPkqTqLFL3wduIJUmqb+VcPW+FF5QlSapHk0dPBmDlBovUkqTqLFL3YffJ+UhqT3wlSapLMyfMBGDB6gUFRyJJkqopF6mXb1hebCCSpLplkboP08ZOY3TzaJZvWG7/LEmS6tAuE3YBYOHqhQVHIkmSqpkyZgoAy9dbpJYkVWeRug8RYcsPSZLq2IzxMwBYuMYitSRJ9WjK6KxIvWLDimIDkSTVLYvUNSiP0Fq0ZlHBkUiSpO5mTLBILUlSPbPdhySpLxapa7Dz+J0Bi9SSJNWj8sXkp1Y/VXAkkiSpmvJIatt9SJJ6YpG6BjuPs0gtSRq5IqIpIm6PiN/m83tGxE0RMTciLouI1iLj62r3YU9qSZLqUldPakdSS5J6UFiRut5PeCs5klqSNMK9F7i/Yv5LwNdTSvsAy4GzC4kqV9nuI6VUZCiSJKmKcrsPe1JLknpS5Ejquj7hrTR9/HQAFq21SC1JGlkiYhbwMuDCfD6AE4Ff5JtcAryikOByE1onMK5lHOva1nnyK0lSHepq9+FIaklSDwopUjfCCW+l8kjqxWsWFxyJJElD7jzgw0BnPr8jsCKl1J7PzwdmFhBXl4hgnx32AeChZx4qMhRJklRFV7sPe1JLknpQ1Ejq89jGE96IODci5kTEnKVLlw56oGC7D0nSyBQRLweWpJRu3cb3D1nO3n/q/gA8+MyDg/o5kiSp/8rtPhxJLUnqyZAXqbf3hDeldEFKaXZKafa0adMGOLrqykXq+avm2+tSkjSSHAOcFhGPAz8lu+vpG8DkiGjOt5kFLKj25qHM2fvvmBepn7ZILUlSvdlxzI4APLPumYIjkSTVqyJGUm/XCW8Rdhq3E7tM2IXlG5Yz56k5RYcjSdKQSCl9LKU0K6W0B/B64M8ppTcC1wKvzjc7A7i8oBC7dBWpHUktSVLdGd86nlFNo1jfvp61m9YWHY4kqQ4NeZG6kU54y0pR4lXPehUAP7/v5wVHI0lS4T4CfCAi5pK17Lqo4Hhs9yFJUh2LCKaNy+6qenrd0wVHI0mqR0X1pK6m7k54K73mwNcAWZHalh+SpJEmpXRdSunl+fSjKaWjUkr7pJRek1LaWHR85ZHUDz/zMB2dHQVHI0mSups6dioAS9cNzbOlJEmNpdAidb2f8FY6ZrdjmDF+Bo+veJxbF25TO21JkjRIJoyawC4TdmFjx0bmrZxXdDiSJKmbaWOzkdRL11qkliRtrZ5GUte1UpQ4ff/TAfjzY38uOBpJktSdD0+UJKl+ldt9OJJaklSNRep+OGingwB4dPmjBUciSZK68+GJkiTVL0dSS5J6Y5G6H/aashdgkVqSpHp0wNQDALh3yb0FRyJJ0tCLiKaIuD0ifpvP7xkRN0XE3Ii4LCJai4yvq0jtSGpJUhUWqfvBIrUkSfXrsJ0PA+D2RbcXG4gkScV4L3B/xfyXgK+nlPYBlgNnFxJVrtzu4+l1TxcZhiSpTlmk7oc9Ju8BwLyV82jvbC82GEmStIVykfruJXfT1tFWbDCSJA2hiJgFvAy4MJ8P4ETgF/kmlwCvKCS43IzxMwB4ctWTRYYhSapTFqn7YXTzaHaZsAvtne08udLEKklSPZk0ehJ7TdmLTR2buP/p+/t+gyRJw8d5wIeBznx+R2BFSqk8umo+MLPaGyPi3IiYExFzli4dvFYc++ywDwCPLHtk0D5DktS4LFL307N3ejYAN8y/oeBIJElSd4dMPwSA+5beV3AkkiQNjYh4ObAkpXTrtrw/pXRBSml2Smn2tGnTBji6zfacsidB8PiKx73jSZK0FYvU/fSivV4EwB8f+WPBkUiSpO72mpw9P2LeinkFRyJJ0pA5BjgtIh4HfkrW5uMbwOSIaM63mQUsKCa8zOjm0ew6aVc6UgfzVpqnJUlbskjdTy/e58VAVqROKRUcjSRJqlR+fsTjKx4vNA5JkoZKSuljKaVZKaU9gNcDf04pvRG4Fnh1vtkZwOUFhdil3PLj4WceLjgSSVK9sUjdTwdNO4hdJuzCwjULuWfJPUWHI0mSKnQVqVc+XmgckiTVgY8AH4iIuWQ9qi8qOB723WFfAOYum1twJJKkemORup8iglP2PgWAPzzyh4KjkSRJlfacsicAjy1/rOBIJEkaeiml61JKL8+nH00pHZVS2iel9JqU0sai4yuPpLZILUnqziL1Nij3pb7u8euKDUSSJG1h90m7A1m7j87UWXA0kiSpUleRerlFaknSlixSb4PZu8wG4M7FdxYciSRJqjRh1ARmTZzFxo6N3LnIPC1JUj1xJLUkqScWqbfB3lP2ZmzLWOavms+y9cuKDkeSJFV46T4vBeA3D/2m4EgkSVKlvafsDcCjyx+lvbO94GgkSfXEIvU2aCo18eydng3gKC1JkurMP+z/DwBc+fCVBUciSZIqjWkZw6yJs2jvbGfeinlFhyNJqiMWqbfRodMPBeCeJfcUHIkkSap03O7HUYoSty68lXVt64oOR5IkVdh/x/0BePCZBwuORJJUTyxSb6O9puwFwLyVXv2VJKmeTBg1gUOnH0p7Zzs3L7i56HAkSVKFA6YeAMADTz9QcCSSpHpikXob7TZpNwCeWPlEwZFIkqTujtn1GAD+/sTfC45EkiRVetbUZwFw/9L7C45EklRPLFJvI4vUkiTVr2N2y4vUT1qkliSpnnSNpH7GkdSSpM0sUm8ji9SSJNWv8kjq/3vy/+hMnQVHI0mSyspFakdSS5IqWaTeRjMmzKApmli4ZiEb2zcWHY4kSaqw66Rd2W3SbqzcuJJ7l9xbdDiSJCm3y4RdmNA6gWfWP8PT654uOhxJUp2wSL2NmkvNzJw4E4D5q+YXHI0kSeruebOeB8Ccp+YUHIkkSSqLCB+eKEnaikXq7bD3lL0BeHjZwwVHIkmSujts+mEA3Ln4zmIDkSRJW3jWNB+eKEnakkXq7eDVX0mS6tdhOx8GwB2L7ig0DkmStKUDdsz7Uj9tkVqSlLFIvR184IMkSfXr0J0PBbKR1CmlgqORJEllh0w/BIAb599YcCSSpHphkXo7dI2kfsaR1JIk1ZsZ42cwdexUVmxYwRMrnyg6HEmSlDt292NpKbVw04KbWLZ+WdHhSJLqgEXq7VAuUt+39D5HaEmSVGcioqvlh32pJUmqHxNGTeAFu72AztTJnx79U9HhSJLqgEXq7bDrxF3ZYcwOPL3uaUdoSZJUhw6dnrX8sC+1JEn15dR9TgXgqrlXFRyJJKkeWKTeDhHBUTOPAuDmBTcXHI0kSerOhydKklSfykXq38/9vXcmS5IsUm+vo3bJitS3PHVLwZFIkqTujpxxJAD/9+T/eQIsSVIdefZOz2bmhJksWrOIX93/q6LDkSQVzCL1dnIktSRJ9euAqQcwY/wMFq9dzL1L7y06HEmSlIsI/u2F/wbAB6/+IJ2ps+CIJElFski9nZ4z8zkAzHlqDh2dHQVHI0mSKkUEJ+91MgBXPnxlwdFIkqRKb5/9dnabtBuPr3icvz3xt6LDkSQVyCL1dtpp3E7sMXkP1rat5f6n7y86HEmS1M2rnvUqAL550zfZ2L6x4GgkSVJZKUq88dlvBOATf/4E69vWFxyRJKkoFqkHwHN2yUZT3/DkDQVHIkmSuvuH/f+BQ6YfwoLVC/j1A78uOhxJklThHc95BzPGz+CvT/yV/7ntf4oOR5JUEIvUA+D4PY4H4I+P/rHYQCRJ0lZKUeKth70VgF/c94uCo5EkSZVmTZzF1178NQAuu/eygqORJBXFIvUAeMk+LwHgj4/8kbaOtoKjkSRJ3b3qwKzlx5UPX2nLD0mS6szL93s5Y5rH8H9P/h/zV80vOhxJUgEsUg+APafsyf477s+qjau4beFtRYcjSZK6mTVxFs+a+izWt6/nrsV3FR2OJEmqML51PC/d96WAdz1J0khlkXqAPHfWcwEsUkuSVKeOmnkUADcvuLngSCRJUnevPei1APzs3p8VHIkkqQgWqQfIETsfAVikliSpXpUfdHzzUxapJUmqNy/b92WMaR7DDfNv4ImVTxQdjiRpiFmkHiBHzMiK1LcuvLXgSCRJUjXlu57+/sTfC45EkiR1N651HC/f7+WAo6klaSSySD1ADp9xOC2lFu5cfCdL1i4pOhxJktTNYTsfxoTWCTyy/BEfyiRJUh36p2f/EwDn3Xge69vWFxyNJGkoWaQeIONbx3PyXifTmTq54sErig5HkiR101xq5oW7vxCA6x+/vuBoJElSd6ftfxqH73w4C1Yv4JI7Lyk6HEnSELJIPYD+8Vn/CMAv7/9lwZFIkqRqjt/9eACue/y6QuOQJElbK0WJDxz9AQB+cOcPCo5GkjSUhrxIHRG7RsS1EXFfRNwbEe/Nl+8QEVdHxMP51ylDHdv2On3/0ylFiWsevYYVG1YUHY4kSdtlOObs4/c4HoDr5l1XaBySJKm6Vx7wSsa1jOOG+Tcwb8W8osORJA2RIkZStwP/mlI6EHge8M6IOBD4KHBNSmlf4Jp8vqFMGzeNY3c/lrbONn4/9/dFhyNJ0vYadjn78BmHM6F1AnOXzbUvtSRJdWhc6zhO2fsUAP7wyB8KjkaSNFSGvEidUlqYUrotn14N3A/MBE4Hyk2nLgFeMdSxDYQT9zgRgDlPzSk4EkmSts9wzNmVfalt+SFJUn168d4vBuCPj/yx4EgkSUOl0J7UEbEHcDhwEzA9pbQwX7UImN7De86NiDkRMWfp0qVDE2g/HDL9EADuXnJ3wZFIkjRwhlPOti+1JEn1rTyS+prHrqG9s73gaCRJQ6GwInVEjAd+CbwvpbSqcl1KKQGp2vtSSheklGanlGZPmzZtCCLtn2dPfzYAdy2+q+BIJEkaGMMtZ5f7Ul/7+LXFBiJJkqrac8qe7LvDvqzYsIJbFtxSdDiSpCFQSJE6IlrITnYvTSn9Kl+8OCJm5OtnAEuKiG177TF5D8a3jmfRmkUsXVs/o8YkSdoWwzFnHz7jcHYYswOPLn+U2xbeVnQ4kiRtl+H4oGPYPJr6qrlXFRyJJGkoDHmROiICuAi4P6X0tYpVVwBn5NNnAJcPdWwDoRQljphxBAA3zL+h4GgkSdp2wzVnN5eaecshbwHgWzd/q+BoJEnabsPuQccAp+9/OgAX3HoB69vWFxyNJGmwFTGS+hjgzcCJEXFH/nop8EXgRRHxMHByPt+Qjt3tWACuf/z6giORJGm7DNuc/S/P+ReaS81cfMfF/PmxPxcdjiRJ22w4PugY4OS9TubIGUeyeO1ifnn/L4sOR5I0yIa8SJ1S+ltKKVJKh6SUDstfV6aUnkkpnZRS2jeldHJKadlQxzZQjtvjOAD++Ogf6UydBUcjSdK2Gc45e78d9+OTx34SgHdd+S7aOtoKjkiSpO03nB50HBG84eA3AD7sWJJGgsIenDicHbPrMUwbO417ltzDd+d8t+hwJElSFR855iPsPWVv7n/6fj589Ydp72wvOiRJkrbZcHvQMcALd38hAH994q8FRyJJGmwWqQfBmJYxfP3FXwfgJ/f8pOBoJElSNaOaR3HRaRcRBOfddB7vueo9RYckSdI2GY4POgY4fOfDGdcyjoeeeYgbnvSZT5I0nFmkHiQv2fclANyy4BY2dWwqOBpJklTNcXscx/dP/z4Al9x5CRvaNxQckSRJ/TNcH3QM0NLUwr/M/hcA3vK/b/GuJ0kaxixSD5IdxuzA/jvuz8aOjdyx6I6iw5EkST0447AzOHznw1nXto5rH7u26HAkSeqvYfugY4DPn/R59p6yN3OXzeVLf/sSWecSSdJwY5F6EB2z6zEA/GHuHwqORJIk9ea0/U8D4Of3/bzgSCRJ6p/h/KBjgNamVj7+wo8D8IlrP8Gnrv1UwRFJkgaDRepB9JqDXgPAD+/6oVd7JUmqY68/+PUA/PL+X7K+bX3B0UiSpEpnHnYm//3y/6YpmvjcXz/H1Y9cXXRIkqQBZpF6EJ2818nsPH5nHl72ML+6/1d9v0GSJBXigKkH8JxdnsOqjat43+/fV3Q4kiSpQkRw7pHn8tkTPgvAm379Ju5efHfBUUmSBpJF6kHUXGrmU8dmtyK966p3sWDVgoIjkiRJPfnOy77DqKZRXHDbBdy39L6iw5EkSd188Pkf5EV7vYgla5dw3MXHcdfiu4oOSZI0QCxSD7JzjzyX43Y/jkVrFnHUhUfx5Moniw5JkiRVMXuX2Zx12FkAfOa6z9iqS5KkOtPS1MIVb7iCf9jvH1i+YTmn/eQ0NrRvKDosSdIAsEg9yJpKTfzitb9g3x325anVT/Gju35UdEiSJKkH7z/6/YxuHs3P7/s5377520WHI0mSuhndPJqfveZnHDjtQOatnMfvHvpd0SFJkgaAReohMHXsVP7thf8GwG2Lbis4GkmS1JP9dtyPH7ziBwB89i+fZc2mNQVHJEmSuhvdPJqzDz8bgB/f8+OCo5EkDQSL1EPkyBlHAnDrU7cWHIkkSerNqw98NUfPOpql65byvdu/V3Q4kiSpitcd9DqC4HcP/Y4VG1YUHY4kaTtZpB4iz5r2LEY3j+axFY+xZO2SosORJEk9iAg+fMyHATjvxvPo6OwoOCJJktTdzIkzOX6P49nYsZFf3vfLosORJG0ni9RDpLnUzIl7ngjAf8/574KjkSRJvfmH/f6BvabsxWMrHuPyBy8vOhxJklTFWw59CwAf//PHuXfJvQVHI0naHhaph9AHj/4gAN++5dts6thUcDSSJKknTaUm3vfc9wHwgT98gKdWP1VsQJIkaStvPuTNHLf7cSxeu5ijLjyK6x+/vuiQJEnbyCL1EDp+j+M5eKeDWbJ2CVc8eEXR4UiSpF6cc8Q5PHfmc5m3ch6fvf6zRYcjSZK6aSo1cfnrL+d1B72OdW3reNmPX8a1j11bdFiSpG1gkXoIRQTnHnEuABfcekHB0UiSpN6MaRnDhaddCMCld1/Kmk1rCo5IkiR1N2n0JC79x0t58yFvZm3bWl78oxfz24d+W3RYkqR+skg9xN50yJsY3Tyaqx+9mrnL5hYdjiRJ6sXBOx3MMbsew+pNq/naDV8rOhxJklRFU6mJ75/+fd5z1Hto62zjNT9/DX974m9FhyVJ6geL1ENsypgpvO6g1wFwzhXn0NHZUXBEkiSpN1846QsA/Off/pP7l95fcDSSJKmaplIT5516Hm874m1saN/Aa37+GpatX1Z0WJKkGlmkLsAXT/4iO4/fmevnXc/P7v1Z0eFIkqReHLv7sZx52JlsaN/A237zNlJKRYckSZKqiAjOf9n5vGC3F7BozSI+8IcPFB2SJKlGFqkLsPP4nfncCZ8D4NPXfZr2zvaCI5IkSb35xqnfYNrYafz9yb/78GNJkupYU6mJi067iNHNo7nkzku46uGrig5JklQDi9QFecuhb2GfHfbh4WUPc/EdFxcdjiRJ6sXEURP55LGfBOCj13yUto62giOSJEk92W/H/fjsCZ8F4KzLz+K6x68rNiBJUp8sUhekpamFfz/+3wH40NUf4uFnHi44IkmS1Jt/nv3P7DVlLx54+gHe+Ks3sr5tfdEhSZKkHrz/ee/nhD1OYPHaxZx4yYm8//fv9yKzJNUxi9QFesPBb+Af9vsHVmxYwQu//0KeWv1U0SFJkqQetDa1ctmrL2N863h+ft/POec35xQdkiRJ6kFTqYk/vOkPfOrYT1GKEufddB4f/dNHiw5LktQDi9QFigh+/Kofc9zux7F47WLO/N8z6UydRYclSZJ6MHuX2Vx/5vW0lFr46T0/5ZFljxQdkiRJ6kFLUwv/fsK/c/Wbr6a51MzXbvwaH/zjB9nYvrHo0CRJ3VikLtj41vH85FU/YerYqVz96NWcd+N5RYckSZJ6ccSMI3jTIW+iM3XygT9+gJRS0SFJkqRenLDnCVx02kWUosRXb/gqs/9nNo+veLzosCRJFSxS14EZE2Zw0WkXAfCxaz7GXYvvKjgiSZLUm8+e8FkmjZrEFQ9ewct+/DLuW3pf0SFJkqRevOXQt/CXM//Cvjvsyz1L7uG4i4/j0eWPFh2WJClnkbpOnLb/afzzkf/Mpo5NnHPFOXR0dhQdkiRJ6sHMiTP58at+zOjm0Vw19ype/KMXs2jNoqLDkiRJvThmt2OYc+4cjp51NE+sfILjLz7e1l2SVCcsUteR/3rRfzFzwkxueeoWvnXzt4oOR5Ik9eKl+76Ue99xLwdNO4j5q+Zz6HcP5YmVTxQdliRJ6sXEURP5/Zt+z/N3fT5PrnqS4y85noefebjosCRpxLNIXUcmjprId172HQA+fPWHufaxawuOSJIk9WavKXtx1Ruv4sgZR7Jk7RLedeW7fAiyJEl1buKoifz+jb/nhbu9kPmr5nPsxcdy/ePXFx2WJI1oFqnrzGn7n8b7nvs+2jrbeOVlr+SeJfcUHZIkSerFrpN25fLXX86E1gn85qHf8NE/fbTokCRJUh8mjJrAVW+8ipP2PIlFaxZx/CXH88W/fbHosCRpxLJIXYe++uKv8qpnvYqVG1dywiUncOldlxYdkiRJ6sXMiTP51et+RXOpmS//35e57J7Lig5JkiT1YVzrOK5641V88thPEgQfu+ZjvO4Xr/OBipJUAIvUdagUJX74yh9y4p4n8vS6p3nTr9/E5//y+aLDkiRJvTh5r5P52ilfA+Csy8/iGzd+wwchS5JU51qaWviPE/6D7778u4xuHs3P7v0ZB3z7AP71D//KsvXLig5PkkYMi9R1akzLGK5+89V892XfpRQlPnHtJ/jFfb8oOixJktSLdx31Lt5+5NtZ376e9/3hfbzohy/imXXPFB2WJEnqw7lHnstD73qIMw49g/bOdr5249fY+5t7c9blZ3H1I1ezqWNT0SFK0rBmkbqOlaLEP8/+Z/7r5P8C4HW/eB3vvvLdrG9bX3BkkiSpmojg/Jefz+Wvv5ydx+/MtY9fy1EXHsXdi+8uOjRJktSHXSftysWvuJhbz72VE/c8kRUbVnDxHRdzyo9OYZev7sIn/vwJlq9fXnSYkjQsWaRuAB84+gN8+PkfJgi+fcu3eeVlr/QqriRJdey0/U/jlrfdwpEzjuTR5Y9y2H8fxjt+9w7WbFpTdGiSJKkPh884nD+9+U/ccPYNfPLYT7LfjvvxzPpn+PxfP8+e39iTV/z0FXzxb1/kb0/8zdZekjRAIqVUdAzbbPbs2WnOnDlFhzFkbl94Oy/+0YtZum4pJ+xxAj97zc+YOnZq0WFJ0rAWEbemlGYXHUejG2k5u2xd2zr+9Q//yoW3X0h7Zzt7Tt6Tr57yVV6y70sY3Ty66PAkadgwXw+MkZqv+5JS4v+e/D8+es1H+dsTf9ti3U7jduJVz3oV/3r0v7L3DnsXFKEkNYbe8rVF6gZzx6I7eMmlL2HRmkU0l5o549AzOOeIc3jerOcVHZokDUue9A6MkZizK9256E7OvPxM7lh0BwDjW8fzygNeyedP/Dy7Ttq12OAkaRgwXw+MkZ6v+5JS4uFlD3PT/Ju4Yf4N/OGRP/Do8kcBaCm18OZD3szJe53McXscxy4Tdik4WkmqPxaph5l5K+Zxxv+ewV/m/YVE9vN7+X4v52unfI19d9y34OgkaXjxpHdgjNScXWlTxya+ddO3uPTuS7l90e1AVqx+53PeyQeO/gA7jdup4AglqXGZrweG+bp/UkrcsegOvnHTN7jkzku2WLfPDvswe5fZPGeX5/BPz/4ndh6/c0FRSlL9sEg9TD349INccOsFXHj7hazauIogOP2A03nvc9/LcbsfR0QUHaIkNTxPegfGSM/Z3T2y7BE+8qeP8Mv7fwnAmOYxnLTXSRy1y1G8/uDXe9FZkvrJfD0wzNfb7p4l9/Dbh37L9fOu529P/G2L51A0l5o5cNqBPH/W8zlw2oHsP3V/Dpp2EDMnziwwYkkaehaph7mnVj/Fp679FD+864ddD1Tcd4d9OeeIczjzsDMdmSVJ28GT3oFhzq7uhidv4PN//Ty/e/h3Wyw/etbRnL7/6Zy2/2kcMPUALzxLUh/M1wPDfD0w2jvbuWPRHdy56E5+89Bv+O1Dv6Ujbf2Axd0n7c4LdnsBR808ir2n7M1eU/Zizyl7+twKScOWReoRYtGaRXznlu9w0e0X8dTqp4Dsiu1L9nkJh+18GLtP2p3nznouB007yJNdSaqRJ70Dw5zdu3kr5nX1trzsnstY376+a93+O+7PKw94Jc+e/mwOnHYgB047kNam1gKjlaT6Y74eGObrwbFyw0ruWnwXtzx1Cw8+/SAPPPMAdyy6g1UbV221bVM0ccDUA9h98u7sv+P+7DVlL2ZNnMVuk3Zj90m7s8OYHTyfl9SwLFKPMO2d7Vz18FX8z23/w+8e/h2dqXOL9dPHTeeEPU/gsOmHccDUAzhg6gHsNWUvWppaCopYkuqXJ70Dw5xduzWb1vCHuX/gioeu4LcP/ZZl65dtsb6l1MKB0w7ksJ0P63odOv1QpoyZUlDEklQ88/XAMF8PnY7ODu5Zcg9/feKv3L34bh5b8RiPLn+Ux1Y8ttU5fKWJoyay+6Td2Xn8zuw8fmemj5vOtHHTmDZ2GlPHTu2anjZuGhNaJ1jQllRXGqpIHRGnAt8AmoALU0pf7GlbE2jfFqxawDWPXcNDzzzEo8sf5brHr2PhmoVbbdcUTew2aTf2nLIne03ei72m7MXuk3dn90m7s+ukXZk+bjqjmkcV8B1IUrE86a2uP/kazNnbqr2znesev44/PfonHl72MHcvvpu5y+Z2PTi50vjW8UwfN52dxu3EtHHTmDpmatfJ6tSx2fSE1gmMbRnLuNZx2deWcYxrHceY5jGexEpqaObr6szXjWdd2zruW3of81fN54GnH+DxFY8zf9V85q2cx7wV81i9aXXN+2ptas3+FsiL1pNGTWLiqIlMHDWRCa0Tuqa7lo3aepl3b0kaSA1TpI6IJuAh4EXAfOAW4A0ppfuqbW8C7b+UEg88/QB/feKv3L/0fu5/+n4eePoB5q2c1+d7x7eOZ+rYqew4ZkemjJnC2Jax2at5bNf0mJYxm5e3jGVU0yham1ppaWrJvpZatpjuvq7afFOpaQiOjCRV50nv1vqbr8GcPZDWbFrD3Yvv5o5Fd2T9Lhffyd1L7mZd27rt2m+5aF0uYldOdxW0e1jffbq51ExTqYmmaOr1aylKva6TpFqZr7dmvh5+Uko8s/4Znlj5BIvXLGbRmkUsXruYp9c9zdJ1S1m6dilL1y3N5tcuZW3b2u3+zNamViaOmsjo5tGMahrFqOZRXV+rLmsavcX8qKZ8u27Lyn8rVP4tUJ4uRanrb4Ly9EBvV572Ir00tHrL181DHUwfjgLmppQeBYiInwKnAz0mUfVPRPCsac/iWdOetcXyDe0beGLlE9ntRcsf45Hlj/DEyie6XkvWLmHNpjWs2bSGx1c8PrQxE7Q0tdBSaulKIkEQEdl8Ph3Edq3vb0z92r4f+2/UfW/L/qXB0trUys1vu7noMIYz83WBxreO5+hdj+boXY/uWpZSYtXGVSxas6jrJPXpdU93nbSWp9e2rWXtprWsbVvLurZ1XdMb2jewrm3ddhe6B1pvRe3K/F7OV92X1bKunLsqpwfKYJz4GuMA7XMExlivvnLKVzh5r5OLDmO4Ml8PMxHRdXdULda3rd/i74JVG1dt8Vq9aXWfyzZ1bOLpdU8P8ndWnHKdoKdid+XfEcBWfzv0Z115flvX1fT91JiPitrfYOxzMGLU1k7a8yS++uKvDupn1FuReibwZMX8fOC5lRtExLnAuQC77bbb0EU2zI1uHs1+O+7HfjvuV3V9+eS3fJK7fMNy1retZ337+q6T2vVt2XTlso0dG9nUsYm2jrbsa2dbv+Y3dWwikbqmJakWo5psTzTI+szXYM4eShHBpNGTmDR6Evuzf7/f35k6u4rW69rWdRWz+5ruaX17ZzsdqYOOzo5ev3amzh7XAdl8R8dAHy5JdWTlhpVFhzCcma9HuDEtY9ht0m7sNmnbfq4pJTa0b2D1ptVsaN/AxvaNbOzYuMV0b8s2tufLK5fl0+2d7dnfAXnuL09X/m1QbbrW99SyXWfqJJH8e0Oqwd477D3on1FvReo+pZQuAC6A7FakgsMZMSpPfofiF7NSR2cHbZ1ttHW0kUhZIkmJRCKl1JVYysu2ZX1/VOsD2uv2/Wip06j73pb9S4PJq+T1wZzdOEpRYnzreMa3ji86lC7VCtiVyypzObBFrq91XTl3VU4PlMFoqWeMA7TPERhjPdtj8h5FhzDima/Vk4hgTMsYxrSMKTqUQVHO/70VuIGqfy9U/o1Ry7ry/Lauq/X7qWm7gvY3GPscjBhV3aRRkwb9M+qtSL0A2LVifla+TCNYUym7rXd08+iiQ5EkZczXGnSlKFFqKtFCS9GhSFKjMl9LvehqC+rfG1JdqLcn0twC7BsRe0ZEK/B64IqCY5IkSVsyX0uSVP/M15KkhlFXI6lTSu0R8S7gD0AT8L2U0r0FhyVJkiqYryVJqn/ma0lSI6mrIjVASulK4Mqi45AkST0zX0uSVP/M15KkRlFv7T4kSZIkSZIkSSOIRWpJkiRJkiRJUmEsUkuSJEmSJEmSCmORWpIkSZIkSZJUGIvUkiRJkiRJkqTCWKSWJEmSJEmSJBXGIrUkSZIkSZIkqTAWqSVJkiRJkiRJhbFILUmSJEmSJEkqjEVqSZIkSZIkSVJhLFJLkiRJkiRJkgoTKaWiY9hmEbEUmDdAu5sKPD1A+xruPFa18TjVzmNVO49V7QbqWO2eUpo2APsZ0QYwZ/tvoHYeq9p5rGrnsaqNx6l25us64jl2ITxOtfNY1c5jVTuPVW0GPV83dJF6IEXEnJTS7KLjaAQeq9p4nGrnsaqdx6p2HqvhyZ9r7TxWtfNY1c5jVRuPU+08VsOXP9vaeJxq57Gqnceqdh6r2gzFcbLdhyRJkiRJkiSpMBapJUmSJEmSJEmFsUi92QVFB9BAPFa18TjVzmNVO49V7TxWw5M/19p5rGrnsaqdx6o2HqfaeayGL3+2tfE41c5jVTuPVe08VrUZ9ONkT2pJkiRJkiRJUmEcSS1JkiRJkiRJKoxFakmSJEmSJElSYUZ8kToiTo2IByNibkR8tOh4ihYR34uIJRFxT8WyHSLi6oh4OP86JV8eEfHN/NjdFRFHFBf50IuIXSPi2oi4LyLujYj35ss9Xt1ExOiIuDki7syP1b/ny/eMiJvyY3JZRLTmy0fl83Pz9XsU+g0MsYhoiojbI+K3+bzHqYqIeDwi7o6IOyJiTr7Mf3/DmDl7S+bs2piva2e+7h/zdW3M1yOP+XpL5uvamK9rZ77uP3N2bYrO2SO6SB0RTcD/A14CHAi8ISIOLDaqwl0MnNpt2UeBa1JK+wLX5POQHbd989e5wPlDFGO9aAf+NaV0IPA84J3574/Ha2sbgRNTSocChwGnRsTzgC8BX08p7QMsB87Otz8bWJ4v/3q+3UjyXuD+inmPU89OSCkdllKanc/772+YMmdXdTHm7FqYr2tnvu4f83XtzNcjhPm6qosxX9fCfF0783X/mbNrV1jOHtFFauAoYG5K6dGU0ibgp8DpBcdUqJTSX4Bl3RafDlyST18CvKJi+Q9S5kZgckTMGJJA60BKaWFK6bZ8ejXZf3gz8XhtJf+e1+SzLfkrAScCv8iXdz9W5WP4C+CkiIihibZYETELeBlwYT4feJz6w39/w5c5uxtzdm3M17UzX9fOfL3d/Pc3fJmvuzFf18Z8XTvzdf+Ys7fbkP0bHOlF6pnAkxXz8/Nl2tL0lNLCfHoRMD2f9vjl8ltADgduwuNVVX57zR3AEuBq4BFgRUqpPd+k8nh0Hat8/UpgxyENuDjnAR8GOvP5HfE49SQBf4yIWyPi3HyZ//6GL3+GtfHfQC/M130zX9fsPMzXtTJfjyz+DGvjv4FemK/7Zr7ul/MwZ9eq0JzdvD1v1siTUkoRkYqOo55ExHjgl8D7UkqrKi+yebw2Syl1AIdFxGTg18ABxUZUfyLi5cCSlNKtEXF8weE0gheklBZExE7A1RHxQOVK//1ppPPfwJbM17UxX/fNfN1v5mupF/4b2JL5ujbm69qYs/ut0Jw90kdSLwB2rZiflS/TlhaXh+znX5fky0f88YuIFrIEemlK6Vf5Yo9XL1JKK4BrgaPJbgcpXyyrPB5dxypfPwl4ZmgjLcQxwGkR8TjZrZEnAt/A41RVSmlB/nUJ2R9mR+G/v+HMn2Ft/DdQhfm6/8zXvTJf94P5esTxZ1gb/w1UYb7uP/N1n8zZ/VB0zh7pRepbgH0je6pnK/B64IqCY6pHVwBn5NNnAJdXLH9LZJ4HrKy4BWDYy/sSXQTcn1L6WsUqj1c3ETEtv8JLRIwBXkTWY+xa4NX5Zt2PVfkYvhr4c0pp2F8xTyl9LKU0K6W0B9n/R39OKb0Rj9NWImJcREwoTwOnAPfgv7/hzJxdG/8NdGO+rp35ujbm69qZr0ck83Vt/DfQjfm6dubr2pmza1cXOTulNKJfwEuBh8j693y86HiKfgE/ARYCbWT9ZM4m679zDfAw8Cdgh3zbIHty8yPA3cDsouMf4mP1ArJ+PXcBd+Svl3q8qh6rQ4Db82N1D/CpfPlewM3AXODnwKh8+eh8fm6+fq+iv4cCjtnxwG89Tj0en72AO/PXveX/v/33N7xf5uytjoc5u7bjZL6u/ViZr/t/zMzXvR8f8/UIfJmvtzoe5uvajpP5uvZjZb7etuNmzu79+BSesyPfsSRJkiRJkiRJQ26kt/uQJEmSJEmSJBXIIrUkSZIkSZIkqTAWqSVJkiRJkiRJhbFILUmSJEmSJEkqjEVqSZIkSZIkSVJhLFJLI0BEpIj4asX8ByPiMwWGJEmSujFfS5JU/8zX0uCwSC2NDBuBf4yIqUUHIkmSemS+liSp/pmvpUFgkVoaGdqBC4D3Fx2IJEnqkflakqT6Z76WBoFFamnk+H/AGyNiUtGBSJKkHpmvJUmqf+ZraYBZpJZGiJTSKuAHwHuKjkWSJFVnvpYkqf6Zr6WBZ5FaGlnOA84GxhUchyRJ6tl5mK8lSap352G+lgaMRWppBEkpLQN+RpZIJUlSHTJfS5JU/8zX0sCySC2NPF8FfAqxJEn1zXwtSVL9M19LAyRSSkXHIEmSJEmSJEkaoRxJLUmSJEmSJEkqjEVqSZIkSZIkSVJhLFJLkiRJkiRJkgpjkVqSJEmSJEmSVBiL1JIkSZIkSZKkwlikliRJkiRJkiQVxiK1JEmSJEmSJKkwFqklSZIkSZIkSYWxSC1JkiRJkiRJKoxFakmSJEmSJElSYSxSS5IkSZIkSZIKY5FakiRJkiRJklQYi9SSJEmSJEmSpMJYpJYkSZIkSZIkFcYitSRJkiRJkiSpMBapJUmSJEmSJEmFsUgtSZIkSZIkSSqMRWpJkiRJkiRJUmEsUkuSJEmSJEmSCmORWpIkSZIkSZJUGIvUkiRJkiRJkqTCWKSWJEmSJEmSJBXGIrUkSZIkSZIkqTAWqSVJkiRJkiRJhbFILUmSJEmSJEkqjEVqSZIkSZIkSVJhLFJLkiRJkiRJkgpjkVqSJEmSJEmSVBiL1JIkSZIkSZKkwlikliRJkiRJkiQVxiK1ahYRB0TEzRFxS0TcEREXRcTYouMaziKiKSI+HBH/FxG3RcTbBmi/4yPi/Ih4JN/vreV9R8QeEbE+/xnfmX/2/gPxuY0gIiZHxDsG+TMiIr4ZEXMj4q6IOKLKNmMj4ncR8UBE3BsRXxzMmCQNL+bsoWfOHnr1krPz7T4fEU9GxJrBjEdS4zNHDz1z9NCrsxx9ZETcnW/3zYiIfPlnImJB/jO6IyJeOpjxqm8WqdUfC4FTUkrPSSkdBqwC3ldoRMPfZ4BxwEkppSNSSv8zQPu9EFgO7JtSOgI4FdihYv0jKaXDUkqHApcA/zZAn9sIJgODmkyBlwD75q9zgfN72O4rKaUDgMOBYyLiJYMcl6Thw5w99D6DOXuoTaZ+cvZvgKMGORZJw4M5euh9BnP0UJtM/eTo84G3VWx7asW6r+c/o8NSSlcOZrDqm0Vq1SyltDKltAIgIkrAaKA8/7b8SvCdEfHL8pXgiLg4Ih6LiHvyK1sH58uvi4jZ+fTnKkedRMRH8qtcd5ZHj/a0fUQcHxEpIk7N56fkVys/k88fFhE35p/964iYki/fJyL+lH/GbRGxd0Rcml89W5bHfEdEvD0izoyIb/d2bCJidER8P4/79og4IV/e9d6ImB0R1+XTzRHxdMX38Nt8eoeIWBERH8x3/UbghcDNEXFNROxWcVxfnU+fX/H9Vi4/Jz82U7vFujfZSdQnUkqd+c92aUrpSz18exPJEm/37zki4sv5z/buiHhdvrzqcez23srveWpEPF6xbn5EjM+n/zeyq9H3RsS5Fdt0VFzt/FO+rOrvYG8i4kP5e+6KiH/PF38R2Dvf95cjuzp+Tf57cndEnN7XfmtwOvCDlLkRmBwRMyo3SCmtSyldm09vAm4DZg3AZ0saAczZPTNnm7P7qc+cDZBSujGltHAAPk/SMGeO7pk52hzdT33m6Hx+Yp6nE/AD4BUD8NkaBBap1S8RMSYi7gCWAocC5SuQv8qvBB8K3A+cXfG2D6WUDgb+ApzYbX87ASdVzL+E7D+a5+b7+q/ets/dBrwln/4n4M6KdT8APpJSOgS4G/h0vvxS4P/ln/F8YGFK6Y35lewr8pgPSyl9t++jAsA7gZRSejbwBuCSiBhd43srfQx4omJ+T+CSfL+XAt+s3DgiPgWUUkqf6bZ8NPB2YEmVzzgIuLOcSHtQTiaPAB8AvlZlm38EDiP7PTgZ+HJEzNjO49jdW1NKRwKzgfdExI758vUVVztPzpdV/R2MiNMi4j+67zgiTiG7inpU/n0cGRHHAh9l8xXvDwEbgFfmV8ZPAL4akd0e1G1/l1Uk+MrXW7pvC8wEnqyYn58vqyoiJgP/AFzT0zaS1J05u0fmbHP2oOVsSaqFObpH5mhz9EDn6Jn58p62eVdeXP9e5BdfVByL1OqXlNL6/D/K6WRJ6+P5qoMj4q8RcTfZVcqDKt725Yh4GDgN+Hm3XX4S+ELF/MnA91NK6/LPW9bH9pDdLjUqInbIP+MKgIiYBExOKV2fb3cJcGxETABmppR+nX/GhvLn9eJ1+X+Mt0TEy6usfwHwo3x/DwDzgP362OcWImIm8Dzg1xWLO4Ef59M/zD+n7Eyy4//JKrt7J9n3u76Gz/14/r09VbG4nEz2Jrv17IIqb30B8JOUUkdKaTFwPfCcvj6vn94TEXcCNwK7kiW/nlT9HUwpXZFS+lSV7U/JX7eT/UF2QA/7D+ALEXEX8CeyhDa9+0YppddVJPjK1w9q/m6rfXhEM/AT4JsppUe3Z1+SRhZztjm7gjm7wmDlbEmqlTnaHF3BHF1hiHP0+cDeZMX1hcBXB+Ez1A8WqbVNUkrtwE/Z/J/nxcC78iuT/052y1LZh1JK+wL/ka8r2wM4OKX0mxo/trftfwx8CXgQ2FTj/vrjsvyPiH8C/nsQ9g/Z1ejPAqli2epett8BeD/wlW7LJwKvp+c47wMOjezWMlJKn8+/t4k9bH8FcGyvkQ+CiDie7I+ro/OruLez5e9VdxfT8+9g1Y8A/rMi6e2TUrqoynZvBKYBR+bHaXG1fffziu8Csj8Oymbly6q5AHg4pXReH9+PJFVlzh4U5uwK5mxJ2jbm6EFhjq5gjmYBW7bN7NompbQ4vzjQSXY3g8+WKJhFatUsIvaNzb2bguzq6s356gnAwohoIfvPp5pVQGUfp0+z+TahsquBs2Jz760d+ti+7DdkD5f7XnlBSmklsDwiXpgvejNwfUppNTA/Il6Rf8aoqP1pysuA5irL/0r+fUfEfsBuZIm9VnsDe6SU/tht+S1kiZF8/3+tWPe1lNJ3gF3yW2zK3g98K2V9jLeSUpoLzAE+FxFNecyjyZJLNS8AHqmy/K9kV8KbImIaWcK9ucp222oSsDyltC4iDiC7Gt6bWn4HK/0BeGts7tM1M7/tbXW+r8o4lqSU2iLribZ7tZ3184rvFcBbIvM8YGWq0scyIj6Xf/77avh+JKmLORswZ1cyZ1cYjJwtSbUyRwPm6Erm6AoDnaPz+VUR8bz839tbgMvzWCv7V78SuKeG71eDqNp/ClJPxgOXRkRrPn898J/59CeBm8h6at3Elv8ZfTkiPkF2JfOciuXzU0p/qfyAlNLvI+IwYE5EbAKuZPMTcLfavuJ9m8j6KxERJ1esOgP4bp4sHwXOype/GfjvyHoqtQGvydf35B/zuMYDH6qy/jvA+ZHdEtMOnJlS2pj9H7jFe/eMiL9Vef8BFbFVehdwUUR8iKwP1lurbPPPwBURUb76HuS3SPXiHODLwNyIeIbs9qUPV6zfO7IeaUF2Bf2crfaQ3T51NNntaQn4cEppUR+fW+n5+bFoBnauOC7T8q+/B94eEfeT/WFyYx/7q/o7GBGnAbO735qUUvpjRDwLuCH/Oa0B3pRSeiQi/h4R9wBXkY0k+E3+s50DPNCP77EnVwIvBeYC66j42UfEHSmlwyJiFtltZw8At+UxfjuldOEAfL6k4c+cbc6uZM7edn3m7Hz6v8hGBo6NiPnAhalbb1NJypmjzdGVzNHbrqYcDbyDbIT4mDyWq/Ll/5X/TiXgcbLfARUoUkp9byVJkiRJkiRJ0iCw3YckSZIkSZIkqTAWqSVJkiRJkiRJhbFILUmSJEmSJEkqjEVqSZJGuIjYNSKujYj7IuLeiHhvvnyHiLg6Ih7Ov07Jl0dEfDMi5kbEXRFxRLHfgSRJkiSpkTX0gxOnTp2a9thjj6LDkCQNY7feeuvTKaVpfW/ZuCJiBjAjpXRbREwAbgVeAZwJLEspfTEiPgpMSSl9JCJeCryb7GnazwW+kVJ6bm+fYc6WJA2mkZCvh4L5WpI0mHrL181DHcxA2mOPPZgzZ07RYUiShrGImFd0DIMtpbQQWJhPr46I+4GZwOnA8flmlwDXAR/Jl/8gZVe6b4yIyRExI99PVeZsSdJgGgn5eiiYryVJg6m3fG27D0mS1CUi9gAOB24CplcUnhcB0/PpmcCTFW+bny/rvq9zI2JORMxZunTp4AUtSZIkSWpoFqklSRIAETEe+CXwvpTSqsp1+ajpfvUISyldkFKanVKaPW2ad2BLkiRJkqqzSC1JkoiIFrIC9aUppV/lixfn/arLfauX5MsXALtWvH1WvkySJEmSpH6zSC1J0ggXEQFcBNyfUvpaxaorgDPy6TOAyyuWvyUyzwNW9taPWpIkSZKk3jT0gxMlSdKAOAZ4M3B3RNyRL/s34IvAzyLibGAe8Np83ZXAS4G5wDrgrCGNVpIkSZI0rFikliRphEsp/Q2IHlafVGX7BLxzUIOSJEmSJI0YtvuQJEmSJEmSJBXGIrUkSZIkSZIkqTAWqSVJkiRJkiRJhbFILUmSJElSnYuIXSPi2oi4LyLujYj35st3iIirI+Lh/OuUfHlExDcjYm5E3BURRxT7HUiS1DOL1JIkSZIk1b924F9TSgcCzwPeGREHAh8Frkkp7Qtck88DvATYN3+dC5w/9CFLklQbi9SSJEmSJNW5lNLClNJt+fRq4H5gJnA6cEm+2SXAK/Lp04EfpMyNwOSImDG0UUuSVBuL1JIkSZIkNZCI2AM4HLgJmJ5SWpivWgRMz6dnAk9WvG1+vqz7vs6NiDkRMWfp0qWDF7QkSb2wSJ1LKRUdgiRJ6kNKic7UWXQYkiQVJiLGA78E3pdSWlW5LmUntv06uU0pXZBSmp1Smj1t2rQBiTGlxKaOTQOyL0nSyDDii9Tr29bT/B/NjP3C2KJDkSRJvXj1z15N6T9K/ObB3xQdiiRJhYiIFrIC9aUppV/lixeX23jkX5fkyxcAu1a8fVa+bFD9/N6f0/q5Vs66/KzB/ihJ0jAy4ovUzaVmOlIHbR1tRYciSZJ60VxqBmB9+/qCI5EkaehFRAAXAfenlL5WseoK4Ix8+gzg8orlb4nM84CVFW1BBs241nG0d7azbP2ywf4oSdIw0lx0AEUrn/B2pA5SSmR5X5Ik1ZsxLWOA7C4oSZJGoGOANwN3R8Qd+bJ/A74I/CwizgbmAa/N110JvBSYC6z7/+zdd5ycZb3///dnZrb33Wx6LwQSSgIhFAGlg4UIIkWlCEdAwYbHYzt6jp6fyldR0aOCIBx6U6RIR6q0YICQ3gvZTbLZJJvtfa/fH/fMZnazfWfmnt19PR+P+3Hfc037ZCz3znuu+3NJSsjU5oL0AkkipAYA9MuID6nNTEELqtW1qqWtRSnBFL9LAgAAXcgIeSF1XXOdz5UAAJB4zrnXJXU3q+rULh7vJF0b16K6UJhRKImQGgDQPyO+3Yek9mC6uY2WHwAAJKvMFG/9CNp9AACQvAipAQADEbeQ2szuMLNdZrYiauwhM1sa3rZELlEys6lmVh913y3xqqsrKYFwSE1fagAAklZkJjXtPgAASF4FGV67j4r6CrW5Np+rAQAMFfFs93GnpN9Lujsy4Jy7MHJsZr+SVBn1+I3OuXlxrKdbkZnULW0tfrw9AADog0hPatp9AACQvEKBkHLTclXVWKWqxirlp+f7XRIAYAiI20xq59xrkrq8vie8KvEFkh6I1/v3R2TxRNp9AACQvGj3AQDA0EDLDwBAf/nVk/pESWXOufVRY9PM7H0ze9XMTuzuiWZ2lZktMbMl5eXlMSmGdh8AACQ/2n0AADA0FKR7LT8IqQEAfeVXSH2xOs6i3iFpsnNuvqTrJd1vZrldPdE5d6tzboFzbkFxcXFMimHhRAAAkl+k3QczqQEASG7MpAYA9FfCQ2ozC0k6T9JDkTHnXKNzbk/4+F1JGyUdlKiaIu0+6EkNAEDyisykpic1AADJjZAaANBffsykPk3SGudcSWTAzIrNLBg+ni5plqRNiSqIdh8AACQ/elIDADA0EFIDAPorbiG1mT0g6S1Js82sxMyuDN91kQ5cMPEkScvMbKmkv0q6xjmXsLMZ7T4AAEh+7e0+6EkNAEBSi4TUFfUVPlcCABgqQvF6Yefcxd2MX97F2COSHolXLb2JzKSm3QcAAMmLdh8AAAwNzKQGAPSXXwsnJpVIT2rafQAAkLxo9wEAwNBQkF4gSdrbQEgNAOgbQmrR7gMAgKGAdh8AAAwNzKQGAPRX3Np9DBmtrTpqXY1CO5hJDQBAMqPdBwAAQwMhNQCgv5hJ3dysm25YqmfupSc1AADJLDMlU3JSPSE1AABJjZAaANBfhNQpXquP1FapubXJ52IAAEB38i69Si0/kU5ZXut3KQAAoDslJZrw9Os6aYtUUV/hdzUAgCGCkDoYVKt5H0RLU6Pf1QAAgG4EQyEFnRRqaJJzzu9yAABAV954Q4VXfEXXvePNpOacDQDoC0JqSS0h72Noa2zwuRIAANAdy8qWJGU1SQ0tnLMBAEhKxcWSpDF1psbWRtW3sOAxAKB3hNSSWlOC3r6JL7wAACSt7HBI3Sy+8AIAkKwiIXW99z2bvtQAgL4gpNb+mdStDXzhBQAgaWVlSZKym6T6Zs7ZAAAkpXBIParWa/NBSA0A6AtCakmtIe8XXkdPagAAklc4pM5qkuqa63wuBgAAdKmoSJKUX9sqayOkBgD0DSG1pFZ6UgMAkPwiITXtPgAASF4pKVJBgYJOKqyXKuor/K4IADAEEFJLagvPpCakBgAgiUXNpKbdBwAASSzc8qO4jpnUAIC+IaSW1JoSkkS7DwAAklrUwom0+wAAIIlFQupaQmoAQN8QUktqTYn0pG7yuRIAABLPzO4ws11mtiJq7CEzWxretpjZ0vD4VDOrj7rvloQVGr1wIu0+AABIXsykBgD0U8jvApJBW8j7GNoamUkNABiR7pT0e0l3RwaccxdGjs3sV5Iqox6/0Tk3L1HFtYtq91FBuw8AAJIXM6kBAP3ETGpJbeF2H6LdBwBgBHLOvSapy2+QZmaSLpD0QEKL6krUwom0+wAAIIlFzaSuaGDhRABA7wipJblwuw/R7gMAgM5OlFTmnFsfNTbNzN43s1fN7MTunmhmV5nZEjNbUl5ePvhKohdOpN0HAADJa9QoScykBgD0HSG1pLaUFEksnAgAQBcuVsdZ1DskTXbOzZd0vaT7zSy3qyc65251zi1wzi0oDs+oGpSohRPrafcBAEDyoic1AKCfCKkluUhI3dzscyUAACQPMwtJOk/SQ5Ex51yjc25P+PhdSRslHZSQgqIWTqTdBwAASYye1ACAfiKkluRSvZDaaPcBAEC00yStcc6VRAbMrNjMguHj6ZJmSdqUkGpo9wEAwNAQNZN6T/0en4sBAAwFhNSSXPvCiYTUAICRx8wekPSWpNlmVmJmV4bvukgHLph4kqRlZrZU0l8lXeOcS8wUqYwMSVJmi9TQyExqAACSVlRIXdNUo4aWBp8LAgAku5DfBSSFlMhMatp9AABGHufcxd2MX97F2COSHol3TV0KBNScnqqUhia11Fb5UgIAAOiDcEg9qk6Sk8pryzUpb5K/NQEAkhozqSW51FTvgJ7UAAAktZbMNElSW021z5UAAIBupadL2dlKbZXyGqTyunK/KwIAJDlCaql9JnWAkBoAgKTWmpEuSXKE1AAAJLeolh/ltYTUAICeEVJLUvtM6hZ/6wAAAD1qzfT6Uqum1t9CAABIMDO7w8x2mdmKqLGHzGxpeNsSXjNCZjbVzOqj7rsl4QVHQupaZlIDAHpHT2pJSvNCamZSAwCQ3FxmpndQS0gNABhx7pT0e0l3RwaccxdGjs3sV5Iqox6/0Tk3L1HFHYCZ1ACAfiCklmSpXn/LADOpAQBIblleSG119T4XAgBAYjnnXjOzqV3dZ2Ym6QJJpyS0qJ4wkxoA0A+0+5D296RuIqQGACCZuexsSVKQkBoAgGgnSipzzq2PGptmZu+b2atmdmLCK2ImNQCgH5hJLSmQ5i3CFGghpAYAIJkFwiF1oK7O50oAAEgqF0t6IOr2DkmTnXN7zOwoSY+Z2VznXFXnJ5rZVZKukqTJkyfHrqJwSD2qTlrNTGoAQC+YSS3JUr2Q2phJDQBAUgtk50qSgnUNPlcCAEByMLOQpPMkPRQZc841Ouf2hI/flbRR0kFdPd85d6tzboFzbkFxOFiOCdp9AAD6gZBaUiAt3JOamdQAACS1UCSkrm/0uRIAAJLGaZLWOOdKIgNmVmxmwfDxdEmzJG1KaFW0+wAA9AMhtaRAerjdBwsnAgCQ1FJy8709ITUAYIQxswckvSVptpmVmNmV4bsuUsdWH5J0kqRlZrZU0l8lXeOc25uwYiVmUgMA+oWe1JKCaRnevqXV50oAAEBPgjl5kqS0xla1tLUoFOBPGQDAyOCcu7ib8cu7GHtE0iPxrqlHUTOp9zXsU3Nrs1KCKb6WBABIXsyklhRI9dp9BJsJqQEASGYWXjgxu0mqbar1uRoAANCt0aMlSWNqJTlpd91uf+sBACQ1QmpFz6Ru87kSAADQo6wsb9ck1TYTUgMAkLSysqScHKW3SPkNtPwAAPSMkFpSKD1TEu0+AABIepGQupmZ1AAAJL1x47xdNYsnAgB6RkgtKRgOqVOYSQ0AQHKLmkld01TjczEAAKBHkZC6hpnUAICeEVJLCmV4IXWomZAaAICkFu5JndVMuw8AAJLe2LGSmEkNAOhd3EJqM7vDzHaZ2Yqosf82s1IzWxrePh513/fMbIOZrTWzM+NVV1dCGd6srFRmUgMAkNzCM6lZOBEAgCGAmdQAgD6K50zqOyWd1cX4b5xz88Lb05JkZnMkXSRpbvg5fzSzYBxr6yASUqe0uES9JQAAGIjwTOpsFk4EACD5hUPqsTXMpAYA9CxuIbVz7jVJe/v48EWSHnTONTrnNkvaIGlhvGrrLBgOqdObpdY2Fk8EACBp5eR4u0Z6UgMAkPSiF05kJjUAoAd+9KS+zsyWhduBFITHJkjaFvWYkvDYAczsKjNbYmZLystjc5Kz9HRJUlqr1NzWHJPXBAAAcRAJqWn3AQBA8ov0pK6Rdtft9rkYAEAyS3RIfbOkGZLmSdoh6Vf9fQHn3K3OuQXOuQXFxcWxqSotzdu1SM2thNQAACStcLuPnEaplpnUAAAkN2ZSAwD6KKEhtXOuzDnX6pxrk3Sb9rf0KJU0KeqhE8NjiREOqdNbpKbWpoS9LQAA6KdQSE1pIQUkNVVV+F0NAADoSfTCifSkBgD0IKEhtZmNi7p5rqQV4eMnJF1kZmlmNk3SLEnvJKywqHYfhNQAACS35kzvvN2yj5AaAICkVlgol5qqvEaprnK32lyb3xUBAJJUKF4vbGYPSPqYpFFmViLpvyR9zMzmSXKStki6WpKccyvN7GFJqyS1SLrWOZe4FQyj2n3sJaQGACCpNWdlSBU1aquu9LsUAADQEzPZ2LHShx9qdLXT3vq9GpU5yu+qAABJKG4htXPu4i6Gb+/h8T+V9NN41dOj1FRv1yY1tzT6UgIAAOib1uwMSZKrrvK5EgAA0KtwSD2uRtpVu4uQGgDQpUQvnJiczNQYMklScx2LMAEAkMzasrO8g0pCagAAkl7U4ollNWU+FwMASFaE1GFN4ZC6pb7W50oAAEBPXHa2JMlq+GEZAICkFw6px1dLZbWE1ACArhFShzWneB8FITUAAEkuJ0eSFKjhnA0AQNKbOFGSNKFa2lmz0+diAADJipA6rDnkfRStdXzhBQAgmVluniQpVFPncyUAAKBXEyZ4uypCagBA9wipw5pTgpKYSQ0AQLIL5uVLklLqGvwtBAAA9C48k3piFe0+AADdI6QOawm3+2htYFYWAADJLJhXIElKqSWkBgAg6UWF1MykBgB0h5A6rCU8k7qtod7nSgAAQE9SwiF1el2Tz5UAAIBeRdp9VEtl1YTUAICuEVKHtaSEJElt9cykBgAgmaUWjZYkZdY1+1wJAADoVU6O2nJzlNEiNe7a4Xc1AIAkRUgd1pIaDqmZSQ0AQFILFY6SJOXWOzW1MpsaAICkF275kbazXG2uzediAADJiJA6rLU9pKa/JQAASa3Aa/dR0CDVNNX4XAwAAOhNYIIXUo+tbNOeuj0+VwMASEaE1GFt4XYfjoUTAQBIbvn53q5Bqm2q9bcWAADQOxZPBAD0gpA6rCUtRZLkmEkNABhhzOwOM9tlZiuixv7bzErNbGl4+3jUfd8zsw1mttbMzkx4wdEhdTMhNQAASY+QGgDQC0LqsLbUcEjdSEgNABhx7pR0Vhfjv3HOzQtvT0uSmc2RdJGkueHn/NHMggmrVNrf7qOemdQAAAwJEyZ4u2qprLbM52IAAMmIkDrMhUNqNTb6WwgAAAnmnHtN0t4+PnyRpAedc43Ouc2SNkhaGLfiupKbqzaTcpukmrp9CX1rAAAwAMykBgD0gpA6zKWmege0+wAAIOI6M1sWbgdSEB6bIGlb1GNKwmMHMLOrzGyJmS0pLy+PXVWBgOoyvLUkmip2x+51AQBAfIRD6glVUlkNM6kBAAcipA5rSw+H1MykBgBAkm6WNEPSPEk7JP2qvy/gnLvVObfAObeguLg4psXVZXnn7aY9u2L6ugAAIA7CIfXkSmlnLTOpAQAHIqQOc+npkqRAPTOpAQBwzpU551qdc22SbtP+lh6lkiZFPXRieCyh6rO983YzITUAAMmvsFAtGWnKbZKqy7b1/ngAwIhDSB3mMjMlEVIDACBJZjYu6ua5klaEj5+QdJGZpZnZNEmzJL2T6Pqacrzzdste2n0AAJD0zNQycbwkKbQt4b9tAwCGgJDfBSQLl5khSQoSUgMARhgze0DSxySNMrMSSf8l6WNmNk+Sk7RF0tWS5JxbaWYPS1olqUXStc651kTX3JybJUlqq6hI9FsDAIABsKlTpfWblVHKVVAAgAMRUkdkeV92g/VNPhcCAEBiOecu7mL49h4e/1NJP41fRb1rzc3xDir2+lkGAADoo5RpMyW9rPxdVWppa1EoQBwBANiPdh9hlumF1Cn1LJwIAECya8vPkyQFKqt8rgQAAPRFYOo0Sd7iieW15T5XAwBINoTUEdnZkqRQIzOpAQBIdlZQIEkKVlX7XAkAAIlhZneY2S4zWxE19t9mVmpmS8Pbx6Pu+56ZbTCztWZ2pj9VR5kyxdvtk3bW7PS3FgBA0iGkDgtEZlI3NPtcCQAA6I3lF0qSUqpqfa4EAICEuVPSWV2M/8Y5Ny+8PS1JZjZH0kWS5oaf80czCyas0q5MnixJmlJJSA0AOBAhdVgg2+ttmUpIDQBA0gsVFkmS0qrrfK4EAIDEcM69JqmvizEskvSgc67RObdZ0gZJC+NWXF8wkxoA0ANC6rBglhdSpzW2+FwJAADoTcqo0ZKk9BrWkgAAjHjXmdmycDuQgvDYBEnboh5TEh47gJldZWZLzGxJeXkce0WPH6/WYEBja6Vdez6M3/sAAIYkQuqwYE6uJCm1sdXnSgAAQG/SisZIkjJrCakBACPazZJmSJonaYekX/X3BZxztzrnFjjnFhQXF8e4vCjBoGpG50uS6jeti9/7AACGJELqsFC2F1KnE1IDAJD0MorHSZKya7kCCgAwcjnnypxzrc65Nkm3aX9Lj1JJk6IeOjE85qumiWMlSW1bN/tcCQAg2RBSh4Vy8iRJ6U1tPlcCAAB6kzV6oiQpt54flwEAI5eZjYu6ea6kFeHjJyRdZGZpZjZN0ixJ7yS6vgOE+1KnbtvucyEAgGQT8ruAZJESDqkzmtok5yQznysCAADdSR/lzcTKq5eaW5uVEkzxuSIAAOLLzB6Q9DFJo8ysRNJ/SfqYmc2T5CRtkXS1JDnnVprZw5JWSWqRdK1zzvdfdlNnz5X0jAq37fa7FABAkiGkDktLy1JjUEprldTYKKWn+10SAADohmVlqTkgZbZIe6t3qzB/XO9PAgBgCHPOXdzF8O09PP6nkn4av4r6L+vwoyRJk3bUqqWtRaEAkQQAwEO7j7C0UJpqI5Owamt9rQUAAPTCTFUZ3p8xNbtKfC4GAAD0RWjOoZKk2eXSzpqdPlcDAEgmhNRhacE01UVC6ro6X2sBAAC9q87yZl/Vl+/wuRIAANAnM2eq1aTpFdL2chZPBADsR0gdlhZKU21q+AYzqQEASHp1md6vyw27mYkFAMCQkJ6uXaMzFXJS5ar3/K4GAJBECKnDmEkNAMDQUp+dJklq3rPL50oAAEBf7Z40SpLUvHK5z5UAAJIJIXVYMBBUXXgmdUt1lb/FAACAXjXkZEqSmveU+1wJAADoq5oZkyRJgXXrfa4EAJBMCKmj1Kd4H0dL1T5/CwEAAL1qDofUbRV7fa4EAAD0VetBsyRJ2Zu2+VwJACCZEFJHaUgLSpKaayp9rgQAAPSmJTdHkuT2VfhcCQAA6KvUQw+XJI36cLfPlQAAkgkhdZTGcEjdSrsPAACSXlt+riTJ9u3ztxAAANBnuYcvlCRN2l4jOedzNQCAZBG3kNrM7jCzXWa2Imrsl2a2xsyWmdmjZpYfHp9qZvVmtjS83RKvunoSCalbaqv9eHsAANAf+fmSpGAl520AAIaKsVPnqjxTymp00vbtfpcDAEgS8ZxJfaekszqNvSDpUOfc4ZLWSfpe1H0bnXPzwts1cayrW01pKZKkthq+7AIAkOwsv0CSFKqq8bkSAADQV3lpeVpX7EURtcve9bkaAECyiFtI7Zx7TdLeTmPPO+dawjffljQxXu8/EE3p4ZC6li+7AAAku0BhkSQptarO50oAAEBfmZlKxmdLkqo/+JfP1QAAkoWfPamvkPRM1O1pZva+mb1qZid29yQzu8rMlpjZkvLy8pgW1BIOqR3tPgAASHopRcWSpPSaep8rAQAA/bFn8ihJUvOq5T5XAgBIFr6E1Gb2A0ktku4LD+2QNNk5N1/S9ZLuN7Pcrp7rnLvVObfAObeguLg4pnU1Z6R571FbG9PXBQAAsZcyepwkKaeqwedKAABAf9TOmCRJCq5d73MlAIBkkfCQ2swul/RJSZ93zlvK1znX6JzbEz5+V9JGSQclura29HBIXUO7DwAAkl36hCmSpPzKJsn7kwIAAAwBrbO9r/vZm0t8rgQAkCwSGlKb2VmS/kPSOc65uqjxYjMLho+nS5olaVMia5Ok1sx0SZKro7clAADJLqdwrOpCUkazk/iBGQCAISNzxiFqCEq55VVSNe02AQBxDKnN7AFJb0mabWYlZnalpN9LypH0gpktNbNbwg8/SdIyM1sq6a+SrnHO7e3qdePJZWZ6tRNSAwCQ9PIzClSWHb5RVuZrLQAAoO8mFEzWuqLwjbVrfa0FAJAcQvF6YefcxV0M397NYx+R9Ei8aumzrEhIzQJMAAAku7y0PG3Ikqbtk9zOnbKZM/0uCQAA9MGU/ClaM0o6fJekNWukBQv8LgkA4DNfFk5MVi7DC6kDhNQAACS9lGCKducEJUkNpVt9rgYAAPTV5LzJWjMqfGPNGl9rAQAkB0LqKJaVJUkKNjT4XAkAAOiLffneosf12wmpAQAYKoozi7VpTIokqXnVcp+rAQAkA0LqKIEsr7FlsK7R50oAAEBf1OR7V0E1by/xuRIAANBXZqaqaeMlSa2rVvpcDQAgGRBSR7FsL6QONTT5XAkAAOiL+sIcSVLrzh0+VwIAAPqjZdYMSVLqpq1SS4vP1QAA/EZIHSWUnStJSiGkBgBgSGgoypck2a4yfwsBAAD9Mmb0dG3NkwLNLdKmTX6XAwDwGSF1lGBOniQptb5Jcs7nagAAQG9aigslScFde3yuBAAA9MeU/ClaVRy+sZKWHwAw0hFSR0nLzFFDUAq2Oam+3u9yAABAL9qKvW+3qbsrfK4EAAD0x5S8KVoxOnyDkBoARjxC6ijpoXRVpodvVFb6WgsAAOidjR0rScrYW+VzJQAAoD+m5E/RyshM6hUrfK0FAOA/QuooGaEMVaaFb1TxZRcAMDKY2R1mtsvMVkSN/dLM1pjZMjN71Mzyw+NTzazezJaGt1t8K1xS+qixagpIaXWNXAUFAMAQMjlvMjOpAQDtCKmjZKRkqCoSUjOTGgAwctwp6axOYy9IOtQ5d7ikdZK+F3XfRufcvPB2TYJq7FJ+RoF2ZYVv7NrlZykAAKAfJuRM0NrRpjZJbu1aqbnZ75IAAD4ipI5Cuw8AwEjknHtN0t5OY88751rCN9+WNDHhhfVBXlqeyrLDN8rKfK0FAAD0XUowRQVFE7W5QLLmZmn9er9LAgD4iJA6SkYoaiY17T4AAIi4QtIzUbenmdn7ZvaqmZ3Y3ZPM7CozW2JmS8rLy+NSWH56vsoiM6l37ozLewAAgPiYkh+1eCJ9qQFgRCOkjpKREtWTmpnUAADIzH4gqUXSfeGhHZImO+fmS7pe0v1mltvVc51ztzrnFjjnFhQXF3f1kEHLT8/X9pzwje3b4/IeAAAgPqbkRS2eSF9qABjRCKmjZIQy9rf7YCY1AGCEM7PLJX1S0uedc06SnHONzrk94eN3JW2UdJBfNean56skEpGXlPhVBgAAGIAOiycuX+5rLQAAfxFSR0kPpbNwIgAAkszsLEn/Iekc51xd1HixmQXDx9MlzZK0yZ8qvZC6NBJSl5b6VQYAABiAKXlTtHxM+AYhNQCMaITUUTq0+2AmNQBghDCzByS9JWm2mZWY2ZWSfi8pR9ILZrbUzG4JP/wkScvMbKmkv0q6xjm3t6vXTYS89DyVhtt9OEJqAACGlCn5U7S2SGoJmrRxo1Rb63dJAACfhPwuIJl0WDiRmdQAgBHCOXdxF8O3d/PYRyQ9Et+K+i49lK5dBSmSmuW2bZP5XRAAAOizKXlT1BySNo5O0ewdTdKqVdLRR/tdFgDAB8ykjpIeSqcnNQAAQ0xtcb53sJ2Z1ACA4cvM7jCzXWa2Imrsl2a2xsyWmdmjZpYfHp9qZvXhq6Gir4hKKlPyp0iS3itu9gaWLvWvGACArwipo6QEU1Sd4X0kbRUVPlcDAAD6whUWqD4kBaqqpepqv8sBACBe7pR0VqexFyQd6pw7XNI6Sd+Lum+jc25eeLsmQTX2S2ZKpsZmj9Xicc4bePttfwsCAPiGkLqT2hyv34fbs9vnSgAAQF/kZxS096Vm8UQAwHDlnHtN0t5OY88751rCN9+WNDHhhQ3SjIIZemtS+MZbb/laCwDAP4TUndTlhPt97NnjbyEAAKBP8tPzVZobvkFIDQAYua6Q9EzU7Wlm9r6ZvWpmJ/pVVG+mF0zX+2OlltSQtHq1xFXNADAiEVJ3Up+bKUkyTowAAAwJBRkFKomE1CUlvtYCAIAfzOwHklok3Rce2iFpsnNuvqTrJd1vZrndPPcqM1tiZkvKy8sTU3CU6QXT1RySts8Y4w28/37CawAA+I+QupPmnEy1mBSorpGamvwuBwAA9KIoo4h2HwCAEcvMLpf0SUmfd845SXLONTrn9oSP35W0UdJBXT3fOXerc26Bc25BcXFxgqreb3rBdEnS2skZ3sB77yW8BgCA/wipO8lIydTe8LlRe/f2+FgAAOC/ooyi/e0+mEkNABhBzOwsSf8h6RznXF3UeLGZBcPH0yXNkrTJnyp7Fgmp3xkTbq3NTGoAGJEIqTtJD6VrT2b4Bn2pAQBIekWZRfvbfTCTGgAwTJnZA5LekjTbzErM7EpJv5eUI+kFM1tqZreEH36SpGVmtlTSXyVd45xLyllYkZD6pfx93gAzqQFgRAr5XUCyyUjJYCY1AABDSFFGVEi9bZuvtQAAEC/OuYu7GL69m8c+IumR+FYUG2Ozxyo9lK7Xc/fJhUKytWulmhopO9vv0gAACcRM6k4yQhnaEwmpmUkNAEDSK8os0ub88I1NmySvHScAABgCAhbQtPxpagpJDbOne+fxZcv8LgsAkGCE1J3Q7gMAgKGlKKNI5VlSbVpAqqzkSigAAIaYSMuPXQdN9AZo+QEAIw4hdScZKcykBgBgKCnKLJJM2loU9AY2bvS3IAAA0C+RkHrjlHD/riVLfKwGAOAHQupOMkJRPakJqQEASHpFGUWSpPUFbd7Ahg0+VgMAAPprRsEMSdI7U8PLZv3znz5WAwDwAyF1JxmhjP3tPrhcGACApJeblqtQIKQ1+a3eADOpAQAYUiIzqf9ZWC3l5nprTJSU+FwVACCRCKk7SQ+l0+4DAIAhxMxUmFGojQXhAUJqAACGlEhIvaFys3TCCd7gq6/6WBEAINEIqTvJSMlg4UQAAIaYoowibSwM3yCkBgBgSJlWME2StLlis9pOOtEbJKQGgBGFkLqTjBALJwIAMNQUZRYxkxoAgCEqMyVTY7PHqrmtWWVHHewNElIDwIhCSN1Jeih9/8KJ9KQGAGBIKMoo0rY8qS0UknbskGpr/S4JAAD0w6zCWZKkVZPSpawsad0675wOABgRCKk7OaDdh3O+1gMAAHpXlFGktoBUNb7IG9i0yd+CAABAvxxUdJAkaW3VJukjH/EGX3vNx4oAAIlESN1JRihDDSlSY2pQampiJhYAAENAUaYXTu8en+8NrF3rXzEAAKDf2kPq3Wulj37UG6TlBwCMGITUnWSkeL0+qnNSvQH6UgMAkPSKMryQ+sPJed7AsmU+VgMAAPprdtFsSdK6vev2h9SvvOJfQQCAhIprSG1md5jZLjNbETVWaGYvmNn68L4gPG5m9jsz22Bmy8zsyHjW1p30ULokqSo7xRsgpAYAIOlFZlKvnxheWOKDD3ysBgAA9FdkJvW6Peuko4+WsrOl1aulbdt8rgwAkAjxnkl9p6SzOo19V9KLzrlZkl4M35aksyXNCm9XSbo5zrV1KSslS5K0NzvoDeza5UcZAACgHyIzqZePC/9pQ0gNAMCQMr1gugIW0JZ9W9QYcNKpp3p3PPusv4UBABIiriG1c+41SXs7DS+SdFf4+C5Jn44av9t53paUb2bj4llfV7JTsyVJu3LCH83OnYkuAQAA9FNkJvXyvEYpPV3aulWqrPS5KgAA0FdpoTRNy5+mNtemjRUbpbPP9u4gpAaAEcGPntRjnHM7wsc7JY0JH0+QFH0dT0l4rAMzu8rMlpjZkvLy8pgXl5XqzaTemRWpkJAaAIBkF5lJXda4R5rt9bTUmjU+VgQAAPqrQ8uPs8IXZf/jH1Jzs49VAQASwdeFE51zTpLr53Nudc4tcM4tKC4ujnlNkZnU27NavYGyspi/BwAAiK3ITOo99Xukgw/2Blev9rEiAADQX5GQeu3utdKUKdIhh0hVVdJbb/lcGQAg3vwIqcsibTzC+0jT51JJk6IeNzE8llCRkLoks8UbYCY1AABJrzCjUJK0t36v2iIhNTOpAQAYUmYXeVdDrduzzhuIzKZ+5hmfKgIAJIofIfUTki4LH18m6fGo8UvNc6ykyqi2IAkTCak/TG/0BgipAQBIeqnBVOWk5qjNtalu5mRvkJnUAAAMKe3tPvZ2CqnpSw0Aw15cQ2oze0DSW5Jmm1mJmV0p6QZJp5vZekmnhW9L0tOSNknaIOk2SV+JZ23dSQumKWhBfZgR7nlFuw8AAIaESMuPvVPDy10QUgMAMKR0aPchSSedJGVkSEuXSjsSPocNAJBAcQ2pnXMXO+fGOedSnHMTnXO3O+f2OOdOdc7Ncs6d5pzbG36sc85d65yb4Zw7zDm3JJ61dcfMlJWapZ3Z4QFmUgMAMCREFk/cOTZHCgSkjRulxkafqwIAAH01IXeCMlMyVV5Xror6Cik9XTr5ZO/O557ztzgAQFz5unBisspOzdbeDMkFg1JFBV9wAQAYAiIzqXe7GmnaNKmtTVq/3ueqAABAXwUsoFmFsyRJ6/eGz+G0/ACAEYGQugvZqdlyAaml2Puyq127en4CAADwXWQm9Z66PdIhh3iDLJ4IAMCQEmn5sWZ3+Bx+9tne/vnnpdZWn6oCAMQbIXUXIosnNo8q8AZo+QEAQNJrD6nro0Jq+lIDADCkHDLKO4evLg+fw2fOlGbM8K5yfucdHysDAMRTjyG1mZ0SdTyt033nxasov0VC6vpR+d4AiycCAJLcSD1nRxuVOUpSeCb1nDne4KpVPlYEAEBHnK97N3f0XEnSqt1R53BafgDAsNfbTOobo44f6XTff8a4lqSRlZIlSaorzPEGmEkNAEh+I/KcHS0SUpfXle8PqVeu9LEiAAAOMOLP172ZU+ydw1eVR4XUkZYfzzzjQ0UAgEToLaS2bo67uj1sRGZSVxd4YTUhNQBgCBjwOdvM7jCzXWa2Imqs0MxeMLP14X1BeNzM7HdmtsHMlpnZkbH7JwzOmOwxkqSy2rL9IfXatVJLi49VAQDQwYj8jt0fswpnKWhBbarYpPrmem/wYx+TUlOlJUuk8nJf6wMAxEdvIbXr5rir28NGJKSuys/wBmj3AQBIfoM5Z98p6axOY9+V9KJzbpakF8O3JelsSbPC21WSbh5IsfEwJiscUteUSdnZ0pQpUlOTtHGjz5UBANBuRH7H7o+0UJpmFs5Um2vT2j1rvcGsLOmkkyTnpBde8LdAAEBc9BZSTzezJ8zs71HHkdvTennukBUJqSvy07yBHTt8rAYAgD4Z8DnbOfeapL2dhhdJuit8fJekT0eN3+08b0vKN7NxMftXDEKHmdQSfakBAMloRH7H7q/2vtS0/ACAESPUy/2Loo5v7HRf59vDRqQndXlBqjdQUuJjNQAA9Emsz9ljnHORX2l3ShoTPp4gaVvU40rCYwf8omtmV8mbba3JkycPoIT+6TCTWpLmzvW+yK5cKZ17btzfHwCAPhiR37H7a86oOfqb/tYxpD7rLOlb35Kee05qa5MCvc25AwAMJT2G1M65V6Nvm1mKpEMllTrndsWzMD9FZlLvKAyH1Fu3+lgNAAC9i+c52znnzKzflyA7526VdKskLViwIO6XMOem5SotmKba5lrVNtUqi8UTAQBJZqR+x+6vLhdPPOQQr5XX1q3SW29JH/mIT9UBAOKhx58ezewWM5sbPs6T9IGkuyW9b2YXJ6A+X0RC6u05kkIhb+HEhgZ/iwIAoAdxOGeXRdp4hPeRL86lkiZFPW5ieMx3Ztax5cdc71Jh2n0AAJLFSP2O3V9dhtRm0mc+4x3/5S8+VAUAiKfero850TkXmX70RUnrnHOHSTpK0n/EtTIfRULq6tY6aeJEb3Dbth6eAQCA72J9zn5C0mXh48skPR41fql5jpVUGdUWxHcdWn4ccog3uGaN1NLiY1UAALQbkd+x+2v2qNkKWEAb9m5QY0vj/jsuuMDbP/qot4giAGDY6C2kboo6Pl3SY5LknNsZr4KSQSSkrmmqkSI9NGn5AQBIbgM+Z5vZA5LekjTbzErM7EpJN0g63czWSzotfFuSnpa0SdIGSbdJ+kqs/gGx0GEmdU6ONG2a1NTkBdUAAPhvRH7H7q/0ULpmFMxQq2vVuj3r9t9x9NHSqFHShx9K69Z1/wIAgCGnt5B6n5l90szmS/qIpGclycxCkjLiXZxfslK9hRNrm2u9nlcSITUAINkN+JztnLvYOTfOOZfinJvonLvdObfHOXeqc26Wc+4059ze8GOdc+5a59wM59xhzrklcf+X9cMBiyfOm+ft33/fn4IAAOhoRH7HHoguW34EAtJpp3nHzz/vQ1UAgHjpLaS+WtJ1kv5P0jeift09VdJT8SzMTx1mUs+Y4Q2uX+9jRQAA9GpEnrM7aw+pa8Mh9fz53p6QGgCQHAZ8vjazO8xsl5mtiBorNLMXzGx9eF8QHjcz+52ZbTCzZWZ2ZJz+PXHTZUgtSWed5e3/9rcEVwQAiKdQT3c659ZJOquL8eckPRevovzWIaSe450YWXQJAJDMRuo5u7P2dh+RmdRHhr+Tv/uuTxUBALDfIM/Xd0r6vbyFFiO+K+lF59wNZvbd8O3vSDpb0qzwdoykm8P7IWNusbcA8vJdyzve8elPS+np0iuveFc8R65+BgAMaT2G1Gb2u57ud859LbblJAdCagDAUDNSz9mdHTCTeuFCb//OO15v6tRUnyoDAGBw52vn3GtmNrXT8CJJHwsf3yXpFXkh9SJJdzvnnKS3zSzfzMYl02LHvZk3dp4kaenOpR3vyMuTFi2SHnpIuu8+6fvfT3htAIDY663dxzWSTpC0XdISSe922oalDiH1rFlSMCht2iTV1/tcGQAA3RqR5+zOOiycKEnFxd4Pzg0NzKYGACSDWJ+vx0QFzzsljQkfT5C0LepxJeGxA5jZVWa2xMyWlJeXD6CE+Jg9arbSgmnavG+z9jXs63jnpZd6+7vvlpxLeG0AgNjrLaQeJ+lWSWdKukRSiqTHnXN3OefuindxfslK8RZOrGmq8WZczZrlnfjWrvW5MgAAujUiz9mdHbBwoiSddJK3f+01HyoCAKCDuJ2vw7Om+53YOududc4tcM4tKC4uHkwJMRUKhHTYmMMkScvKlnW884wzpDFjvO/oS5JqDWcAwAD1GFI75/Y4525xzp0s6YuS8iWtMrNLElGcX7JSvZC6tqlWzjlafgAAkt5IPWd3dsBMakk6/nhvv3ixDxUBALBfHM7XZWY2TpLC+13h8VJJk6IeNzE8NqTMGzNPUhctP0Ih6eKLveN77kloTQCA+OhtJrUkKbwS8NclfUHSMxrmlw2nBlOVGkxVq2tVY2ujNNdbsIGQGgCQ7EbaObuzgvQCpQRSVNVYpfrmcJuu6L7UAAAkgRier5+QdFn4+DJJj0eNX2qeYyVVDqV+1BHd9qWWpEvCuf6990p1dQmrCQAQHz2G1Gb2EzN7V9L1kl6VtMA5d6VzbtintSyeCAAYSkbyOTuamWlczjhJ0s6and7grFlSbq5UWuptAAD4ZDDnazN7QNJbkmabWYmZXSnpBkmnm9l6SaeFb0vS05I2Sdog6TZJX4n9vyb+egypjzzS+yG6osILqgEAQ1pvM6n/U97lR0dI+rmk98xsmZktN7NlPT5ziOsypF650seKAADo0Yg9Z3c2Pme8JKm0OhxIBwLS0Ud7x2+95VNVAABIGsT52jl3sXNunHMuxTk30Tl3e7h9yKnOuVnOudOcc3vDj3XOuWudczOcc4c554Zk4+bDxxwuSVpZvlJNrU0HPuDrX/f2v/sdCygCwBAX6uX+aQmpIgl1WDzxoIO8L7gbNkiNjVJams/VAQBwgBF7zu5sQs4ESdL26u37B086SXrxRenVV6Xzz/epMgAAOF/3R05ajmYWztSGvRu0Zvea9tC63fnnS//+796EshdflE47zZ9CAQCD1tvCiVu72iRtk3RCYkr0R05ajiSpurFaSk+XZsyQ2tqkdet8rgwAgAON5HN2Z+0zqauiWnt87GPe/pVXEl4PAAARnK/7r8eWH6mp0lfCnUx++9uE1QQAiL3eelLnmtn3zOz3ZnZGeNGFr8rrbXVBYkr0R25ariSpqrHKG6AvNQAgiY3kc3ZnkZC6w0zqY47xfnResUIqL/epMgDASMf5uv/mjZknqZuQWpKuvtq72vmpp7yrnwEAQ1JvPanvkTRb0nJJ/ybpZUnnS/q0c25RnGvzVV5aniSpsrHSGyCkBgAktxF7zu4s0u6jvSe15H15Pf547/i113yoCgAASZyv+23+uPmSegipi4uliy/2elL//veJKwwAEFO9hdTTnXOXO+f+JOliSXMknemcWxr3ynzWHlI3EFIDAIaEEXvO7qzLmdTS/pYfL7+c2IIAANiP83U/Rdp9vL/zfbnuFkeMLKB4xx1SVVViCgMAxFRvIXVz5MA51yqpxDnXEN+SkkOk3Uf7TOq5c709ITUAIDmN2HN2ZxNyu1g4UZJOPtnb05caAOAfztf9ND5nvMZmj9W+hn3aVLGp6wfNm+ctklxdLd15ZyLLAwDESG8h9RFmVhXeqiUdHjk2s2H982ReujeTur0n9ezZkpm3cGJzcw/PBADAFyP2nN1Z+8KJ1aUdZ1wdfbSUkSGtXCnt2uVTdQCAEY7z9QAsGL9AkrRk+5LuH/S1r3n7W27xWn8AAIaUHkNq51zQOZcb3nKcc6Go49xEFemHA9p9ZGZK06ZJLS0sxgAASDoj+ZzdWU5qjrJSslTXXLf/x2aJvtQAAN9xvh6Yo8YdJUl6d8e73T/onHOksWOl1au5agoAhqDeZlKPWJGZ1O3tPqT9falXrvShIgAA0Bdm1t7yo8PiiZJ0yine/sknE1wVAAAYqD7NpE5Jka65xjv+r/9iNjUADDGE1N1on0kdHVIfeqi3X7o08QUBAIA+63bxxPPP9/Z/+5tUX5/gqgAAwEBEz6Ruc23dP/Ab35AKCqR//lP6178SUxwAICYIqbvRvnBiQ1RIffTR3p6THQAASW1CTngmdVWnmdQHHSQtWOAtrPT00z5UBgAA+mtczjiNzxmvqsYqbdy7sfsH5uVJV1zhHd9yS2KKAwDEBCF1Nw5YOFGSFi709u+8w6VDAAAksW5nUkvS5z7n7e+7L4EVAQCAwehTyw9JuvpqyUy6915p69YEVAYAiAVC6m502e5jwgRp3Dhp3z4WTwQAIIlFZlJvq9p24J0XXuh9eX3qKammJsGVAQCAgejT4omSNGuW94N0c7P0P/+TgMoAALFASN2N9oUTo9t9mEnz53vHy5f7UBUAAOiLyXmTJXUTUo8f77XwamqSXnstwZUBAICB6PNMaslbODEYlO68kwlmADBEEFJ3o8uZ1NL+xRMJqQEASFpT8qdIkrbu6+Yy39NP9/YvvJCgigAAwGBEZlK/t+O9nhdPlLzZ1JddJrW2Sj/+cQKqAwAMVsJDajObbWZLo7YqM/uGmf23mZVGjX880bVFSw+lKxQIqam1SY0tjfvviITUK1b4UxgAAOhVZCb1h5Ufdv2AM8/09o8/zjoTAAAMAWOyx2hi7kRVN1Vr7e61vT/hhz+UUlK8NSg29rDYIgAgKSQ8pHbOrXXOzXPOzZN0lKQ6SY+G7/5N5D7n3NOJri2amXU9m5qZ1AAAJL2ijCJlhDJU2VjZsXVXxPHHe2tNbN4svflm4gsEAAD9dtzE4yRJb5W81fuDp06VPv9578fo3/wmvoUBAAbN73Yfp0ra6JxLyiV3u+xLfcghUnq6tHattHOnT5UBAICemFnPfamDQW9RJUm6554EVgYAAAbq+EnHS5Le3NbHH5ivv97b33qrtGZNnKoCAMSC3yH1RZIeiLp9nZktM7M7zKzAr6IiupxJnZ4unXyyd/zssz5UBQAA+qLXvtSXXOLtH35Yamzs+jEAACBpRGZS9zmkPuww6UtfkpqbpWuvpcUXACQx30JqM0uVdI6kv4SHbpY0Q9I8STsk/aqb511lZkvMbEl5eXlca+xyJrUkfeIT3v6ZZ+L6/gAAYOAm5/bSl/qww6QjjpAqKqSnfe0yBgAA+mD+uPlKC6Zp9e7V2lu/t29P+vnPpaIi6aWXpAcfjG+BAIAB83Mm9dmS3nPOlUmSc67MOdfqnGuTdJukhV09yTl3q3NugXNuQXFxcVwLzE3LlSRVNVZ1vOOUU7z966/zSywAAEmq18UTpf2zqWn5AQBA0ksNpuroCUdLkhaXLO7bk4qKpF/8wju+/nqppiZO1QEABsPPkPpiRbX6MLNxUfedK2lFwivqpMt2H5I0e7ZUWCht3y5tTcp22gAAjHjtIXVVDyH15z4nBQLSk09KZWUJqgwAAAxUv1t+SNLll0sLF3rrSv3v/8anMADAoPgSUptZlqTTJf0tavgXZrbczJZJOlnSN/2oLVp7SN253UcgIH3kI97x668nuCoAANAXvfaklqRx46RPftLrVfnnPyeoMgAAMFDtiyeW9COkDgSkn/7UO/7Zz6QPe/gBGwDgC19CaudcrXOuyDlXGTV2iXPuMOfc4c65c5xzO/yoLVp7T+rOM6ml/SH1G28ksCIAANBXfWr3IUnXXeftb77ZC6sBAEDSisykXlyyWC1tLX1/4mmnSeef77X7+PKXad0JAEnGz3YfSS8yk/qAntSSdMIJ3p6QGgCApDQhZ4JMptLq0p6/xJ52mtfKq7RUevzxxBUIAAD6bUz2GE0vmK7a5lqt2NXPLqH/+79Sfr63YDKLKAJAUiGk7kFk4cQD2n1I0lFHSamp0ooVUkVFgisDAAC9SQulaWz2WLW5Nm2v3t79A832z6b+/e8TUxwAABiw9pYf/elLLUljx0o33ugdf+1r0u7dMa4MADBQhNQ96LHdR3q61/LDOemRRxJcGQAA6Is+9aWWpEsvlbKzpVdflZYvT0BlAABgoAa0eGLEFVdIp5ziBdTXXx/jygAAA0VI3YP89HxJUkVDNzOlv/hFb89CSwCAYcjMZpvZ0qitysy+YWb/bWalUeMf97vW7vS5L3VurnTZZd7xH/4Q56oAAMBgDHgmteRdQfWnP3kTz+65R3rqqRhXBwAYCELqHhRlFEmSKuq7Cak/8xkpL09avJhZVwCAYcc5t9Y5N885N0/SUZLqJD0avvs3kfucc0/7VmQvJuf2MaSWpGuv9fb33CNVdnEVFQAASAqHjj5U2anZ2rxvs3ZU7+j/C8ycKf3kJ97x5z8vrV4d2wIBAP1GSN2DwoxCSdKe+j1dPyAz0zuhScymBgAMd6dK2uic66VvRnLp80xqSTrkEO/y37o66d5741wZAAAYqFAg1N7y47Wtrw3sRb71Lem887wfpj/5SWnXrhhWCADoL0LqHkRC6r31e7t/0L/9m7e/5x6poSEBVQEA4IuLJD0Qdfs6M1tmZneYWUFXTzCzq8xsiZktKS8vT0yVnbT3pK7sY7Z+9dXe/pZbvHUnAABAUvrY1I9Jkl7d+urAXiAQkO6+WzrqKGnTJunCCzn3A4CPCKl7kJeep4AFVNVYpebW5q4fNH++dOSRUkWF9Je/JLZAAAASwMxSJZ0jKXKiu1nSDEnzJO2Q9Kuunuecu9U5t8A5t6C4uDgRpR5gSp4XUm+q2NS3J3z609Lo0dKKFdIbb8SvMAAAMCgfnfJRSYMIqSUpK0t68kmpuFh65RXpj3+MTXEAgH4jpO5BwAIqSPcmh+1r2Nf9A7/8ZW//y19KbW3xLwwAgMQ6W9J7zrkySXLOlTnnWp1zbZJuk7TQ1+p6MKNwhiRp877Nam1r7f0JqanSlVd6x7/+dRwrAwAAg3H0hKOVEcrQqvJVKq8dxBVbY8dK//u/3vE3viG9/HJM6gMA9A8hdS967UstSZdcIk2Y4C2e+NvfJqgyAAAS5mJFtfows3FR950raUXCK+qj7NRsjc0eq6bWJpVUlfTtSV/9qpSWJj36qLRqVXwLBAAAA5IaTNVxkwbZlzriwgulf/93qaVFOv98afPmGFQIAOgPQupeFGUWSeqlL3VamvSHP3jHP/yhtG9f/AsDACABzCxL0umS/hY1/AszW25myySdLOmbvhTXR7MKZ0mSNuzd0LcnjBsnXXGFd/zzn8epKgAAYsPMZpvZ0qitysy+YWb/bWalUeMf97vWWItJy4+IG26QPvEJae9er/1Xbe3gXxMA0GeE1L3o0+KJkrRokXTKKd6J7LbbElAZAADx55yrdc4VOecqo8Yucc4d5pw73Dl3jnNuh5819mZm4UxJ/QipJek//kMKBqX77qM3NQAgqTnn1jrn5jnn5kk6SlKdpEfDd/8mcp9z7mnfioyTmIbUwaB0773SQQdJy5Z5P1izkCIAJAwhdS/a233U9dDuI+Kb4YlkP/6xtHFjHKsCAAB9NaCQeupU6Tvf8b6cXn45s6kAAEPFqZI2Oue2+l1IIhwz8RilBdO0vGx57xPL+iI/X3rsMSknR3r4Yen//b/BvyYAoE8IqXtRlOG1++ixJ3XEJz4hXXCB90X2//v/4lwZAADoi/aQuqIfIbUk/ehH0mGHSRs2SN/7XhwqAwAg5i5S1DoSkq4zs2VmdoeZFfhVVLykh9J1zMRj5OQG35c64pBDpHvu8Y6//33pwQdj87oAgB4RUveiOLNYkrSrdlfvDzbzelcGAt5lQosXx7k6AADQmwHNpJa8NSfuuksKhaT//V/ppZfiUB0AALFhZqmSzpH0l/DQzZJmSJonaYekX3XzvKvMbImZLSkvL09EqTF16rRTJUnPb3w+di+6aJE3i9o56dJLpedj+NoAgC4RUvdiTPYYSVJZbVnfnjB9unTNNd6qwOeeK1VXx7E6AADQm0hIvXHvRrW5tv49ef58b1FkyWv7UVnZ48MBAPDR2ZLec86VSZJzrsw51+qca5N0m6SFXT3JOXerc26Bc25BcXFxAsuNjbNmniVJenbDs3Kx7CH97W9L118vNTdL553Hj9UAEGeE1L0Yk+WF1H2aSR1x003SMcdIO3ZIP/hBfAoDAAB9kpuWq9FZo1XfUq8d1QNY4/F735OOPlratk36+tdjXyAAALFxsaJafZjZuKj7zpW0IuEVJcBR445SYUahNu/b3P+rpnpiJv3yl9Jll3ktPc8+W/rLX3p/HgBgQAipezE6a7QkqaymjzOpJSklRfrDH/ZfHvzQQ3GqDgAA9EVkNvX6vev7/+SUFOnuu6X0dK/9x6OPxrg6AAAGx8yyJJ0u6W9Rw78ws+VmtkzSyZK+6UtxcRYMBHX69NMlSc9tfC62Lx4ISHfcIX3ta1JTk3TRRd5tAEDMEVL3ot/tPiKOOkr69a+94y9+kaAaAAAfHVR0kCRp7e61A3uBgw/2elNK0lVXSaWlMaoMAIDBc87VOueKnHOVUWOXOOcOc84d7pw7xzk3gMuJhobolh8xFwh4V0v/5CdSW5t05ZXSjTfG/n0AYIQjpO5FZCb1rtpd/e9vdd11Xn/q+nrp4oulO++MfYEAAKBXs4tmS5LW7hlgSC155/XTTpN275bOP99bfwIAAPjuzBlnSpJe2vyS6prrYv8GZt4aFTfd5N3+9relb33LC60BADFBSN2LzJRMZadmq6m1SZWN/VwsyUz64x+ln/3MWxX4iiukb36TxRQBAEiwg0cdLElas3vNwF8kEJDuv1+aOFF6+23pf/4nRtUBAIDBGJczTgsnLFR9S72e3/h8/N7o61/3/hZISfGunL70Uq8NCABg0Aip+yCyeGK/+lJHmHkLLv3yl97xTTdJRx4pPfssv7oCAJAgMZlJLUnFxV4vSjPvst/IjCoAAOCrT8/+tCTp8bWPx/eNLr5YeuopKTtbuu8+6VOfYiIaAMQAIXUfRPpS76rdNfAX+fd/l5YskQ4/XNqwwVsZ+IgjpH/9K0ZVAgCA7swonKGgBbW5YrMaWhoG92Knny796U/e8Te/6S2WDAAAfLXo4EWSpL+v/bta2uLckuv006VXXvF+vH7+eemEE6RNm+L7ngAwzBFS90GkL3W/F0/sbP586Z//lL7/fWnyZGnFCmnhQq+/5c9/Lu0YtutYAADgq9RgqqYXTJeT04a9Gwb/gl/60v5w+rrrpNtuG/xrAgCAATtk1CGaWThTe+r36M1tb8b/DY86SnrzTWnWLGnZMmnBAum55+L/vgAwTBFS98Gg2n10lpsr/fSn0po13mILmZnSiy/uD64vvNA7sdXWDv69AABAu5j0pY72la9Iv/mNd3z11SyQDACAj8xsf8uPNXFu+RExc6Z3dfSnPiVVVEif+IR0663emlQAgH4hpO6DSEg9qHYfnWVkSL/4hbR1q3T33dJnPuOdyB5+WDrrLCkvz+td/eUve19616yR6uqk1tbY1QAAwAjS3pd69yD7Ukf7xje883lkgeT77ovdawMAgH6JtPx4bO1jcokKivPypMcek77zHe/7+tVXS5/8pLR9e2LeHwCGiZDfBQwFMWv30ZVRo6RLLvG2khLvcuGnnpLef3//dsst+x8fDEpjx0rjx3vbhAn798XFUijk9b0eNy72tQIAMIS1z6TeE6OZ1BHf/rbU2Cj98IfSpZdKNTXSVVd5iysCAICEOW7icSrOLNamik1aWb5Sh44+NDFvHAhIN9wgHXaY1wbs6aeluXOl//1f6fOf528CAOgDQuo+iCycGJeQOtrEidKPf+xtNTXSu+9Kixfv38rLpaYmqbTU27pjJuXne/vCQi8IT0mRCgq8FYhzc72gOzdXysnxxrKz9x+np0ttbfsvUTLzTrrRW02Nt4Jx5D6z/h+npXkzytPTvbHI+wWDXr2hkFdHc7PU0rK/puhfxEMh7/HRr9+XDQAw4kRC6tXlq2P/4v/5n945+n/+R7rmGm8Rpf/7P+9cCwAAEiIYCOpTB31Kdyy9Q4+veTxxIXXE5z8vnXyyt3bF0097k9EeecSbeDZmTGJrAYAhhpC6D+LS7qM32dnSRz/qbdEaG6WdO71Lh7Zv98LqyPHu3V54/PbbXj8sSdq7V9oQgwWihqOuguv+BN09BeDJ9Dr9ea14fMaAH1JTpWee8bsKJJk5xXMkSavKV6nNtSlgMe569uMfS7Nne72q//Y36YMPpD/9STr11Ni+DwAA6NaigxfpjqV36LG1j+kHJ/0g8QWMHy89+aT3Y/U3vuG1AnnpJem735W+/nVvXSoAwAEIqfugvd1HLBZOHKy0NGnKFG/rTlOTVFnpHe/dK+3Z44Xbe/dK9fXefTt3SlVV+2dE19TsP25o2D87WfJmLre1ddzS0ryZ2ZGZzZHHRO+7G3PO69XV1OTV09DQcXZ0a6s3e7q5ueOs6s6zpdvavMe2th74+t1t0boaAzA8pKX5XQGSUEFGgSbkTFBpdak2V2zWjMIZsX0DM28G1cKF0mc/64XUp50mXX65dOONUlFRbN8PAAAc4LTppykjlKEl25eopKpEE3MnJr4IM2+titNO866weuYZ6fvfl373O+lHP5L+7d+877kAgHaE1H0Qafexs2anz5X0UWqq159a2r/Hft2F130JufsSgsfidfyqKZYzn/kBAH5iFj+6cejoQ1VaXaoVu1bEPqSOmDVLeucdL5j+yU+8BZCfekq66Sbp4ov57ycAAHGUmZKpM2eeqcfWPKYn1j6hrxz9Ff+KmTzZa/vxwgveTOr33vOuuPrtb72Flz/1Kf4uAIAwQuo+yEvLU0YoQ7XNtapurFZOWo7fJWEw6EsNACPW3OK5em7jc1pZvlKLDl4UvzdKTfVmTJ1/vnT11dIrr3izrH/6U+lXv5LOOit+7w0AwAi3aPYiPbbmMT2+9nF/Q+qI00/3ZlU/8oj0ve9Ja9dKixZJJ50k/b//Jx17rN8VAoDvYtyMcXgyM43LGSdJ2lGzw+dqAADAQEUWUFqxa0Vi3vCgg7w+lH/+szRhgrRqlXT22d4X040bE1MDAAAjzCcP+qQCFtDLm19WZUOl3+V4zLwfr1eu9GZSFxVJr70mHXecN746Dgs7A8AQQkjdR+NzxkuStldv97kSAAAwUAkPqSXvS+mVV0qbNnmX9mZnS0884QXYH/+49NBD3voMAAAgJkZljtIJk09Qc1uzntmQZItpp6ZKX/ua92P1974nZWR4M6znzPHaf7z8Mq0TAYxIhNR9REgNAMDQN6d4jiRpze41am5tTuybp6ZK3/62d4nvpZd6CwI/84x00UXS2LHewkpvvcUXUwAAYmDRbK+t1+NrH/e5km7k5Uk/+5m0fr33N0B6uvTkk9Ipp0hHHindc4/U1OR3lQCQMITUfTQ+m5AaAIChLis1S9MLpqu5rVnr9673p4jx46W77pJ27JB+9zvvi2hlpfSnP0nHHy8dfLDXu3rdOn/qAwBgGIiE1E+vf1qNLY0+V9ODCROkm2+WPvxQ+vGPpdGjpaVLvR+0p071guw9e/yuEgDijpC6j9p7UlfTkxoAgKHssNGHSZKWlS3zt5BRo6SvflV6911p+XLp3//dm1G9bp30n/8pzZ7ttQS59lrp0Ue9IBsAAPTJjMIZOnzM4apqrNLzG5/3u5zeFRdLP/qRtHWrdMcd0qGHej9o/+AH0qRJ3kLMb7/NFVcAhi1C6j6KtPsorS71uRIAADAY88bOkyS9v+N9fwuJduih0i9/KW3bJj31lPS5z0kFBd4lwH/8o3Teed4CSyeeKP3Xf0n/+IdUVeV31QAAJLWL5l4kSXpw5YM+V9IP6enSF78oLVsmPf+8dNZZUn29dOut3iKLs2dLP/mJt9YFAAwjhNR9NDF3oiRpW9U2nysBAACDMX/sfEnS+zuTKKSOCIW8xRTvu0/atUt6803vi+gJJ3j3v/66d/v006X8fOmww6QrrpB+/Wvpueek0lJmWAEAEHbhoRdKkh5f87jqmut8rqafzLzz/TPPSCtXSt/6lnfF1fr13g/WM2Z4fx/ceqtUUeF3tQAwaCG/3tjMtkiqltQqqcU5t8DMCiU9JGmqpC2SLnDOJcX/207NnypJ2rpvq7+FAACAQWmfSb3zfTnnZGb+FtSdUMibMXXccdIPf+i1+3j5ZenVV73w+v33pRUrvC1aXp40d663zZ7t9bg++GCvr2Uw6Ms/BQAAP0wvmK5jJhyjxaWL9eS6J3XB3Av8Lmlg5syRbrxRuuEG6cUXvUUVH31UeuMNb/vqV6Uzz/SuvDrnHKmw0O+KAaDffAupw052zu2Ouv1dSS86524ws++Gb3/Hn9I6mpAzQQELaHv1djW1Nik1mOp3SQAAYAAm501WQXqBdtftVml1afvVUkkvL0/69Ke9TZIaGqT33vMWV1q5cn9gvXevF2K/+WbH56emej2uDz5YmjXLm4E1bZrX57K42Hv9ZA3sAQAYoIsOvUiLSxfrwRUPDt2QOiIU8sLoM8+Uqqulv/3NC6xfekn6+9+9LRiUPvIR6eyzve3wwzm/AxgS/A6pO1sk6WPh47skvaIkCalTgimakDNB26q2aVvlNs0onOF3SQAAYADMTPPHzddLm1/S+zveHzohdWfp6dLxx3tbhHNSWZkXVq9eLa1dK61Z422lpV3PvI4IhaS0NCklxZuBFb0VFEg5OVJ2trfl5HihdmTLyZFyc70tO1sK0FEOAJAcLph7ga5/7no9vf5pVTZUKi89z++SYiMnR7rsMm/bsUN6/HEvtH7pJem117zte9+Txo3zwupzzvHah2Rm+l05AHTJz5DaSXrezJykPznnbpU0xjm3I3z/TkljOj/JzK6SdJUkTZ48OVG1SvJafmyr2qatlVsJqQEAGMLmjw2H1Dvf16dmf8rvcmLHzOtXOXasdNppHe+rrt4fWm/YIG3cKG3dKpWUSLt3e/e3tHiP3bdvcAsypafv3zIyvCA7P3//lpXV8TFpaR1vR49lZHiPjwToqan795H7Q8k27wIAkCzG54zXR6d+VK9seUUPr3xYXzrqS36XFHvjxknXXONt+/Z5Cyw/84z07LPS9u3SHXd4W0aGdPLJ0kc/Kp10knTkkd75FACSgJ9/0Z/gnCs1s9GSXjCzNdF3OudcOMBWp/FbJd0qSQsWLEjoykBT8qfonx/+U1v2bUnk2wIAgBiLLJ743o73fK4kgXJypAULvK0rTU3e1tjotQzZu1fas8dbjKmiQqqp2b9VVXk9sisrvXC7qsrbqqu9raHB2xIlFPJmhqWnewF2KLR/Hwjs34LBA++P3nc+Tknxnme2/zW6Ou48Fgx6rxMKdX/c3/ui/x3dbcHg/i1SS/QWPdb5/qGkq3o7j/V2u7fHxOL1ACSNK+dfqVe2vKI/vfun4RlSR8vPl84/39uck5Ytk5580ptp/a9/SU8/7W2SF1ofe6zXHuTww6VDD5VmzvTOfwCQYL6F1M650vB+l5k9KmmhpDIzG+ec22Fm4yTt8qu+rkzJmyKJxRMBABjqjhp/lCTpndJ3knvxxERKTfW27GypqGjgr9PW5gXd9fVeUF1Xtz/Q3rfP2+rq9gfZXW2NjfuP6+ul2lpvrLnZC9Ij+8j9LS37g3IgmcUyLB/I42L1+p0N5v6775Y+NYyuaEFSOn/O+fr6s1/Xuzve1ZLtS7RgfDc/2A43ZtIRR3jbD37gzap+6SXpn//02oGsWeMtyvzyy/ufk5bmPf7oo73tyCO9hZiZcQ0gznwJqc0sS1LAOVcdPj5D0k8kPSHpMkk3hPeP+1Ffd6bmT5Ukba0kpAYAYCg7qOggFaQXaEfNDn1Y+aGm5E/xu6ThIxDwZmZlZCTm/ZzzAutIKN7S4oXYkX1bm/eY1lZv63x/T8fNzd5zI6/R1tb7cWurdxx5r5aWjsedb/f1vuj36WqL/PsiW6Se6K3zWPTtofJDjeviQsrOY73d7u0xA3m9vupLLSNNc7PfFWAESA+l6/IjLtev3/61/rTkT1pwzggJqTsbP176whe8TZJ27ZLeeENavHj/IsxbtkjvvONtEaGQt/jyoYd23KZP967eAYAY8Gsm9RhJj4ZnLYUk3e+ce9bM/iXpYTO7UtJWSUm19G5kJjXtPgAAGNoCFtCxE4/VMxue0VslbxFSD2Vm3qyvtDS/KwE6imdYPpDHxer1Oxvs/VlZPd8PxMhVR12lX7/9a92/4n7deMaNw2cBxcEYPVo691xvi9i3T3rvPa81yL/+JX3wgbeOxapV3vbww/sfm54uTZu2f5s+vePtPD5jAH3nS0jtnNsk6YguxvdIOjXxFfVN5AssM6kBACOFmW2RVC2pVVKLc26BmRVKekjSVElbJF3gnKvwq8aBOm7icV5Ive0tXXToRX6XA2C46UsfawAJM3vUbJ089WS9vOVl3bf8Pn3l6K/4XVJyys+XTjnF2yLq6qTVq72Z1itW7J91vW2bN756ddevVVjYfYA9ZQo/MAPogKXQ+2Fy3mRJUklViVraWhQK8PEBAEaEk51zu6Nuf1fSi865G8zsu+Hb3/GntIE7btJxkqS3St7yuRIAAJAI1yy4Ri9veVm3LLlFX17wZdak6KvMTOmoo7wtWnW1tHmztGmTt49skduRhZjffffA1zSTJkyQJk/29uPHS+PGSWPGeNvYsd5+9Giv3QiAYY//pfdDeihdY7PHamfNTm2v3t4eWgMAMMIskvSx8PFdkl7REAypF05YKJPp/Z3vq765XhkpCeqhDAAAfPHpgz+t0VmjtXzXcr1V8paOn3S83yUNbTk50uGHe1tnznk9r7sLsLdtk0pKvK0nZt6CztHB9ZgxXqA9frw3lpcn5ebu3zIyuHoFGIIIqftpSt4U7azZqa37thJSAwBGAifpeTNzkv7knLtV0hjn3I7w/TvlrTVxADO7StJVkjR5cvKdM3PTcnXo6EO1fNdyvbvjXZ0w+QS/SwIAAHGUGkzVFfOu0A1v3KA/vfsnQup4MtsfKB933IH3Nzd7AfW2bdL27VJpqbRzp1RW1nFfXi7t3u1tK1f27b2DQS+szsnpGF5Hb93dFz2ek8MsbiCB+F9bP03Nn6rFpYu1tXKrTtSJfpcDAEC8neCcKzWz0ZJeMLM10Xc651w4wD5AONC+VZIWLFjQy8pZ/jhu4nHebKptbxFSAwAwAnzpqC/phjdu0EMrHtJvzvyNCjMK/S5pZEpJ2d+fuictLV5AXVbWMcDescPbdu6Uqqq8rbra29fXSxUV3jZYmZn7A+ucHG+x18zMA/fRxxkZ3paevv84eiwlRUpNPXBLSWEGOEY0Qup+mpLnLZ64Zd8WfwsBACABnHOl4f0uM3tU0kJJZWY2zjm3w8zGSdrla5GDcNyk43Tre7fq7dK3/S4FAAAkwPSC6Tpzxpl6buNzuvuDu/WNY7/hd0noSSjktfQYO7bvz2lu3h9Ydw6wO2/djUfuq6vztp074/dvjJaS0n2IHR1m9/f+yFjk9aO3YNCfjUAenRBS99OUfC+k3rpvq8+VAAAQX2aWJSngnKsOH58h6SeSnpB0maQbwvvH/atycI6b6F1++ua2N+WcYwElAABGgGsWXKPnNj6nP/zrD/rqwq8qGAj6XRJiKSVFKiz0tsFwzguoo0Pr2lpvLLKPPo7s6+v3bw0NHW83NXW/NTfv3+rqYvNZJDMz/wLyRG2BQNdbT/cNZjMb+POS4HsQIXU/Tc2fKknaWklIDQAY9sZIejQc3IYk3e+ce9bM/iXpYTO7UtJWSRf4WOOgHFR0kIozi7WzZqfW7F6jQ4oP8bskAAD6zcy2SKqW1CqpxTm3wMwKJT0kaaqkLZIucM7FoP/B0PfJgz6pqflTtWHvBj2+9nGdd8h5fpeEZGTmtfDIyvIWaow357z2Jj0F2ZEwu7fHdH5cY2PHEDx6a21N/NbWtv/f29IS/88WvYsE3N0F3eecI91zT1xLIKTuJ9p9AABGCufcJklHdDG+R9Kpia8o9sxMZ848U/cuu1fPbniWkBoAMJSd7JzbHXX7u5JedM7dYGbfDd/+jj+lJZdQIKRvHfctffWZr+oXb/xC5x58LldTwX9m+1twZGX5XU18OecF1X4E5IkM4rvaerov+jGRz2ggW3+f65y3tbZ2/59ZfX3c/2tBSN1PkZnUW/ZtUXNrs1KCKf4WBAAABuWsGWd5IfXGZ/XN477pdzkAAMTKIkkfCx/fJekVEVK3++K8L+q/XvkvLS5drNc/fF0nTjnR75KAkSO61Qf8Fwmpewq6E/CfVSDu7zDMZKVmaVr+NDW3NWv93vV+lwMAAAbpjBlnyGR6dcurqmseAf3/AADDkZP0vJm9a2ZXhcfGOOd2hI93ymvjdQAzu8rMlpjZkvLy8kTUmhSyUrN07dHXSpJ++eYvfa4GAHwUafERCnkLbKanSxkZ3oz+nBwpL0/Kzo57GYTUA3Do6EMlSSt2rfC5EgAAMFjFWcU6avxRamxt1KtbXvW7HAAABuIE59yRks6WdK2ZnRR9p3POyQuyD+Ccu9U5t8A5t6C4uDgBpSaP6xZep/RQuv6+7u9aXb7a73IAYEQjpB4AQmoAAIaXs2acJUl6dsOzPlcCAED/OedKw/tdkh6VtFBSmZmNk6Twfpd/FSan0VmjdfkRl0uSfvXWr/wtBgBGOELqASCkBgBgeDlrZjik3khIDQAYWswsy8xyIseSzpC0QtITki4LP+wySY/7U2Fyu/6462Uy3bPsHu2o3tH7EwAAcUFIPQCE1AAADC/HTDxGeWl5WrdnnTZVbPK7HAAA+mOMpNfN7ANJ70h6yjn3rKQbJJ1uZuslnRa+jU5mFc3SuYecq6bWJv36rV/7XQ4AjFiE1AMwu2i2ghbUhr0bVN9c73c5AABgkEKBkE6fcbok6bkNz/lcDQAAfeec2+ScOyK8zXXO/TQ8vsc5d6pzbpZz7jTn3F6/a01W3z/h+5KkP/zrD9pZs9PnagBgZCKkHoC0UJoOKjpITk6rd7O4AgAAw0F7X2pafgAAMKIcNf4oLZq9SPUt9brhdSacA4AfCKkHiJYfAAAML2fOPFOS9OKmF9XU2uRzNQAAIJF+/LEfS5JuWXKLSqpKfK4GAEYeQuoBIqQGAGB4mZg7UYeOPlS1zbV648M3/C4HAAAk0BFjj9D5c85XY2ujfv7Pn/tdDgCMOITUA0RIDQDA8NPe8mMDLT8AABhp/vuj/y2T6bb3btPWfVv9LgcARhRC6gEipAYAYPg5ayZ9qQEAGKnmjp6riw+7WM1tzfrhyz/0uxwAGFEIqQdoRsEMpQXTtK1qmyobKv0uBwAAxMAJk09QVkqWlpUt0/bq7X6XAwAAEux/Tv4fpQZTdc+ye7S4ZLHf5QDAiEFIPUDBQFBzR8+VJC3dudTfYgAAQEykhdJ0yrRTJNHyAwCAkWh6wXRdf+z1kqSvP/t1tbk2nysCgJGBkHoQjplwjCTprZK3fK4EAADEyicP+qQk6cEVD/pcCQAA8MP3T/y+xmaP1eLSxbp/+f1+lwMAIwIh9SAcP+l4SdKb2970uRIAABArn53zWaUGU/Xi5hdVWlXqdzkAACDBctJydMOpN0iSvvOP76imqcbnigBg+COkHoTjJh4nyZtJ7ZzzuRoAABALBRkF+tRBn1Kba2P2FAAAI9QlR1yio8cfre3V23XD6zf4XQ4ADHuE1IMwvWC6ijOLtbtutzZWbPS7HAAAECOXHH6JJOnuZXfzQzQAACNQwAL67Vm/lSTd+OaN2lSxyeeKAGB4I6QeBDPTcZPCs6m30ZcaAIDh4uxZZ6soo0grdq3QB2Uf+F0OAADwwXGTjtMXDv+CGlsbdfWTV/PDNQDEESH1IEVaftCXGgCA4SM1mKqLDr1IknT3B3f7XA0AAPDLr8/4tYoyivSPTf/QXR/c5Xc5ADBsEVIPUvviiSWE1AAADCeXHnGpJOn+5ferpa3F52oAAIAfirOKddNZN0mSrn/uepXVlPlbEAAMU4TUg3T0+KOVEkjR8rLlqmqs8rscAAAQI0ePP1oHFR2kstoyPb/xeb/LAQAAPvn8YZ/XWTPPUkVDhb727Nf8LgcAhiVC6kHKSMnQkeOOlJPT2yVv+10OAACIETPTZUdcJkn6/Tu/97kaAADgFzPTLZ+4RVkpWXp45cN6eOXDfpcEAMMOIXUMnDD5BEnSS5tf8rkSAAAQS1cfdbUyUzL1zIZntLxsud/lAAAAn0zJn6Jfnv5LSdJVf79KW/dt9bkiABheCKlj4OyZZ0uSnl7/tM+VAACAWCrKLNIV866QJN341o0+VwMAAPx0zYJrdM7sc1TZWKkvPPoF1qwAgBgipI6BEyafoKyULC3ftVzbKrf5XQ4AAIih64+7XgEL6P7l93OeBwBgBDMz3X7O7RqfM16vf/i6fvbPn/ldEgAMG4TUMZAWStNp00+TJD2z4RmfqwEAALE0rWCaLph7gVraWnTT2zf5XQ4AAPDRqMxRuvvTd8tk+vGrP9YbH77hd0kAMCwQUsfIx2d9XBItPwAAGI6+ffy3JUm3vner9jXs87cYAADgq1Onn6r/+Mh/qM216XN/+5x21+32uyQAGPIIqWMk0pf6xc0vqrGl0edqAABALB057kidOu1U1TTV6JYlt/hdDgAA8Nn/nPw/OmbCMfqw8kNd/MjF9KcGgEEipI6RSXmTdNjow1TTVKPXP3zd73IAAECMRWZT/3bxb/lBGgCAES4lmKK/XvBXjc4arX9s+od+8OIP/C4JAIa0hIfUZjbJzF42s1VmttLMvh4e/28zKzWzpeHt44mubbAiLT+eXPekz5UAAIBYO2PGGTp8zOHaWbNT9y671+9yAACAzybmTtTD5z+soAX1izd/wd8HADAIfsykbpH0LefcHEnHSrrWzOaE7/uNc25eeBtyzZ0/ffCnJUn3r7hfTa1N/hYDAABiyszaZ1P/8s1fclkvAADQR6d+VDeddZMk6conrtQ/t/7T34IAYIhKeEjtnNvhnHsvfFwtabWkCYmuIx6OmXCM5hbP1a7aXcymBgBgGLpw7oWalj9Na/es1Z1L7/S7HAAAkASuW3idvrrwq2pqbdK5D52rDXs3+F0SAAw5vvakNrOpkuZLWhweus7MlpnZHWZW0M1zrjKzJWa2pLy8PFGl9omZ6fJ5l0uS/rrqr/4WAwAAYi4lmKKfnfozSdKPXv6Raptqfa4IAAAkg9+c+Rt9YtYntKd+jz5x/ye0t36v3yUBwJDiW0htZtmSHpH0DedclaSbJc2QNE/SDkm/6up5zrlbnXMLnHMLiouLE1Vun5178LmSpKfWP8WiSgAADEMXzL1AR48/WjtqduhXb3X55woAABhhgoGgHvjMAzpizBFat2edznvoPNqAAkA/+BJSm1mKvID6Pufc3yTJOVfmnGt1zrVJuk3SQj9qG6wZhTN0+JjDVdVYpWc2PON3OQAADNhwXux4MAIW0I1n3ChJ+sUbv9CO6h0+VwQAAJJBTlqOnvzckxqfM16vbn1Vlzx6iZpbm/0uCwCGhISH1GZmkm6XtNo59+uo8XFRDztX0opE1xYrlx5+qSTp/5b+n8+VAAAwKMN2sePBOmnKSVo0e5Fqm2v1o5d/5Hc5AAAgSUzMnai/X/x35aTm6OGVD+v8v5yvhpYGv8sCgKTnx0zqj0i6RNIpnWZg/cLMlpvZMkknS/qmD7XFxBcO/4JSAin6+9q/a83uNX6XAwDAgAznxY5j4f+d9v8UCoR0+/u369Utr/pdDgAASBJHjjtSL176ogrSC/TE2id0zgPnqK65zu+yACCpJTykds697pwz59zh0TOwnHOXOOcOC4+f45wbstfOjskeoy/O+6KcnH72z5/5XQ4AAIM23BY7joXZo2br+yd8X05OX3z8i6ppqvG7JAAAkCSOnnC0Xrn8FY3OGq0XNr2gU+8+VeW1w+/vIQCIFd8WThzuvnvCdxW0oO5ffr827N3gdzkAAAzYcF3sOBZ+cNIPNG/sPG3et1nXPX2d3+UAAIAkcviYw/Xq5a9qUu4kvV3yto69/ViutgaAbhBSx8m0gmm67IjL1Opa9bVnvibnnN8lAQDQb8N5seNYSA2m6t5z71VGKEN3fXCX7lx6p98lAQCAJHLwqIO1+N8W66hxR2lTxSYdf/vxemXLK36XBQBJh5A6jn566k+Vn56vZzY8o4dXPux3OQAA9MtIWOw4FuaOnqs/fuKPkqSvPPUVrdg1oj8OAECCmdkkM3vZzFaZ2Uoz+3p4/L/NrLTTWlDwwbiccXr18ld1zuxzVNFQoTPuOUN3f3C332UBQFIhpI6jsdlj9YvTfiFJ+vqzX1dFfYXPFQEA0C/DfrHjWLl83uW67IjLVN9Sr8/+5bP0pwYAJFKLpG855+ZIOlbStWY2J3zfb6LXgvKvRGSlZulvF/xN3zjmG2pua9Zlj12mbz//bbW0tfhdGgAkBULqOLvyyCt1wuQTVFZbpu/84zt+lwMAQJ+NhMWOY+kPH/+D5hTP0Zrda/SVp75Cqy8AQEI453Y4594LH1dLWi1pgr9VoSvBQFC/Oes3+v3Zv1fQgrrxrRt19n1na0/dHr9LAwDfEVLHWcACuvWTtyolkKLb3rtNz2983u+SAABAHGSlZukvn/2LMlMydc+ye3TH+3f4XRIAYIQxs6mS5ktaHB66zsyWmdkdZlbgX2WIdu3Ca/XipS+qOLNY/9j0Dy24bYGW7lzqd1kA4CtC6gQ4pPgQ/fCkH0qSLvzrhXqn9B2fKwIAAPEwp3iO/vhxrz/1l5/6sp5a95TPFQEARgozy5a32PE3nHNVkm6WNEPSPEk7JP2qm+ddZWZLzGxJeXl5osod8T469aN696p3dfT4o7Vl3xYd++dj9aclf+JKLAAjFiF1gvzgpB/oM4d8Rvsa9umMe87QsrJlfpcEAADi4LJ5l+n6Y69Xc1uzznv4PK6iAgDEnZmlyAuo73PO/U2SnHNlzrlW51ybpNskLezquc65W51zC5xzC4qLixNXNDQpb5Je++Jr+rf5/6bG1kZd89Q1OufBc1RaVep3aQCQcITUCRKwgB74zAM675DzVNlYqTPvPVObKzb7XRYAAIiDG8+4Udcefa2aWpu06MFFemnzS36XBAAYpszMJN0uabVz7tdR4+OiHnaupBWJrg29Sw+l67ZzbtN9592nvLQ8PbnuSc354xzd9u5tzKoGMKIQUidQSjBF9513n06eerJ21uzUGfeeoV21u/wuCwAAxJiZ6Xdn/05XHXmVGloa9KkHPqV/bv2n32UBAIanj0i6RNIpZrY0vH1c0i/MbLmZLZN0sqRv+lolevS5wz6nVdeu0jmzz1FVY5WuevIqnXbPadpUscnv0gAgIQipEyw9lK7HLnpM88fO14a9G3TyXSdr496NfpcFAABiLGAB3fzJm3X5vMtV11ynj9//cT225jG/ywIADDPOudedc+acO9w5Ny+8Pe2cu8Q5d1h4/Bzn3A6/a0XPxueM12MXPqYHPvOARmWO0kubX9JhNx+m3779W7W2tfpdHgDEFSG1D3LTcvXM55/RIaMO0aryVTr6tqN19wd3cykPAADDTMAC+vOn/qwvHP4F1TTV6NyHztW/PfFvqm+u97s0AACQhMxMFx16kVZ9ZZUuPvRi1TXX6RvPfUMn3XmS1uxe43d5ABA3hNQ+GZM9Rm9d+ZY+ddCnVNFQocseu0wn/t+JWl2+2u/SAABADAUDQd316bt005k3KT2Urtvfv10fueMjrE0BAAC6VZxVrPs/c78ev+hxjcsepze3val5t8zTz//5czW1NvldHgDEHCG1j/LS8/T4RY/r7k/frTFZY/TGtje04LYFuvuDu/0uDQAAxFDAAvr6sV/X21e+rekF0/X+zvd11K1H6eGVD/tdGgAASGLnzD5Hq65dpSvnX6nG1kZ9/6Xva84f5ugvK//C1dgAhhVCap+ZmS454hKtvW6tLjn8EtU11+myxy7TJY9ewqU8AAAMM0eMPUJLvrSk/UqqC/96oS559BLta9jnd2kAACBJ5afn68/n/FkvXPKCDh51sDZWbNQFf71Ax91+nF7Y+AJhNYBhgZA6SeSl5+muT9+l28+5XRmhDN277F4d8odD9M1nv6ldtbv8Lg8AAMRIQUaBHr/ocd38iZuVmZKpe5fdq7l/nKs/v/dnNbc2+10eAABIUqdNP03Lv7xct3ziFo3JGqPFpYt1xr1n6CN3fETPb3yesBrAkEZInUTMTFfMv0IfXPOBPn/Y5xW0oG5afJMm/nqizn/4fD2x9gl6TwEAMAyYma5ZcI3ev/p9HTPhGG2v3q4v/f1LmvPHOfrrqr/yJRMAAHQpFAjp6gVXa8PXNujnp/5cRRlFeqvkLZ1575k6/o7j9eyGZ/k7AsCQREidhGYVzdK9592r1694XYtmL1Kra9Ujqx/RogcXacKvJ+irT39VS3cu9btMAAAwSAcVHaQ3r3xTD37mQc0qnKUNezfos3/5rE78vxP1+oev+10eAABIUtmp2fruCd/Vlm9s0Q2n3qBRmaP0dsnbOvu+s3Xs7cfqvmX3qaGlwe8yAaDPbCj/wrZgwQK3ZMkSv8uIu5KqEt2//H7d/cHdWlm+sn38o1M+qk/M+oTOO+Q8zSic4WOFADB8mdm7zrkFftcx1I2Uc/ZgtLS16M/v/Vk/evlHKq8rlySdNOUkfenIL+ncg89VVmqWzxUCQPLifB0bnK+HrpqmGt38r5v1yzd/2f53RFFGkb4474u68sgrdfCog32uEAB6Pl8TUg8hzjkt3blU/7f0/3Tbe7e1/yqaEkjR5w77nI6beJxOnX6qZhbO9LlSABg++NIbGyPtnD0YVY1V+tWbv9JNi29SVWOVJG+21AVzLtDl8y7XCZNPkJn5XCUAJBfO17HB+Xroq22q1X3L79PNS27ucAX28ZOO1xXzrtBn535WuWm5/hUIYEQjpB6GKuor9PzG5/W3NX/Twysf7nDf9ILpunDuhTp/zvk6YswRCgaCPlUJAEMfX3pjYySfsweqsqFSD654UHd+cKfeLnm7fXxq/lSdd/B5Om7ScTpt+mnKT8/3r0gASBKcr2OD8/Xw4ZzTO6Xv6M/v/VkPrnxQNU01kqT0ULoWzV6kzx/2eZ0x4wylhdJ8rhTASEJIPcy9t+M9vbb1Nb257U39Y9M/VNFQ0X7frMJZumDuBZo/dr4WTlioSXmTfKwUAIYevvTGBufswVm7e63u+uAu3bPsHpVUlbSPpwXTdObMM3XcxOO0cMJCLRi/gNlRAEYkztexwfl6eKptqtVfVv1Fd31wl17Z8kr7eF5ans495FxdNPcinTLtFKUEU/wrEsCIQEg9grS2teqNbW/o7g/u1jMbntH26u0d7p9RMEPfPPab+visj2tK/hQFjLUzAaAnfOmNDc7ZsdHm2vTa1tf00uaX9NrW1/Ta1tfktP9vOZPpkOJDtHDCQi0cv1ALJyzU3NFzlR5K97FqAIg/ztexwfl6+Nu6b6vuX36/Hlz5oJaVLWsfz0/P1xkzztDHZ35cZ808S2Oyx/hYJYDhipB6hGpqbdLf1/5dS7Yv0Xs739PiksWqbKxsvz87NVsfmfQRnT3zbB0+5nDNHzefS4YBoBO+9MYG5+z42Fa5Ta9seUXvlL6jxaWLtXTnUjW3NXd4TMACmlEwQ3OK52hu8VzNHT1Xc4rn6OBRBxNeAxg2OF/HBufrkWXN7jV6aMVDenDlg1qze02H+44cd6TOnnm2zphxhhZOWMjfDABigpAakrzZV4+sekR3fXCX3t3xrnbW7Oxwv8k0b+w8fXTKR3XMxGO0cMJCTcufxuJMAEY0vvTGBufsxGhsadQHZR+0h9ZLti/R+j3r1epaD3hswAKaXjBdh40+TIePOVwzC2dqZuFMzSiYoVGZozj/AxhSOF/HBufrkWvD3g16Zv0zembDM3p5y8tqaGlovy8tmKZjJh6jkyafpJOmnKTjJh2n7NRsH6sFMFQRUqNLZTVlemLtE1qyfYk+KPtA7+1474DZV6MyR7VfMnzMxGN09PijVZRZ5FPFAJB4fOmNDc7Z/mlsadS6Peu0snylVpWv0srylVq5a6U27N3QZXgtSblpuZpRMEMzC2fq4FEHa3bRbE3InaBDRh2i0VmjCbABJB3O17HB+RqSVN9cr1e3vqqn1z+tl7e8rBW7VnS4P2hBHTnuSJ00xQutT5h8ggozCn2qFsBQQkiNPqlrrtPbJW/r9Q9f1zul7+id0ndUXld+wOMis67mFM/RKdNO0dHjj1Zeep4PFQNA/PGlNzY4ZyefSHi9rGyZVpav1MaKjdqwd4M27t3YoT1YZzmpOZqSP0UTciaoOKtYxZnFGpc9TuNzxmtczrj249y0XMJsAAnD+To2OF+jK3vq9uiNbW+0r4fx3o73Dvih+6CigzR/7HwdOe5IzR87X/PHzdeozFE+VQwgWRFSY0Ccc9paudW7ZLhksd7Z/o7e3f6u6lvqD3js6KzRmpY/TVPzp+7fF3j7yXmT6V8FYMjiS29scM4eOpxz2lO/Rxv3btS6Peu0Zvcard+7XtuqtmlV+SpVNVb16XUyQhkanzNe43PGa3rBdM0omKFxOeNUnFncHm4XZxUrLy2PMBvAoHG+jg3O1+iL6sZqvVXyVntovbh0sZpamw543KTcSZo/br7mjZnXvibGQUUHKTWY6kPVAJIBITVipqWtRavLV2v17tV6b8d7em7jc1qze02HflWdBS2oYyce2/5r6sGjDtahow9VblpuAisHgIHhS29scM4eHpxzqmio0JZ9W7SzZqd21+1WWU2ZdtTs0Pbq7e377dXbVddc16fXTAmkaHrBdM0snKlRmaNUmFGo4sxijc0eq3E54zQ2e6xGZ41WYUYhP3oD6Bbn69jgfI2BaGxp1MrylXp/x/t6f+f7em/He/qg7IMu/xYIWlCzimZ5izgXHawp+VM0OW9y+0ava2B4I6RGXLW5Nu2o3qHN+zZry74t2lyxef/xvs36sPJDtbm2Ds8JBUL6yKSPaG7xXM0onKEJORM0NnusDin2el0CQLLgS29scM4eWZxzqm6q1o7qHSqpKtHGio3aVLFJZbVlKq8t1+663SqvK1d5bbmqm6r7/LqZKZkqyijS7FGzNTZ7rIoyijQqc5TG54zXoaMP1bT8aSz6CIxQnK9jg/M1YqW1rVXr967X+zve1wdlH2j17tVauWulNlVsklP3OVRBekGH0LrzNi57nIKBYAL/JQBiiZAavqpsqNSrW1/V2t1r9a/t/9LGio36YOcH3S7WlJ+er4m5EzUpd5KOmXCMpuRP0bjscTp24rH0vgaQcHzpjQ3O2ehOfXO91u1Zpy37tmhP/R7tqdujXbW7VFbrzdDeUb1Du+t2a0/9HrW0tfT6ekELKi89T3lpecpPz28/zkvPU35avkZljtLorNEqzir29pnePj89n3AbGMI4X8cG52vEW31zvdbuWatV5au0bs86fVj5oT6s/FBbK7dqW+U2NbY29vj8UCCkCTkT2kPrSbmTNCZ7jEZnje6wjcocpVAglKB/FYC+IqRG0tlbv1dvbnuzfYGmyOXBy8qWqba5tsvnmEyjs0ZrQu4ETcgJb7kH7ultCSCW+NIbG5yzMVjOOdU01aistkxrdq/xguu6PdpTv0eb923Wyl0r9WHlhz0u+tiTUCDU3ic7Orxu32cVa0zWGE3MnahxOeP44gskGc7XscH5Gn5yzqm8rrw9uO5qK6st69NrRfKD8TnjNSF3gsZnj29f5HlU5iiNyhyloowiFWUWqSijSCnBlDj/6wBIPZ+v+esavijMKNQnD/rkAeORxZpKq0q1fu96vV3ytspqy7SpYpMWlyxWWW2ZymrL9N6O97p97TFZY3TMxGM0JW+KpuRN0dT8qZqS7+2LMooIsAEAGILMTDlpOcpJy9HMwpndPq6ptUmVDZWqbKzssN/XsE/7Gva1txrZVbtr/762XJWNld7M7ZodvdYSsIDG54zXmKwxyk3LVU5ajnLTcpWbmqvctFzlpeepMKNQRRlFKswo9I7DX4LTQ+n8LQIAQBfMrH0m9ILxXf/m1NDSoJKqEm2r3KYPKz/Utqpt2lW7q8N5PXJuj+QH7+98v9f3zk3L7RBaR4fYozJHqSC9QPnp+SrI8PaRjfUygNghpEZSMbP2XzWPGHuEzp9zfvt9LW0tKqspU0lViUqrS1VaVerto4+rSlVWW6Yn1j7R5etnpmTuD66jAuwpeVM0KW+SCtILlJmSyZdHAACGqNRgqoqzvBnR/dHY0qjddbsPCK+jb++s2altVdu0s2anSqpKVFJV0u/6AhZQeihd+en5Gps9VmOzx2pM1hjlpeV5IXxqjrJSs5Sdmq2slKwujyNbwAL9fn8AAIay9FC6ZhbO7PEHa2l/fhBZ0Lm0utRb5Ll6h/bU72lvJRa5KquqsUpVjVXavG9zv+uJBNaRIPuA43CwnZeWp+zU7PbzfWTPLG7AQ0iNISMUCHltPXIndPsY55xW716tNbvXaMu+Ldq6b6u2VIb3+7aosrFSq3ev1urdq7t9jayULI3LGdd+EomcWPLS8jrMhOq8z0jJiMc/GwAAJEBaKK3XvzMimlqbVFpVqt11u9u/1FY3Vbcf72vYp731e7W3fq/21O/x9nXevrG1UXXNdaprrtP26u2DqjnyBTc3LfeAv1sK0gtUkFGgvLQ8ZaVmKTMlU1kp3j7ynMjz04Jp/EAPABhW+pIfRLS5NlU2VLaH1tEBduS4oqGi/aqsyFZRX6GGlgbtrNmpnTU7B1xrWjCt/cqsnNScDsfRP1T3dZ+dmq3MlEx+zMaQQ0iNYcXMNKd4juYUz+ny/n0N+7R131ZtrdzaHlxvrfT2pdWl2tewT7XNtdqwd0O/3zs9lK7CjEIVZxZrVOYoFWcVa1RGuNdVZpGKM4tVlFnU4VfVvPQ8eloCADDEpAZTNa1gmqYVTOv3c1vbWlXfUq+K+or2hSHL68pV2VCp6qZq1TTVqLapVjXN3r62uXb/WFNN+1bbXKvqpmpVN1UPOuwOBULtX4Q7z+5q34ePoxekjHwJjoTfkS0jJYO/bwAAQ0bAAirI8H7c7W2GdjTnnOpb6tsD6/bwOirQjoxXNFS0/6hd3eid7yPHja2NKq8rV3ldeUz/XRmhjN5D7X4G4JE953nEA/+twoiSn56v/LH5OmLsEV3e75xTVWOVdtbsVGWj178y0seyoqGifSbUnvo9HWZG7anfo4aWhvZLifojOzW7w6VAkcuBCtIL2mdBRfZZKVlKC6UpPZSu9FC6MlMyvR6YabmcJAAAGAKCgWB7u45JeZMG/Dqtba3tX3Ajvbcjf7dUNFSoor5CFQ0VqmyoVF1LXfvs7UjIXdVYpepGL+Ruam3yntNQEbN/Z2owVRmhjA7hdW9b57C7t43+3gAAP5lZ+zlpfM74Ab2Gc04NLQ0dAuzoK7QiP1gfsA8fR3647nx/XXOd6lvqVd9Sr93aHeN/uXee7xxcZ6Zktp/7M1Iy9h+HMpSRkqH0ULoyQuF9+P7Icef7Ot9mVvjIQKoFRDEz5aXnKS89r1/Pi/yCuqduj8rryr1FmWq9X0Ijlwjtrt+tvfV7D/hFNfJlcVvVtkHVHgmsIwF2WtALsyOhdvTt9uO+PKbTfanB1B63YCA4qH8HAADoXTAQbP+bZWLuxEG9VlNrU3tg3Xl2V+d9JAyPXH0W+TJc31zfHoTXNdepqbXJW8SysTJG/+Ku9RZkZ4Qy2r8c97bPSgn3/w5/0Y78TZURylBqMJVAHAAQc2bmnYtSMjRGY2L2um2uTfXN9V0H3N3sI1duRYfg3e0j5/lY/sDdk5RAijJSMpQWTFNaKE2pwVSlBdPaA+3oUPuA4+4e0ykM7+6xoUCIvwESJOlCajM7S9JvJQUl/dk5d4PPJQG9av8FNS+zX7Oi2lybappqDrg0KDL7qX0fPq5vqVdDS0P7VttU2/4La+RLod8CFlBqMFUpgZQO4XX0iaS3oDveWygQUtCCCgaC/CILABjxUoOp3hobmUUxeT3nnBpbG9u/HHcOsPu0tfT+mIaWhoT+/RP50tp5i/5y29P9kb+Fuvp7KCWYopRASod95O+pzvd1fnzQgnx5BgB0ELCAN8s5NUvKiu1rR2Z/dw6u61vq28/5keP6lvr2c3Z9s5dnRHKNyGM6jDV3fV9zW7OaG5tj+w/po4AFBh569+F5kfsiEwOjJwumBFJG1Dk+qUJqMwtK+oOk0yWVSPqXmT3hnFvlb2VAfAQs0N6uY4qmDPh1nHOqa65TZWOlGloa1NjS6O1bGw+43dN97bdbu3+N5rbm9l9NO2+NLY1qc21eiK6GGH5S8RUdWnfeR+4LBUIH3I4e6/wakfu6e92gdTwOWKA9NA9YoH2sL+O9PdbMvL1MZiaTtY/3dNz5ed0dJ+vrZadm+/1fLQAYkcys/UtWQUZB3N4n0t+7tzA78iW5233Ul+rIFW6dJwY0tTa1HyejrsLrnoLtboPwfgbkA3qPTo/PCGVwJR4ADCHRs79HZY6K+/s559TU2qT6lno1tjSqsbWx/bzc2NLY4ZzdOfSOHjtgvJfHRP5OaHNt7bPL/RB9ZXtXV7t3eXV8F/d19eN5V+fnyLk7Mms9so9cWR9PSRVSS1ooaYNzbpMkmdmDkhZJIqQGemBm+38l9VlrW2vH4Lq1Uc2tze0nkshJpbm1+7C7T1tbx9t9fb3G1ka1tLWota1Vra5VktTS1qIWtUitPn94iJm0YJoa/jM5g4ThgiufAPgtur93vLX/CN/LFv0lt8N4S33730HRf5NE/iZqbmvusG9qbTpgrLv72lybd7utWfJnktmgPHz+w/rs3M/6XQYAIEmZmReWhtJ8ef+Wtpbug+5+jNc316uh1dtHrjjr/LdCJC+JjLW0tbQfx7uFWm8WzV6kxy56LK7vkWwh9QRJ0Y15SyQdE/0AM7tK0lWSNHny5MRVBqBPgoGgMgLer6pDQZtr6xBad95H3xc5bmlr8Y67GYvc7ur1utq3uTa1Om/f5trax6LHuxrrcTzqeU5Ozjm1ubb2Y6fw7V6OOz+vu+Nke720oD9/wIwUXPkEYKQJWKC9z3WyaXNtPQbbkR/zBxOE9/hag3wPv0IHAAD6IhQIKSctRzlpOQl/79a21g5XxPd0VXx3V8pHwu/2QLx1fzDe03k+EphH9omYFJlsIXWvnHO3SrpVkhYsWOB8LgfAEBfpoS2uMgX6gyufACBJBCzg6wwzAAAQH8FAUJmB5PyRPB6SbcWwUknRq85NDI8BAIDk0dWVTxN8qgUAAAAAMMQlW0j9L0mzzGyamaVKukjSEz7XBAAABsDMrjKzJWa2pLy83O9yAAAAAABJKqlCaudci6TrJD0nabWkh51zK/2tCgAAdNKnK5+cc7c65xY45xYUFxcnrDgAAAAAwNCSVCG1JDnnnnbOHeScm+Gc+6nf9QAAgANw5RMAAEOAmZ1lZmvNbIOZfdfvegAA6E7ShdQAACC5ceUTAADJz8yCkv4g6WxJcyRdbGZz/K0KAICuhfwuAAAADD3OuaclPe13HQAAoFsLJW1wzm2SJDN7UNIiSat8rQoAgC4wkxoAAAAAgOFngqRtUbdLwmMdsNAxACAZEFIDAAAAADBCsdAxACAZEFIDAAAAADD8lEqaFHV7YngMAICkQ0gNAAAAAMDw8y9Js8xsmpmlSrpI0hM+1wQAQJdYOBEAAAAAgGHGOddiZtdJek5SUNIdzrmVPpcFAECXCKkBAAAAABiGnHNPS3ra7zoAAOgN7T4AAAAAAAAAAL4hpAYAAAAAAAAA+IaQGgAAAAAAAADgG3PO+V3DgJlZuaStMXq5UZJ2x+i1hjs+q77hc+o7Pqu+47Pqu1h9VlOcc8UxeJ0RLYbnbP430Hd8Vn3HZ9V3fFZ9w+fUd5yvkwjfsX3B59R3fFZ9x2fVd3xWfRP38/WQDqljycyWOOcW+F3HUMBn1Td8Tn3HZ9V3fFZ9x2c1PPGfa9/xWfUdn1Xf8Vn1DZ9T3/FZDV/8Z9s3fE59x2fVd3xWfcdn1TeJ+Jxo9wEAAAAAAID/v737C7W0qsM4/n0w/4SFppWEI9iQEHNRU4gYeWETxWSREV4UQl4MeBNkUYQRRF12YU6BBFIxCtE/KxKvsnGgq7TMUcesHEOoYWog/9WNNfXrYq/RPWfOObN2zpy1Pe/3Ay9nv2vvGdZ59qzzDGvv/R5JGsZNakmSJEmSJEnSMG5Sv+T20RN4BTGrPubUz6z6mVU/s9qcfF77mVU/s+pnVn3MqZ9ZbV4+t33MqZ9Z9TOrfmbV57Tn5DWpJUmSJEmSJEnD+E5qSZIkSZIkSdIwblJLkiRJkiRJkoaZ/CZ1kp1J/pDkYJKbR89ntCTfSXIkyYG5sQuS3Jvkifb1dW08Sb7RsnskyTvHzXzjJbkkyb4kv0vyWJKb2rh5rZDknCQPJHm4ZfWVNv7mJPe3TH6Q5Kw2fnY7P9juv3ToN7DBkpyR5KEk97Rzc1pFkqeSPJpkf5LftDHX3yZmZx/Pzu5jX/ezrxdjX/exr6fHvj6efd3Hvu5nXy/Ozu4zurMnvUmd5AzgNuADwDbg40m2jZ3VcHuAnSvGbgb2VtVlwN52DrPcLmvHjcA3N2iOy+Io8Nmq2gZcCXyy/fsxrxO9AOyoqrcD24GdSa4EvgrcWlVvAZ4BdrXH7wKeaeO3tsdNyU3A43Pn5rS291TV9qq6vJ27/jYpO3tVe7Cze9jX/ezrxdjX/ezribCvV7UH+7qHfd3Pvl6cnd1vWGdPepMauAI4WFV/qqp/Ad8Hrh08p6Gq6pfA0yuGrwXuaLfvAD4yN35nzfwKOD/JmzZkokugqg5X1W/b7X8w+4F3MeZ1gvY9/7OdntmOAnYAd7XxlVkdy/Au4L1JsjGzHSvJFuCDwLfaeTCnRbj+Ni87ewU7u4993c++7mdfv2yuv83Lvl7Bvu5jX/ezrxdjZ79sG7YGp75JfTHw57nzv7QxHe+iqjrcbv8VuKjdNr+mfQTkHcD9mNeq2sdr9gNHgHuBJ4Fnq+poe8h8Hi9m1e5/DrhwQyc8zm7g88B/2/mFmNNaCvh5kgeT3NjGXH+bl89hH9fAOuzrk7Ovu+3Gvu5lX0+Lz2Ef18A67OuTs68Xshs7u9fQzn7Vy/nDmp6qqiQ1eh7LJMlrgB8Dn66q5+dfZDOvl1TVf4DtSc4Hfgq8deyMlk+SDwFHqurBJFcPns4rwVVVdSjJG4F7k/x+/k7Xn6bONXA8+7qPfX1y9vXC7GtpHa6B49nXfezrPnb2woZ29tTfSX0IuGTufEsb0/H+duwt++3rkTY++fySnMmsQL9bVT9pw+a1jqp6FtgHvIvZx0GOvVg2n8eLWbX7zwP+vrEzHeLdwIeTPMXso5E7gK9jTquqqkPt6xFm/zG7AtffZuZz2Mc1sAr7enH29brs6wXY15Pjc9jHNbAK+3px9vVJ2dkLGN3ZU9+k/jVwWWa/1fMs4GPA3YPntIzuBm5ot28AfjY3/onMXAk8N/cRgE2vXZfo28DjVfW1ubvMa4Ukb2iv8JLk1cD7mF1jbB9wXXvYyqyOZXgdcF9VbfpXzKvqC1W1paouZfbz6L6quh5zOkGSc5O89tht4P3AAVx/m5md3cc1sIJ93c++7mNf97OvJ8m+7uMaWMG+7mdf97Oz+y1FZ1fVpA/gGuCPzK7f88XR8xl9AN8DDgP/ZnY9mV3Mrr+zF3gC+AVwQXtsmP3m5ieBR4HLR89/g7O6itn1eh4B9rfjGvNaNau3AQ+1rA4AX2rjW4EHgIPAj4Cz2/g57fxgu3/r6O9hQGZXA/eY05r5bAUebsdjx35+u/4292Fnn5CHnd2Xk33dn5V9vXhm9vX6+djXEzzs6xPysK/7crKv+7Oyr/+/3Ozs9fMZ3tlpf7EkSZIkSZIkSRtu6pf7kCRJkiRJkiQN5Ca1JEmSJEmSJGkYN6klSZIkSZIkScO4SS1JkiRJkiRJGsZNakmSJEmSJEnSMG5SSxOQpJLcMnf+uSRfHjglSZK0gn0tSdLys6+l08NNamkaXgA+muT1oyciSZLWZF9LkrT87GvpNHCTWpqGo8DtwGdGT0SSJK3JvpYkafnZ19Jp4Ca1NB23AdcnOW/0RCRJ0prsa0mSlp99LZ1iblJLE1FVzwN3Ap8aPRdJkrQ6+1qSpOVnX0unnpvU0rTsBnYB5w6ehyRJWttu7GtJkpbdbuxr6ZRxk1qakKp6GvghsyKVJElLyL6WJGn52dfSqeUmtTQ9twD+FmJJkpabfS1J0vKzr6VTJFU1eg6SJEmSJEmSpInyndSSJEmSJEmSpGHcpJYkSZIkSZIkDeMmtSRJkiRJkiRpGDepJUmSJEmSJEnDuEktSZIkSZIkSRrGTWpJkiRJkiRJ0jBuUkuSJEmSJEmShvkfMVoutYZu/LcAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 1800x1080 with 6 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = plt.subplots(figsize = (25, 15), nrows = 2, ncols = 3)\n",
    "\n",
    "for ind, eta in enumerate([1, 0.75, 0.5, 0.2, 0.1, 0.05]):\n",
    "    gb = GradientBoostingRegressor(n_estimators = 500, max_depth = 5, learning_rate = eta, random_state = 123)\n",
    "    gb.fit(X_train, Y_train)\n",
    "\n",
    "    rmse_test = [np.sqrt(metrics.mean_squared_error(Y_test, predict)) for predict in gb.staged_predict(X_test)]\n",
    "    rmse_train = np.sqrt(gb.train_score_)\n",
    "        \n",
    "    axs[ind//3][ind%3].plot(rmse_train, linewidth = 2, color = 'green', label = 'GB Train')\n",
    "    axs[ind//3][ind%3].plot(rmse_test, linewidth = 2, color = 'red', label = 'GB Test')\n",
    "    axs[ind//3][ind%3].set_title(\"Зависимость ошибки GB от шага: eta = {s[0]}\".format(s = [eta]), fontsize = 10)\n",
    "    axs[ind//3][ind%3].set_xlabel(\"N\", fontsize = 10)\n",
    "    axs[ind//3][ind%3].set_ylabel(\"RMSE\", fontsize = 10)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из представленных выше результатов можно увидеть, что уменьшение размера шага позволяет уменьшить ошибку на тестовой выборке. При этом есть и неприятный момент - чем меньше шаг, тем ниже скорость сходимости.\n",
    "\n",
    "Другой метод борьбы с переобучением - *стохастический градиентный бустинг*. В данном случае, каждый из алгоритмов обучается не на всей выборке, а лишь на какой - то ее части, которая выбирается случайным образом для каждого из алгоритмов."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmQAAAGBCAYAAADIcK0QAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjMuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/Il7ecAAAACXBIWXMAAAsTAAALEwEAmpwYAAB0JElEQVR4nO3deXxV9Z3/8dfn3tzsLAHCvhtEWWLQBLW2FHeL/sClY61oodZlajtaO7XV1tZxqpWWTm0pjugoVeuGMnWptipoKeqoCIrIpmEn7HtIyHKX7++PcxJu9gBJboD3U/O4Z/me7/mchZtPvuec7zHnHCIiIiKSOIFEByAiIiJyvFNCJiIiIpJgSshEREREEkwJmYiIiEiCKSETERERSTAlZCIiIiIJpoRM6jCzk8xsgZl9ZGaLzewxM0tPdFzHMjMLmtmPzez/zOxjM7uhherNNLOHzGy1X++iqrrNbKCZlfnH+FN/3UNbYr1HIzMbbmbv+Of+N1u47jQzu9/MPvD397iWrF/qd6zsdzPrYmYP+ufmZ2Z2SqJjkpZn6odMajOzTnjnxl5//AFgh3PuVwkN7BhmZr8EYsAU51xZC9b7HLAGuMs5FzOzbOA659yvzWwg8KpzboRf9ibgS865SS21fvGY2ZPAu8CfnHPhRMdzvDhW9ruZvQE8DLzsnIsmOh5pHWohkzqcc/vikrEAkApUjd/gt5x9amb/W9VyZmaPm9laM1tqZkvMrOqX/Dwzy/eH7zWzkqr1mNlP/L/2PjWzKY2VN7OxZubM7CJ/PMtv3fkPfzzP/yt4iZm9aGZZ/vQcM5vrr+NjMzvBzJ72/1re7ce82Mz+1cwmm9n0xvaNmaWa2Z/8uD8xs7P96dXLmlm+mc3zh5PMbGfcNrzqD3cxs71m9iO/6onAV4AFZvaWmfWP269f94cfitve+OnX+/umW61YTwBG4ydj/rHd4Zz7dQOb1xHYU882V8ftj/8oLo56z4e4smn+/l1sZpX+flvs76Nsf5mP/J+z/GUy4/bxEjO7wsxu85fbYGY7/OFH/fI/9M+7pWb2A39afOvfGjP7rT/dzGyqX/YzM/tGM45N/PY0eZ7HlZ3unxeZwFjgOuDjWudnQ+ftPDP7gx//UjMb7U//j6q4zPv38yd/OMPMZprXgvKJmU2oJ/aGtr3efw8NnQNm1s3M1sXNK/K3ETN7ybxW2GVmdmNcmWjceTC3OedOPdt7nn+e59dT7lx/uz/z90NKQ/vdzE7349jawDFu6Lz8D/+c6uyPf82PZ6w//pCZLfS3/Z7a9cYd18/NbLl/3Hv709dZ3X+/r/r7fRgwAPgFsLhq+xra7rj6fuNPX2BmOf70w/4+kdanhEzqZf4vUmAHcArwP/6svzjnCpxzpwArgO/ELXa739oyHzinVn3dgXPjxr8GTABO9+v6TWPlfR8D3/KHrwY+jZv3JPAT51wu8Blwtz/9aeBBfx1fArY45yY65/KAV/yY85xzM5reKwB8D3DOuZHAN4EnzCy1mcvGuxPYEDc+CHjCr/dpYFp8YTP7BRBwzv1HrempwL8C2+tZx3Dg06pkrAEn+L+cVgM/BH53iNvR2PmAc67M3795wGbgbH98IfAH4AHnXAFwBfCov9jPgX3OuZH+8XzbOfeAX8cvgFl+Hdeb2WnAt4HTgTOAG8xslF/Pan+ZM4HJ/rTLgTy8c/o8YKqZ9aq1TbWPTW0NnucN6Ar0wzs/R1Lz/GzovAVI9+O/GZgZX6GZfQsvga+6tP0zvP00Gjjb366MWnHUu+1H+O+htuucc6cB+cAtZtbVn159HjjnzvOnNXru1OMXwKraE/1/A48D3/D3bxLwXRrY7865D/3tbWgbGzovAZbgffeA9120KG7ez5xz+UAu8FUzy22g/ol4/zZ34O2npmTjfT9cGb99jWx3lX3+9OnA7+MrPMzvE2llSsikXlW/SIEeeInPz/xZI8y7z+YzDn6xVJlqZoXAeOCFWlX+HIi/5Hke3mWEA/76djdRHmALkGJmXfx1vALVl1g7O+f+6Zd7AhhjZh2APs65F/11lFetrxHf8BOUj8zsknrmfxl4yq9vJbAeOLGJOmswsz54ycOLcZNjwDP+8J/99VSZjLf/f15Pdd/D294mL3Oa2c/8bdscN3m1/0vyBOAHwCMNLP6VqhYO4La46Y2dD005D5ju1/kK0NFv1TgPeLCqkHOuTqtdnC8DLzrnSp1zJcBf8BIV8JNN4Au8X7JV5Z91zkWdc9uAfwIFVZU1cGxqa+g8r2ppesX/g6K6WmBjPednvedt3HLP+ts/H2/fdPann4eXJPzcORfxp10A3OFv7zy8Vu3+teJudNtbyC1m9inwAV4yNKSRss0+d8zsCuAjYFM9s4cCa51zX/jjVfux3v1ea9nb/GP2npmd4U9r6LzEHx/vJ5ohYGtcXVea2cfAJ/62DGtgc54G1uK1es2Nm/4P81oLnzKztPjNBz6sZ/sa2u4qz8Z9nhk3fTIt8H0iLU8JmTTK/8J/joNf3I8D3/f/8roH74u/yu3OuSHAf/rzqgwERjjn/trM1TZW/hng18DnQGUz6zsUs/xE9Gq8ezZaw93AL4H4Gzj3N1K+C14S9Nta0zsCV9FwnMuBU8y77Ixz7j5/2zo2UP4V6v7CqvJOXEvXA3HTH6fh86EpAeCMuJaTPn5S1VKqWsh6Ad80s37NWKa+Y1NbQ+d5VUvTErzktkrxIcQcr3YMVeODgWuA35mZ+dMMuCJuX/Z3zq04zPUeFv/S3XnAmX6r1yc0fj48TvPOnSBwO3D/IYbUnP0e3/Ja1Trc2HlZART6sVT9AYWZDQJ+BJzrt3a+1sj2THTODcT79/aDuOln47VgOuDaQ9yO+rgGhg/3+0RamRIyqcPMhtjBe5gMryVggT+7A7DFzEJ4f9XWpxiIv//gbmpeigGYA3zbDt6D1qWJ8lX+Cowi7hKOc24fsMfMqlpGrgX+6ZzbDxSZ2aX+OlKs+U+L7sa7BFDbO/jbbWYn4rVCfN7MOgFOAAY6596sNf0jvC9D/PrfiZv3O+fcfwO9zeyCuOm3AX90ztWbmDrnVgELgXvNLOjHnIr3y7s+XwZWH8K2QPPOh4a8Cfxb1YiZ5fmDc/D+Uq+antVIHe8Al5pZun+J7jJq7jvwfolGgSx/3jfMe6o1Gy8BrTq3Gzo2Dal9nlfZBSRXjfitvxX1nJ/1nrdx9VTd4/VlvMtP+/zpjzjnnsdrZam6ZPkG8G9VCVrcZdt4jW17S+gE7HHOHTCzk/BaGhvT3HPnGuBvzrmdDcz/HBhYdZ8UB/dvvfu9gTr2c/CYNXReVnkMOBXvu6hKR6AU2GdmPYCvNbI9VeqcP845h/fdkxw3+XPgxNrbRwPbHbfcN+I+34+bfljfJ9L66vuFI5KJd/ml6kvhnxz86/TnwId49z98iPelWmWqmd2F99fY9XHTi/zLLtWcc6/7X3QLzawS+Bvw04bKxy1XiX/fhZmdFzdrEjDDT7jW4N1XBN6X1MNm9p9AGPgXf35DLvfjysT7q7y2/wYe8i+zRIDJzrkK//dg/LKDzOzdepY/KS62eN8HHjOz2/Hu37iunjI3Aa+YWVVrpeFfPm3E9cBUYJWZ7cK7FPHjuPlVl/UMr8Xx+jo1NK6x86EptwAPmtkSvO+i+Xj3r9zrT1+Kl0jdg3cpsg7n3Mdm9jgHE4tHnXOfmPcEadW2pQBznHNL/ON2Jt5leAf82Dm31U8gGjo2tTV0nj9qBx9amUjN8+daf5tCePdBVd0v1dB5C1BuZp/gXRqr73z4d+B9M/srXqve74ElfovoWqD2JfcX69v2ZmxvlS/553QS0DPu/M72P18H/tXMVuAlCx80UV9zz50eNHJvo3Ou3My+DbxgZkl4f9xU3R/W0H6vcpuZXeNv07/70xo6L6vWt5iD30FV0z71j9VKYCPwXiPb/bSZleH9W7w6bvqrZhYDSvBa7C7y6y41r6uaF/31LQBm+N87DW03QJa/DRV497vWdjjfJ9KK1O2FiEg7Y95Tuj9y3sMPIofEvKdg8xtpVZR2SJcsRURERBJMLWQiIiIiCaYWMhEREZEEU0ImIiIikmBKyEREREQS7Kju9qJbt25u4MCBiQ5DREREpEmLFi3a6ZzLrm/eUZ2QDRw4kIUL9VS4iIiItH9mtr6hebpkKSIiIpJgSshEREREEkwJmYiIiEiCHdX3kImIyPEhHA5TVFREeXl5okMRaVJqaip9+/YlFAo1exklZCIi0u4VFRXRoUMHBg4cWP1Sb5H2yDnHrl27KCoqYtCgQc1eTpcsRUSk3SsvL6dr165KxqTdMzO6du16yK25SshEROSooGRMjhaHc64qIRMRETkCv/rVr1q1/l27dnH22WeTmZnJ97///QbL7d69m/PPP58hQ4Zw/vnns2fPnhaNY/LkycyePbtF6zwcmZmZrVb3ypUrOfPMM0lJSeG3v/1tg+XWrl3L6aefTk5ODt/4xjeorKw84nUrIRMRETkCrZ2Qpaam8stf/rLRBAFgypQpnHvuuRQWFnLuuecyZcqUVo3rWNSlSxemTZvGj370o0bL/eQnP+G2225j1apVZGVl8dhjjx3xupWQiYiINMNTTz3F6NGjycvL46abbiIajXLHHXdQVlZGXl4eEydOBODSSy/ltNNOY/jw4TzyyCNHvN6MjAy+/OUvk5qa2mi5l19+mUmTJgEwadIkXnrppUbLb9myhTFjxpCXl8eIESN45513gJotULNnz2by5MnV43PnziU/P58TTzyRV199FYBly5ZV75fc3FwKCwuBhvdDZmYmt99+O8OHD+e8885jwYIFjB07lsGDB/PKK68A8PjjjzNhwgTGjh3LkCFDuOeee+rdhqlTp1JQUEBubi533313o9vbHN27d6egoKDRpyOdc7z99tt8/etfB5q3r5tDT1mKiMhRZeAdr7VKveumXNzgvBUrVjBr1izee+89QqEQN998M08//TRTpkxh+vTpLF68uLrszJkz6dKlC2VlZRQUFHDFFVfQtWvXGvXddttt/OMf/6iznquuuoo77rjjsOLftm0bvXr1AqBnz55s27at0fLPPPMMF154IT/72c+IRqMcOHCgyXWsW7eOBQsWsHr1as4++2xWrVrFjBkzuPXWW5k4cSKVlZVEo1Gg4f1QWlrKOeecw9SpU7nsssu46667mDNnDsuXL2fSpEmMHz8egAULFrB06VLS09MpKCjg4osvJj8/vzqWN998k8LCQhYsWIBzjvHjxzN//nzGjBlTI+ZvfOMbfP7553W25Yc//CHf+ta3mtzm2nbt2kXnzp1JSvJSqL59+7Jp06ZDrqc2JWSN2Fy4l3Wf7aRXTmcG5XZLdDgiIpIgb731FosWLaKgoACAsrIyunfvXm/ZadOm8eKLLwKwceNGCgsL6yRkDzzwQKvGa2ZN3lheUFDAddddRzgc5tJLLyUvL6/Jeq+88koCgQBDhgxh8ODB1fdc3XfffRQVFXH55ZczZMgQoOH9kJyczEUXXQTAyJEjSUlJIRQKMXLkSNatW1e9rvPPP796v11++eW8++67dRKyN998k1GjRgFQUlJCYWFhnYRs1qxZTW5Xe6CErBE7Nuznkzc3EKmMKiETEWknGmvJai3OOSZNmsT999/faLl58+Yxd+5c3n//fdLT0xk7dmy93R+0RgtZjx492LJlC7169WLLli0NJoxVxowZw/z583nttdeYPHlydYtRfCJXO/baSZ6ZcfXVV3P66afz2muvMW7cOB5++GECgUCD+yEUClXXEwgESElJqR6ORCKNriuec44777yTm266qdHtbOkWsq5du7J3714ikQhJSUkUFRXRp0+fQ66nNt1D1ojKjz8EYP+iTxMciYiIJNK5557L7Nmz2b59O+A90bh+/XrASzDC4TAA+/btIysri/T0dFauXMkHH3xQb30PPPAAixcvrvNzuMkYwPjx43niiScAeOKJJ5gwYQLgXfqrL/FYv349PXr04IYbbuD666/n448/BrzEbsWKFcRiseoWriovvPACsViM1atXs2bNGoYOHcqaNWsYPHgwt9xyCxMmTGDJkiXN3g+NmTNnDrt376asrIyXXnqJs846q8b8Cy+8kJkzZ1JSUgLApk2bqo9PvFmzZtW7rw8nGQMvMTz77LOrnziN39dHQi1kjQiFvGw8HHYJjkRERBJp2LBh3HvvvVxwwQXEYjFCoRAPPvggAwYM4MYbbyQ3N5dTTz2VmTNnMmPGDE4++WSGDh3KGWec0SLrHzhwIMXFxVRWVvLSSy/x5ptvMmzYMK6//nr+9V//lfz8fO644w6uvPJKHnvsMQYMGMDzzz8PwIYNG0hLS6tT57x585g6dSqhUIjMzEyefPJJwHta85JLLiE7O5v8/PzqhAegf//+jB49muLiYmbMmEFqairPP/88f/7znwmFQvTs2ZOf/vSnZGRkHPF+GD16NFdccQVFRUVcc801NS5XAlxwwQWsWLGCM888E/AeFnjqqaeabBlszNatW8nPz6e4uJhAIMDvf/97li9fTseOHRk3bhyPPvoovXv35te//jVXXXUVd911F6NGjeI73/nOYa+zijl39CYb+fn5buHCha1W/7L/eZV5i9LpHtzBvzz4jVZbj4iING7FihWcfPLJiQ7jqHT77bdz7bXXkpubm+hQmu3xxx9n4cKFTJ8+PdGhHLb6zlkzW+Scy6+vvFrIGpGc6V3XDkd1ZVdERI5OU6dOTXQI0gzKNBrxWekyAMribjIUERGR1jV58uSjunXscCgha0Qs3buHLOYa7iBORERE5EgpIWtE8oEvAIhZcoIjERERkWNZqyVkZjbTzLab2dK4aXlm9oGZLTazhWY22p9uZjbNzFaZ2RIzO7W14joUqelBAKKBlARHIiIiIsey1mwhexy4qNa03wD3OOfygF/44wBfA4b4PzcCD7ViXM2W2qkj5qK4QIhoNJbocEREROQY1WoJmXNuPrC79mSgoz/cCdjsD08AnnSeD4DOZtartWJrrrROWQQjFQBUlunGfhERqetXv/pVq6/j/vvvJycnh6FDh/LGG2/UW2b69Onk5ORgZuzcubPFY5g8eXJ1Z6iJFP/y85ZW9SqolJQUfvvb3zZYbu3atZx++unk5OTwjW98g8rKyiNed1vfQ/YDYKqZbQR+C9zpT+8DbIwrV+RPq8PMbvQvdy7csWNHa8ZKemYXglHvVQ+VxU2/dFVERI4/rZ2QLV++nOeee45ly5bx+uuvc/PNN1e/wDveWWedxdy5cxkwYECrxnMs69KlC9OmTeNHP/pRo+V+8pOfcNttt7Fq1SqysrJ47LHHjnjdbZ2QfRe4zTnXD7gNOOQtcM494pzLd87lZ2dnt3iA8dJSOmMxr4WsYm9JE6VFRORY9tRTTzF69Gjy8vK46aabiEaj3HHHHZSVlZGXl8fEiRMBuPTSSznttNMYPnw4jzzyyBGv9+WXX+aqq64iJSWFQYMGkZOTw4IFC+qUGzVqFAMHDmx2vVu2bGHMmDHk5eUxYsQI3nnnHaBmC9Ts2bOZPHly9fjcuXPJz8/nxBNP5NVXXwVg2bJl1fslNzeXwsJCoOH9kJmZye23387w4cM577zzWLBgAWPHjmXw4MG88sorgNcx7IQJExg7dixDhgzhnnvuqXcbpk6dSkFBAbm5udx9993N3vaGdO/enYKCAkKhhntXcM7x9ttv8/Wvfx2ASZMm8dJLLx3xutu6Y9hJwK3+8AvAo/7wJqBfXLm+/rSESk/NwtweACr2liY4GhERAeA/OrVSvfsanLVixQpmzZrFe++9RygU4uabb+bpp59mypQpTJ8+ncWLF1eXnTlzJl26dKGsrIyCggKuuOIKunbtWqO+Q3m5+KZNm2q8eqhv375s2nTkvyKfeeYZLrzwQn72s58RjUY5cKDpK0Hr1q1jwYIFrF69mrPPPptVq1YxY8YMbr31ViZOnEhlZWV1611D+6G0tJRzzjmHqVOnctlll3HXXXcxZ84cli9fzqRJkxg/fjzgvYNz6dKlpKenU1BQwMUXX1zj9UlvvvkmhYWFLFiwAOcc48ePZ/78+YwZM6ZGzC39cvFdu3bRuXNnkpK8FKqljkdbJ2Sbga8C84BzgEJ/+ivA983sOeB0YJ9zbksbx1ZHWlpXcMsBqCguS3A0IiKSKG+99RaLFi2ioKAAgLKysgbfmTht2rTql3Jv3LiRwsLCOgnZAw880LoBN0NBQQHXXXcd4XCYSy+9lLy8vCaXufLKKwkEAgwZMoTBgwdX33N13333UVRUxOWXX86QIUOAhvdDcnIyF13kPfM3cuRIUlJSCIVCjBw5knXr1lWv6/zzz6/eb5dffjnvvvtunYTszTffZNSoUQCUlJRQWFhYJyGbNWvWYe+jttRqCZmZPQuMBbqZWRFwN3AD8AczSwLK8Z6oBPgbMA5YBRwAvt1acR2K9PSuOPx7yPbrHjIRkXahkZas1uKcY9KkSdx///2Nlps3bx5z587l/fffJz09nbFjx1JeXl6n3KG0kPXp04eNGw/eZl1UVESfPvXeZn1IxowZw/z583nttdeYPHlydYuRmVWXqR17/Lyq8auvvprTTz+d1157jXHjxvHwww8TCAQa3A+hUKi6nkAgQEpKSvVwJO7NOPWtK55zjjvvvJObbrqp0e1s6Rayrl27snfvXiKRCElJSS12PFotIXPOfbOBWafVU9YB32utWA5XKLUzMbx7yMpK6v6DEhGR48O5557LhAkTuO222+jevTu7d+9m//79DBgwgFAoRDgcJhQKsW/fPrKyskhPT2flypV88MEH9dZ3KC1k48eP5+qrr+aHP/whmzdvprCwkNGjRzd7+QULFjB9+nSefPLJGtPXr19P3759ueGGG6ioqODjjz/mW9/6Fj169GDFihUMHTqUF198kQ4dOlQv88ILLzBp0iTWrl3LmjVrGDp0KGvWrGHw4MHccsstbNiwgSVLljBo0KBm7YfGzJkzh927d5OWlsZLL73EzJkza8y/8MIL+fnPf87EiRPJzMxk06ZNhEKhOi2XLd1CZmacffbZzJ49m6uuuoonnniCCRMmHHG96qm/MaE0nHmJWFmxbuoXETleDRs2jHvvvZcLLriA3Nxczj//fLZs8e6sufHGG8nNzWXixIlcdNFFRCIRTj75ZO64444a934druHDh3PllVcybNgwLrroIh588EGCQa/j8nHjxrF5s9eD1LRp0+jbty9FRUXk5uZy/fXXA7BhwwbS0tLq1Dtv3jxOOeUURo0axaxZs7j1Vu8W7ylTpnDJJZfwpS99iV69avZA1b9/f0aPHs3XvvY1ZsyYQWpqKs8//zwjRowgLy+PpUuX8q1vfatF9sPo0aO54ooryM3N5YorrqhxuRLgggsu4Oqrr+bMM89k5MiRfP3rX2f//v2HvJ54W7dupW/fvvzud7/j3nvvpW/fvhQXFwM19/Wvf/1rfve735GTk8OuXbv4zne+c0TrBTCvcerolJ+f7xYuXNiq63h40h1E0i7gpH7bOPdnDTX6iYhIa1qxYgUnn3xyosM4Kt1+++1ce+215ObmJjqUZnv88cdZuHDhUf2C8frOWTNb5JzLr698W9/Uf9Rx5l2yLD9QkeBIREREDt3UqVMTHYI0gxKyJrigl4iFy+t2wiciIiItb/LkyTX6Pzse6B6yJrgk73UI4Uq9y1JERERahxKypvgJWSRsTRQUEREROTxKyJqS7PWJEolpV4mIiEjrUJbRBPMTsmhMt9uJiIhI61BC1oRAitctSMw1/KJRERE5fv3qV79q9XXcf//95OTkMHToUN544416y0yfPp2cnBzMjJ07d7Z4DJMnT2b27NktXu+hin/5eUtzznHLLbeQk5NDbm4uH3/8cb3lnn32WUaOHElubi4XXXRRi+xvJWRNCGZ4945FUUImIiJ1tXZCtnz5cp577jmWLVvG66+/zs0331z9Au94Z511FnPnzmXAgAGtGs+x7O9//zuFhYUUFhbyyCOP8N3vfrdOmUgkwq233so//vEPlixZQm5ubov0l6aErAlJGd4uilpKgiMREZFEeuqppxg9ejR5eXncdNNNRKNR7rjjDsrKysjLy2PixIkAXHrppZx22mkMHz6cRx555IjX+/LLL3PVVVeRkpLCoEGDyMnJYcGCBXXKjRo1ioEDBza73i1btjBmzBjy8vIYMWIE77zzDlCzBWr27Nk1up+YO3cu+fn5nHjiibz66qsALFu2rHq/5ObmUlhYCDS8HzIzM7n99tsZPnw45513HgsWLGDs2LEMHjyYV155BfA6hp0wYQJjx45lyJAh3HPPPfVuw9SpUykoKCA3N5e777672dvekJdffrn6fZ5nnHEGe/furX4jQxXnHM45SktLcc5RXFxM7969j3jdujGqCckdkikFYgElZCIi7cHIJ0a2Sr2fTfqswXkrVqxg1qxZvPfee4RCIW6++WaefvpppkyZwvTp01m8eHF12ZkzZ9KlSxfKysooKCjgiiuuoGvXrjXqO5SXi2/atKnGq4f69u3Lpk2bDnMrD3rmmWe48MIL+dnPfkY0GuXAgQNNLrNu3ToWLFjA6tWrOfvss1m1ahUzZszg1ltvZeLEiVRWVla33jW0H0pLSznnnHOYOnUql112GXfddRdz5sxh+fLlTJo0ifHjxwPeOziXLl1Keno6BQUFXHzxxTVen/Tmm29SWFjIggULcM4xfvx45s+fz5gxY2rEfCgvF9+0aRP9+vWrHq/a1/GvkAqFQjz00EOMHDmSjIwMhgwZwoMPPtiMPd44JWRNSO2YicUiuEAS0XCMYEiNiiIix5u33nqLRYsWUVBQAEBZWVmdl1hXmTZtGi+++CIAGzdupLCwsE5CdigvF28tBQUFXHfddYTDYS699FLy8vKaXObKK68kEAgwZMgQBg8ezMqVKznzzDO57777KCoq4vLLL2fIkCFAw/shOTmZiy66CICRI0eSkpJCKBRi5MiRrFu3rnpd559/fvV+u/zyy3n33XfrJGRvvvkmo0aNAqCkpITCwsI6CVlLv1w8HA7z0EMP8cknnzB48GD+7d/+jfvvv5+77rrriOpVQtaE9I5dCEbLiQQyqSgPkx5SS5mISCI11pLVWpxzTJo0ifvvv7/RcvPmzWPu3Lm8//77pKenM3bsWMrLy+uUO5QWsj59+rBx48bq8aKiIvr06XOYW3LQmDFjmD9/Pq+99hqTJ0+ubjEyO9jvZu3Y4+dVjV999dWcfvrpvPbaa4wbN46HH36YQCDQ4H4IhULV9QQCAVJSUqqHI5FIo+uK55zjzjvv5Kabbmp0Ow+lhaw5+7qqNfSEE04AvCR1ypQpjcbQHGruaUJmRleCUe/1SZXFZQmORkREEuHcc89l9uzZbN++HYDdu3ezfv16wEswwuEwAPv27SMrK4v09HRWrlzJBx98UG99DzzwAIsXL67zUzsZAxg/fjzPPfccFRUVrF27lsLCQkaPHt3s2BcsWFAn8QBYv349PXr04IYbbuD666+vfqKwR48erFixglgsVt3CVeWFF14gFouxevVq1qxZw9ChQ1mzZg2DBw/mlltuYcKECSxZsqTZ+6Exc+bMYffu3ZSVlfHSSy9x1lln1Zh/4YUXMnPmTEpKSgDvcmPV8Yk3a9asevd1fftk/PjxPPnkkzjn+OCDD+jUqVONy5XgJW3Lly9nx44d1XG2xIvv1ULWhMy0LgRiXlZfsbcE+nRObEAiItLmhg0bxr333ssFF1xALBYjFArx4IMPMmDAAG688UZyc3M59dRTmTlzJjNmzODkk09m6NChNe79OlzDhw/nyiuvZNiwYSQlJfHggw8SDAYBGDduHI8++ii9e/dm2rRp/OY3v2Hr1q3k5uZWz9uwYQNpaWl16p03bx5Tp04lFAqRmZnJk08+CcCUKVO45JJLyM7OJj8/vzrhAejfvz+jR4+muLiYGTNmkJqayvPPP8+f//xnQqEQPXv25Kc//SkZGRlHvB9Gjx7NFVdcQVFREddcc02Ny5UAF1xwAStWrODMM88EvIcFnnrqqQYvJTfHuHHj+Nvf/kZOTg7p6en86U9/qp6Xl5fH4sWL6d27N3fffTdjxowhFAoxYMAAHn/88cNeZxVzzh1xJYmSn5/vFi5c2KrrKPziVd755VbKMgbz/67tQ/+zhrbq+kREpK4VK1a0SCvE8ej222/n2muvJTc3N9GhNNvjjz/OwoULW6Q7iUSp75w1s0XOufz6yquFrAmZ6d3Bec3SlcVNP4EiIiLSnkydOjXRIUgzKCFrQmZmDxz+JcuSigRHIyIicuybPHlyjf7Pjge6qb8J6endieElYhX7dVO/iIiItDwlZE0IJqfhzGshKysuTnA0IiIicixSQtYMzrwWsrKS0gRHIiIiIsciJWTN4AJeQlZeqnvIREREpOUpIWuOoN8xbEU4wYGIiEh786tf/arV13H//feTk5PD0KFDeeONN+otM336dHJycjAzdu7c2eIxTJ48mdmzZ7d4vYcq/uXnLc05xy233EJOTg65ubnVneXWNnbsWIYOHUpeXh55eXn1dkh7qJSQNUeoEoBwZYLjEBGRdqe1E7Lly5fz3HPPsWzZMl5//XVuvvnm6hd4xzvrrLOYO3cuAwYMaNV4jmV///vfKSwspLCwkEceeYTvfve7DZZ9+umnq3v9P5LOaKsoIWsGS/JaxsIR7S4RkePVU089xejRo8nLy+Omm24iGo1yxx13UFZWRl5eHhMnTgTg0ksv5bTTTmP48OE88sgjR7zel19+mauuuoqUlBQGDRpETk4OCxYsqFNu1KhRDBw4sNn1btmyhTFjxpCXl8eIESN45513gJotULNnz67R/cTcuXPJz8/nxBNP5NVXXwVg2bJl1fslNzeXwsJCoOH9kJmZye23387w4cM577zzWLBgAWPHjmXw4MG88sorgNcx7IQJExg7dixDhgzhnnvuqXcbpk6dSkFBAbm5udx9993N3vaGvPzyy9Xv8zzjjDPYu3cvW7ZsOeJ6m0P9kDVDIDUCJRCNaneJiCTaipNap8f+k1euaHidK1Ywa9Ys3nvvPUKhEDfffDNPP/00U6ZMYfr06dUvnAaYOXMmXbp0oaysjIKCAq644gq6du1ao75Debn4pk2barx6qG/fvmzatOkwt/KgZ555hgsvvJCf/exnRKNRDhxouvPzdevWsWDBAlavXs3ZZ5/NqlWrmDFjBrfeeisTJ06ksrKyuvWuof1QWlrKOeecw9SpU7nsssu46667mDNnDsuXL2fSpEmMHz8e8N7BuXTpUtLT0ykoKODiiy+u8fqkN998k8LCQhYsWIBzjvHjxzN//nzGjBlTI+ZDebn4pk2b6NevX/V41b6u/T5LgG9/+9sEg0GuuOIK7rrrrjovPz9UyjCaIZgagxKIuOREhyIiIgnw1ltvsWjRIgoKCgAoKytr8DLVtGnTql/KvXHjRgoLC+skZA888EDrBtwMBQUFXHfddYTDYS699FLy8vKaXObKK68kEAgwZMgQBg8ezMqVKznzzDO57777KCoq4vLLL2fIkCFAw/shOTmZiy66CICRI0eSkpJCKBRi5MiRrFu3rnpd559/fvV+u/zyy3n33XfrJGRvvvkmo0aNAqCkpITCwsI6CdmsWbMOex815Omnn6ZPnz7s37+fK664gj//+c/1vqz8UCgha4ZQRoCynRAlJdGhiIgc9xpryWotzjkmTZrE/fff32i5efPmMXfuXN5//33S09MZO3Ys5eXldcodSgtZnz592LhxY/V4UVERffr0OcwtOWjMmDHMnz+f1157jcmTJ1e3GMW39NSOvXYrkJlx9dVXc/rpp/Paa68xbtw4Hn74YQKBQIP7IRQKVdcTCARISUmpHo5EIo2uK55zjjvvvJObbrqp0e08lBay5u7rqmkdOnTg6quvZsGCBUeckOmmqGZI7uDlrTFLTXAkIiKSCOeeey6zZ8+ufppu9+7drF/vvec4FAoRDnv3Gu/bt4+srCzS09NZuXIlH3zwQb31PfDAA9U3hMf/1E7GAMaPH89zzz1HRUUFa9eupbCwkNGjRzc79oaShfXr19OjRw9uuOEGrr/++uonCnv06MGKFSuIxWLVLVxVXnjhBWKxGKtXr2bNmjUMHTqUNWvWMHjwYG655RYmTJjAkiVLmr0fGjNnzhx2795NWVkZL730EmeddVaN+RdeeCEzZ86kpKQE8C431ve046xZs+rd1/Xtk/Hjx/Pkk0/inOODDz6gU6dOdS5XRiKR6qdYw+Ewr776KiNGjDjk7atNLWTNkNY5HYBoIAUXc1jgyK4Ti4jI0WXYsGHce++9XHDBBcRiMUKhEA8++CADBgzgxhtvJDc3l1NPPZWZM2cyY8YMTj75ZIYOHVrj3q/DNXz4cK688kqGDRtGUlISDz74IMFgEIBx48bx6KOP0rt3b6ZNm8ZvfvMbtm7dSm5ubvW8DRs2kJaWVqfeefPmMXXqVEKhEJmZmTz55JMATJkyhUsuuYTs7Gzy8/OrEx6A/v37M3r0aIqLi5kxYwapqak8//zz/PnPfyYUCtGzZ09++tOfkpGRccT7YfTo0VxxxRUUFRVxzTXX1LhcCXDBBRewYsUKzjzzTMB7WOCpp546oicex40bx9/+9jdycnJIT0/nT3/6U/W8vLw8Fi9eTEVFBRdeeCHhcJhoNMp5553HDTfccNjrrGLOuSOuJFHy8/PdwoULW309c167k9UvfploUhrXPzCGlDTlsSIibWnFihWcfHLr3Mx/rLv99tu59tpryc3NTXQozfb444+zcOFCpk+fnuhQDlt956yZLXLO5ddXvtUuWZrZTDPbbmZLa03/NzNbaWbLzOw3cdPvNLNVZva5mV3YWnEdjvRO3UmKeNe+Kw6oc1gRETl6TJ069ahKxo5XrdnU8zgwHXiyaoKZnQ1MAE5xzlWYWXd/+jDgKmA40BuYa2YnOufq9nyXAJkdswlGDwBZVBSXQ9e6Tb8iIiLSMiZPnlyj/7PjQau1kDnn5gO7a03+LjDFOVfhl6m6+24C8JxzrsI5txZYBTT/jsVWlpnRnUDUayEr31PSRGkRERGRQ9PWT1meCHzFzD40s3+aWYE/vQ+wMa5ckT+tXchIzwZXBighExERkZbX1nenJwFdgDOAAuB5Mxt8KBWY2Y3AjeA97dEWOmT2BLyErKK46Z6MRURERA5FW7eQFQF/cZ4FQAzoBmwC+sWV6+tPq8M594hzLt85l5+dnd3qAQOkpXcj5idkZXuVkImIiEjLauuE7CXgbAAzOxFIBnYCrwBXmVmKmQ0ChgB135yaIIFgiJh5CVnp3n0JjkZERNqTX/3qV62+jvvvv5+cnByGDh3KG2+8UW+Z6dOnk5OTg5lVd1zakiZPnszs2bNbvN5DFf/y85bmnOOWW24hJyeH3Nzc6s5yGzJ+/PgW6RQWWrfbi2eB94GhZlZkZt8BZgKD/a4wngMm+a1ly4DngeXA68D32ssTllWcn5CVlaiFTEREDmrthGz58uU899xzLFu2jNdff52bb765+gXe8c466yzmzp3LgAEDWjWeY9nf//53CgsLKSws5JFHHuG73/1ug2X/8pe/tGhy2JpPWX7TOdfLORdyzvV1zj3mnKt0zl3jnBvhnDvVOfd2XPn7nHMnOOeGOuf+3lpxHS4X8PshK61IcCQiIpIITz31FKNHjyYvL4+bbrqJaDTKHXfcQVlZGXl5eUycOBGASy+9lNNOO43hw4fzyCOPHPF6X375Za666ipSUlIYNGgQOTk5LFhQ9yLSqFGjGDhwYLPr3bJlC2PGjCEvL48RI0bwzjvvADVboGbPnl2j+4m5c+eSn5/PiSeeyKuvvgrAsmXLqvdLbm4uhYWFQMP7ITMzk9tvv53hw4dz3nnnsWDBAsaOHcvgwYN55ZVXAK9j2AkTJjB27FiGDBnCPffcU+82TJ06lYKCAnJzc7n77rubve0Nefnll6vf53nGGWewd+9etmzZUqdcSUkJv/vd77jrrruOeJ1V1OV8M7kkLxGrqIglOBIRkePbg//6dtOFDsP3ZpzT4LwVK1Ywa9Ys3nvvPUKhEDfffDNPP/00U6ZMYfr06SxevLi67MyZM+nSpQtlZWUUFBRwxRVX0LVr1xr1HcrLxTdt2lTj1UN9+/Zl06Z6b7M+JM888wwXXnghP/vZz4hGoxw40PQVoHXr1rFgwQJWr17N2WefzapVq5gxYwa33norEydOpLKysrr1rqH9UFpayjnnnMPUqVO57LLLuOuuu5gzZw7Lly9n0qRJjB8/HvDewbl06VLS09MpKCjg4osvrvH6pDfffJPCwkIWLFiAc47x48czf/58xowZUyPmQ3m5+KZNm+jX7+At7VX7uvb7LH/+85/z7//+76Snpze5z5pLCVlzhbyELFyp91iKiBxv3nrrLRYtWkRBgddbU1lZWYPvTJw2bVr1S7k3btxIYWFhnYTsgQceaN2Am6GgoIDrrruOcDjMpZdeSl5eXpPLXHnllQQCAYYMGcLgwYNZuXIlZ555Jvfddx9FRUVcfvnlDBkyBGh4PyQnJ3PRRRcBMHLkSFJSUgiFQowcOZJ169ZVr+v888+v3m+XX3457777bp2E7M0332TUqFGA12pVWFhYJyGbNWvWYe+j+ixevJjVq1fzwAMP1Ij3SCkhayZLDkMEItFgokMRETmuNdaS1Vqcc0yaNIn777+/0XLz5s1j7ty5vP/++6SnpzN27FjKy8vrlDuUFrI+ffqwcePBrjqLioro0+fIu+ocM2YM8+fP57XXXmPy5MnVLUZmBxseasceP69q/Oqrr+b000/ntddeY9y4cTz88MMEAoEG90MoFKquJxAIkJKSUj0ciUQaXVc85xx33nknN910U6PbeSgtZM3Z1++//z4LFy5k4MCBRCIRtm/fztixY5k3b16jcTSlrZ+yPGoFU71LlZFYKMGRiIhIWzv33HOZPXs227d7L5jZvXs369evB7wEIxz23nO8b98+srKySE9PZ+XKlXzwwQf11vfAAw+wePHiOj+1kzHwnuR77rnnqKioYO3atRQWFjJ6dPNfZrNgwYI6iQfA+vXr6dGjBzfccAPXX3999ROFPXr0YMWKFcRiseoWriovvPACsViM1atXs2bNGoYOHcqaNWsYPHgwt9xyCxMmTGDJkiXN3g+NmTNnDrt376asrIyXXnqJs846q8b8Cy+8kJkzZ1JS4nXYvmnTpurjE2/WrFn17uv69sn48eN58skncc7xwQcf0KlTpzqXK7/73e+yefNm1q1bx7vvvsuJJ554xMkYqIWs2QIdHJRA1CUnOhQREWljw4YN49577+WCCy4gFosRCoV48MEHGTBgADfeeCO5ubmceuqpzJw5kxkzZnDyySczdOjQGvd+Ha7hw4dz5ZVXMmzYMJKSknjwwQcJBr2rNePGjePRRx+ld+/eTJs2jd/85jds3bqV3Nzc6nkbNmwgLa3uO5jnzZvH1KlTCYVCZGZm8uST3qunp0yZwiWXXEJ2djb5+fnVCQ94HbKPHj2a4uJiZsyYQWpqKs8//zx//vOfCYVC9OzZk5/+9KdkZGQc8X4YPXo0V1xxBUVFRVxzzTU1LlcCXHDBBaxYsYIzzzwT8B4WeOqppxq8lNwc48aN429/+xs5OTmkp6fzpz/9qXpeXl5ejXsFW5o551qt8taWn5/vFi5c2CbreuqBr7Pv85uxWCU3P3JRm6xTREQ8K1as4OSTT050GEel22+/nWuvvZbc3NxEh9Jsjz/+OAsXLmT69OmJDuWw1XfOmtki51x+feXVQtZMoax0zEVxgWSi0RjBoK72iohI+zd16tREhyDNoKyimVK7dCEY8TqHrSyLNFFaREREDtfkyZOP6taxw6GErJk6de1NUsTvHHa/OocVERGRlqOErJk6d+hFIOZ1mle2Y29igxEROQ4dzfc8y/HlcM5VJWTN1LlDb5zzWsjKd+oF4yIibSk1NZVdu3YpKZN2zznHrl27SE1NPaTldFN/M2V17IdjCQBlu0uaKC0iIi2pb9++FBUVsWPHjkSHItKk1NRU+vbte0jLKCFrpg4d+xE176b+A7uLExyNiMjxJRQKMWjQoESHIdJqdMmymQKhNGIBLyEr2bMnwdGIiIjIsUQJ2SGIBb17yMr26pKliIiItBwlZIciyb+p/4C6vRAREZGWo4TsUCRXAlBZHktwICIiInIsUUJ2CIJpYQAiYe02ERERaTnKLA5BUrr3GYmFEhuIiIiIHFOUkB2C1E5BAGIuJcGRiIiIyLFECdkhSM/ymsiigbQERyIiIiLHEiVkh6BT9ywAIsE0vb5DREREWowSskOQ1bUbANGkdCL71Fu/iIiItAwlZIegS8deBKJeb/3l23YnOBoRERE5VighOwSdO/TBYgcAKNuu1yeJiIhIy1BCdgg6dOgDzn+f5fa9iQ1GREREjhlKyA5BIL0rDq+FbN+OnQmORkRERI4VSsgORXI6mJeQlezSJUsRERFpGUrIDpELeC8YP7Bvf4IjERERkWOFErJDZEnePWQVJRUJjkRERESOFUrIDlEguRKAcLk6hhUREZGWoYTsEAVTIgBEw9p1IiIi0jKUVRyi5AyvZSwaTU5wJCIiInKsaLWEzMxmmtl2M1taz7x/NzNnZt38cTOzaWa2ysyWmNmprRXXkUrr6O2yKHrBuIiIiLSM1mwhexy4qPZEM+sHXABsiJv8NWCI/3Mj8FArxnVEOnTzErFoIEMvGBcREZEW0WoJmXNuPlDfCx8fAH4MxGczE4AnnecDoLOZ9Wqt2I5EVtdOAISTMnFlZQmORkRERI4FbXoPmZlNADY55z6tNasPsDFuvMif1u5kd+kCQDiUSXTv3sQGIyIiIseENkvIzCwd+CnwiyOs50YzW2hmC3fs2NEywR2Crl36YLEwsWAyFTvUW7+IiIgcubZsITsBGAR8ambrgL7Ax2bWE9gE9Isr29efVodz7hHnXL5zLj87O7uVQ66rY6d+WKwEgH1bdrX5+kVEROTY02YJmXPuM+dcd+fcQOfcQLzLkqc657YCrwDf8p+2PAPY55zb0laxHQrL6AbOS8j2bN2e4GhERETkWNCa3V48C7wPDDWzIjP7TiPF/wasAVYB/wPc3FpxHbG0LoDfQqZLliIiItICklqrYufcN5uYPzBu2AHfa61YWlQwCQJeQla6Wy8YFxERkSOnnvoPgwVLASjfX57gSERERORYoITsMARCXv9jlcrHREREpAUoITsMSWlhACKVwQRHIiIiIscCJWSHITkjBkAklprgSERERORYoITsMKR3DgF6wbiIiIi0DCVkh6FD13QAooHMBEciIiIixwIlZIchq2dXACJJGbjKygRHIyIiIkc7JWSHITu7NwDhUAbhPXsTG4yIiIgc9ZSQHYYu3U4gECkFC3Bgq95nKSIiIkdGCdlhyMwaXP2C8WK9YFxERESOkBKyw2BpnTHn9da/e/OmBEcjIiIiRzslZIfDDMxvIdu6PcHBiIiIyNFOCdnhChwA4MCekgQHIiIiIkc7JWSHyZK891mWl0YTHImIiIgc7ZSQNWLrvnLeX72LNTvqtoIFUr33WVZWaBeKiIjIkVE20Yi/frqZb/7PBzz94YY685IyHACRcHJbhyUiIiLHGCVkjUgNebunPFz3smRylpeIRVx6m8YkIiIixx4lZI1ISQoCUBGJ1ZmX3rMjAJFAxzaNSURERI49SsgakdJIC1nnwX0BCCd1atOYRERE5NijhKwRjbWQ9Rh8MhaLEAllULmnuK1DExERkWOIErJGNHYPWY/O/UkK7wOg+ItVbRqXiIiIHFuUkDWisRayzimdCUS9hGzPmrVtGpeIiIgcW5SQNaLqHrL6ErKABQAvIdtRtLUtwxIREZFjjBKyRqRWtZDVc8kSgMB+APbtKm2rkEREROQYpISsEY21kAEEQl4iVlri2iwmEREROfYoIWtEashrIavvpn6AYFqlN79CvfWLiIjI4VNC1oiUpMZbyJI7ewlbOJrRZjGJiIjIsUcJWSOaaiHL6NEBgAjqrV9EREQOnxKyRjTVQtZp6AkARIIdcU73kYmIiMjhUULWiFAwQDBgRGOOcLRuUtZ9+CgC0QpiwVQqd+1MQIQiIiJyLFBC1oTGWsl6d+lPUngvAHs/W9qWYYmIiMgxRAlZE6ruI6uvL7KOyR3Bee+x3P55YZvGJSIiIscOJWRNqGohK6+nhczMcOYlZDuLdrRpXCIiInLsaLWEzMxmmtl2M1saN22qma00syVm9qKZdY6bd6eZrTKzz83swtaK61A11kIG4FK8zmH3765os5hERETk2NKaLWSPAxfVmjYHGOGcywW+AO4EMLNhwFXAcH+Z/zazYCvG1mzVLWThBnrrz4x48w+E2iwmEREROba0WkLmnJsP7K417U3nXMQf/QDo6w9PAJ5zzlU459YCq4DRrRXboTh4U3/9LWShrl4iFo6oc1gRERE5PIm8h+w64O/+cB9gY9y8In9aHWZ2o5ktNLOFO3a0/n1bKdWdw9bfQpYxIBtQ57AiIiJy+BpNyMzsnLjhQbXmXX64KzWznwER4OlDXdY594hzLt85l5+dnX24ITRbUy1k3YafBEA4lIUrK2n1eEREROTY01QL2W/jhv+31ry7DmeFZjYZuASY6A52b78J6BdXrK8/LeGqb+pvoLf+fv0GgItRkdyJipUft2VoIiIicoxoKiGzBobrG2+SmV0E/BgY75w7EDfrFeAqM0vxW+KGAAsOtf7WcPCm/vpbyHp37EUgWgwWYO/S5W0ZmoiIiBwjkpqY7xoYrm+8BjN7FhgLdDOzIuBuvKcqU4A5ZgbwgXPuX51zy8zseWA53qXM7znn6s+A2lhTLWTpoXSc2wt0ZtvqjfRsu9AatGN/BZ9u3MvOkgq+cmI2fTqnAbCvLMzOkgo6pYXISk/GOUfRnjLKwlE6pCYxd/k2koIBOqaFSA4aZkYoaHTvkEpqKEjRHi+HTgsFSQkFqQhHKamIUFIRIRqreTqYgXMQcxCLOTAImBHwP6163Ig6R2UkRiQaa/SkqvMXgdWeX2dCE8tbE/MbiKOh6Q38jdJQ+aONXtcqIseyzJQkzhvWI2HrbyohG2xmr+D9rqoaxh8f1PBi4Jz7Zj2TH2uk/H3AfU3E0+aq7yFroIUMIJZUjAG7t5W2Sgzrdpby+rKtxJxjSPcOVEZibCsu54tt+9lfHqG0MsLpg7pSWhFhzvJtfL5tf43lB3fLYFdpJfvKwtXTzCApYISj+i0rIiKS0z2zXSdkE+KGf1trXu3xY1JTLWQALq0MK4eSkpbtOi0SjfHA3C94+J9riMQaT5zmfX7widOM5CC5fTvTMS2J+V/sZM1OL1FMCwXJ7pBCcXmYvQfChKOO3p1SSU4KsGH3Ac45qTvZHVIpLgsTicWIxqAyGmPrvjIqIzF6d04jKRigvDJKWThKaihAZkoSGSlJhIIHr34753DUbAkDiDnnt5o5r+XMOZxzGEZKKEBSwDCs3hal2q0zrlZbWt35h7Z8E6M1tq3e6Q2Wb7js0dhwdqy09omI1NazY2pC199oQuac+2f8uJmFgBHAJufc9tYMrL1o6h4ygGCW4bZARWXL9UX2f6t2ct/fVrBsczFmcHFuLyojMbYXl9MlIxmALw/JplNaCAOWbt5HwIyvnpjNGYO7kuzHfaAywpodpfTomEq3zOTqy3TRmCMcjVUnnLGYIxDQb1sREZFEaDQhM7MZwB/9e7w6Ae8DUaCLmf3IOfdsWwSZSM1pIUvpl0X5FgjTGaIRCDbV8Ni4nSUVXPfER5SHY/TomMIfv3kqowd1aXSZK07rW+/09OQkRvTpVGd6MGAEAwdb9JSMiYiIJE5TT1l+xTm3zB/+NvCFc24kcBre05LHvOa0kHUa3BuAcDCL2LbCw17Xyq3FzFm+jR+98Cnl4RhjTsxm3o/ObjIZExERkaNbU005lXHD5wMvADjnttZ+Qu1YlZbstSKVVEQaLNOrVw+2UUJFahaRlR+S3PvkZtfvnGPLvnIemPMFLywqqjHvB+cNqV6/iIiIHLuaSsj2mtkleJ20ngV8B8DMkoC0Vo6tXRjQ1bsvbPWOhp+gHNJ3EIvdYiqTO1K24h2Sz2mwaA3bi8uZ/KePWL6lGIBQ0Mjr15n05CS+MqQbp/bPOuL4RUREpP1rKiG7CZgG9AR+4Jzb6k8/F3itNQNrL4b26ADAF9v2e08E1tMymJ3eDWLFEMxiy7rN1L1j66BV2/dTVhkjGDBuee4TVm0vIWBwzkk9+Om4kxicndlKWyIiIiLtVVNPWX4BXFTP9DeAN1orqPakR8cUOqQmsfdAmB0lFXTvUPexWDODYDGQxfadYU5qoK5PN+7liof+r0YXFkN7dODZG8+ofnJSREREjj9NPWU5rbH5zrlbWjac9sfMGNqjAwvX7+GLrSX1JmQAgfQIsQOwvyTV63yqVktaZSTGD59fXJ2MdUoL8aUTujLlilw6pYVafTtERESk/WrqkuW/AkuB54HNHJ19WR6xE3t6Cdnn2/bz5SHd6i2T0jWZsgNQEc2C/VugY+8a8/++dAurd5QyqFsGf7/1K9XdaYiIiIg0lZD1Av4F+AbeOyZnAbOd9/LG48aJ3b37ugprvZIoXsfeXSjbCFHXBXZ8Xiche/z/1gFw/VcGKRkTERGRGhrth8w5t8s5N8M5dzZeP2SdgeVmdm1bBNdenNjTu7G/9jsi4/U6oT8AsWAWsc0rasxbUrSXTzbspWNqEpeN6tN6gYqIiMhRqamOYQEws1OBW4FrgL8Di1ozqPam6knLwm0lDb7LcGBfr6f88pTOlBZ+XGNeVevYlfn9SE8+sl78RURE5NjT1E39/wlcDKwAngPudM413EPqMaprZgrdMpPZWVLJ5n3l9Olctwu2zl29y5oVKVmsX7Wck6Ix/vzBetJCQV79dAtm8K0zB7Zx5CIiInI0aKq55i5gLXCK//Mrvx8uA5xzLrd1w2s/hnTvwM6SXXyxdX+9CVl6h2RwUcLJHdixdRf/9l9vs3Z3RfX88af0pn/X9LYMWURERI4STSVkg9okiqPA0J4deH/NLj7ftp+zT+peZ74FDGelGB0pLetC0p7VgHcZs3+XdH556Yg2jlhERESOFk11DLu+vulmFgC+CdQ7/1g01L+x/5MNexosU2HlpLqOVFZ24z/zy+lwxpdJSw6S3SGFjqnqa0xERETq1+hN/WbW0czuNLPpZnaBef4NWANc2TYhtg9nD+2OGbyxbBvT3irko3W7a8zfsb+C3cQAcOEunJm8lhF9OnFCdqaSMREREWlUU09Z/hkYCnwGXA/8A/g6cKlzbkIrx9au9OyUSv4A72Xfv5vzBf8y433u//uK6qcun/5wPTuDKQAEolm4zR83WJeIiIhIvKbuIRvsnBsJYGaPAluA/s658laPrB2akNeHj9YdvGT58D/X8PA/15CVHuJAZZRhSele97nBLHZsXUX3WBQC6gRWREREGtdUQhauGnDORc2s6HhNxgCuKuhHclKAr56YzQsLN/LbN78AYM8Bbzf165cFheVUpGSxoczRfc866HpCAiMWERGRo0FTCdkpZlbsDxuQ5o9XdXvRsVWja2eSggGuzO8HwM1jcxjSowMnZGfQITVE0Z4DZIeNv/72E8pTsthRlgzbVyghExERkSY19ZSlrrc1IBAwLhzes3q8R8dUyvZXAl7nsMUHkryE7ORLEhWiiIiIHCWa9eokaZ7UzBBmUSKhdCpL02H78kSHJCIiIkcBJWQtyMxIS/W6vrDyrrBtaYIjEhERkaOBErIW1jErFYDU8iwO7FoFFfsTHJGIiIi0d0rIWljHnp0AyKzIYm1SEmxZkuCIREREpL1TQtbCOvTwXrEUS8piXTgZtixObEAiIiLS7jXV7YUcokz/kmV5amf2lybD5sWJDUhERETaPbWQtbDMLO/1SRUpWZTtD6mFTERERJqkhKyFdeyaBkBZajcC+4Kws1A39ouIiEijlJC1sE7ZaZg5ytO60nFPEvsC6MZ+ERERaZQSshYWDAXI7JSMsyDdi7vyRUg39ouIiEjjWi0hM7OZZrbdzJbGTetiZnPMrND/zPKnm5lNM7NVZrbEzE5trbjaQlZv70nLpEAPVkdTdGO/iIiINKo1W8geBy6qNe0O4C3n3BDgLX8c4GvAEP/nRuChVoyr1XXumQ7AgbTu7NqnFjIRERFpXKslZM65+cDuWpMnAE/4w08Al8ZNf9J5PgA6m1mv1oqttWX18BOy9B5E94R0Y7+IiIg0qq3vIevhnNviD28FevjDfYCNceWK/Gl1mNmNZrbQzBbu2LGj9SI9Ap38hKw0vScddgYI43Rjv4iIiDQoYTf1O+cc4A5juUecc/nOufzs7OxWiOzIdeubCUBJZl/6bTPWJKs/MhEREWlYWydk26ouRfqf2/3pm4B+ceX6+tOOSmmZyXTKTiUWTKZjuDefox77RUREpGFtnZC9AkzyhycBL8dN/5b/tOUZwL64S5tHpR6DvJeMF3ccyLb9KbD5kwRHJCIiIu1Va3Z78SzwPjDUzIrM7DvAFOB8MysEzvPHAf4GrAFWAf8D3NxacbWVqoRsX8dBlO1Nhl2rdGO/iIiI1KvVXi7unPtmA7POraesA77XWrEkQo9BHQHY32EAod1Bwj0coS1LYOBZCY5MRERE2hv11N9KuvbOwAwOpHen7/Ygy1PUH5mIiIjUTwlZK0lKDtKxWyrOgnQp7cmikHrsFxERkfopIWtF3fp5r1CqSOvN6rJU2LQwwRGJiIhIe6SErBV17eP1R1aa0Zvyvcm43Wtgz7rEBiUiIiLtjhKyVtS1d1UHsX3osQOKkoKw6q0ERyUiIiLtjRKyVtStn5eQ7c/sx4BtsCwlBVa/neCoREREpL1RQtaKOnRNJTU9SDi5A733dmF5KBnWzodYNNGhiYiISDuihKwVmRk9T+gMQCRlEOtjHaCiGLYtTWxgIiIi0q4oIWtlPQZXvUJpEOW7jQjA+v9LaEwiIiLSvigha2U9/R7793UaRO+tMT5PTob17yU4KhEREWlPlJC1sh6DOhEw593YvyOVRakpsO5d3UcmIiIi1ZSQtbJQSpDufVLBAnQtO5GPO2RB2R7Y9HGiQxMREZF2QglZGxgwqhcAkdSTWR0O4QAK30xoTCIiItJ+KCFrA/2GdwNgT+cT6bapgrWhJCVkIiIiUk0JWRvo1i+TgMUoS+/OkM0pfJyeCVsWw/6tiQ5NRERE2gElZG0gGAyQ1dkA6L+7Lx937evNWDU3gVGJiIhIe6GErI1kn9AFgM7lffkk4LyJumwpIiIiKCFrMz1O7A5ARVo/2FrMtmAQVv8DouEERyYiIiKJpoSsjWT36wDA/sy+DNns+Dh7oPcapQ0fJDYwERERSTglZG2ka58MDEdpRi+GbE5iUdc+3gxdthQRETnuKSFrI0nJQTplBcEC9N3Tm0VUeDO+eAOcS2xwIiIiklBKyNpQ95yuAGSE+7Jp92b2pHeBnZ+r134REZHjnBKyNtR9YGcASjP7MXST46OhZ3szFs5MXFAiIiKScErI2lB2/0wA9nfox8kbHQs6eS1mLP1f7/2WIiIiclxSQtaGuvXrQCAA+zP7MWRzJh/uWQmDx0KkDD59LtHhiYiISIIoIWtDyalJ9D2xI1iAjpFT2LR7Ldtyv+7NXPgn3dwvIiJynFJC1sZyRnvdXezqmscJW2BBRgdI7+rd3L9jZYKjExERkURQQtbGBp3SDXDs7TyEkzeG+Gj7x3Di17yZK19NaGwiIiKSGErI2lhqRoiunRwukMTgHYP4YMsHuKHjvJkrlJCJiIgcj5SQJUDfEdkAdAqfyI59m1me1ROSM2HLYti+IrHBiYiISJtTQpYA/U/rC8D+DkMZvsHx943/gNxveDM/ejSBkYmIiEgiKCFLgF4ndCZgMfZ36Ed+YQZ/X/d3ogXf8WZ++hyUFyc2QBEREWlTCUnIzOw2M1tmZkvN7FkzSzWzQWb2oZmtMrNZZpaciNjaQiglSK9+qWABcnaPZHvpNuZXbIcBX4bKEvVJJiIicpxp84TMzPoAtwD5zrkRQBC4Cvg18IBzLgfYA3ynrWNrSyd8aSAABzKG028HPPf5czD6em/mR4+qTzIREZHjSKIuWSYBaWaWBKQDW4BzgNn+/CeASxMTWtsYmNsNgN1dTiZ/TTLvb36fHf1Phw69vD7J1s5PcIQiIiLSVto8IXPObQJ+C2zAS8T2AYuAvc65iF+sCOjT1rG1pQ5dUsnqGCMaTGHUrpE4HG9vegdO+7ZXYMEjiQ1QRERE2kwiLllmAROAQUBvIAO46BCWv9HMFprZwh07drRSlG1j0Gm9vIHywaRUOuZumAunTYJAEnz+N9i9JrEBioiISJtIxCXL84C1zrkdzrkw8BfgLKCzfwkToC+wqb6FnXOPOOfynXP52dnZbRNxKxk8uh8Au7oMZ+SGAB9t/YhdSSGvCwwXg3d/n9gARUREpE0kIiHbAJxhZulmZsC5wHLgH4D/pm0mAS8nILY21X1AB1KDYSpSu3Du9pFEXZTX170OX/4hYLD4GdhXlOgwRUREpJUl4h6yD/Fu3v8Y+MyP4RHgJ8APzWwV0BV4rK1ja2sWMPrlpAPQedcgAF5d/Sp0y4ERl0MsDO9NS2SIIiIi0gYS8pSlc+5u59xJzrkRzrlrnXMVzrk1zrnRzrkc59y/OOcqEhFbW8v56hAA9iQPJmdvKkt3LWXtvrXwlX/3Cnz8BJRsT2CEIiIi0trUU3+C9RueTYAoxR0H8i+bTgLgtTWvQY/hMHQcRMr1xKWIiMgxTglZgoVSgvTqGQQge11PAF5d8yrOOTjrVq/Qgv/R65RERESOYUrI2oHBXxkMwO5Yb04u78Kmkk18uuNT6H8G9P8SlO+F96cnNkgRERFpNUrI2oFBo7yWsd1ZJ3HlphzAayUD4Nyfe5/vPwgHdiciPBEREWllSsjagQ5dUune1RELppC1ugcAb6x7g3A0DAO+BCec6710XPeSiYiIHJOUkLUTIy70nrbc7E7gNDeAvRV7eXvj297Mr/zQ+/xwBlSWJihCERERaS1KyNqJnDP6kESY4o6DuLJoGAAvfPGCN3PAWdC3AMr2wMdPJjBKERERaQ1KyNqJUHKQ3r29py1TV6aRGkzlwy0fsnzXcjCDL9/mFfy/P0KkMoGRioiISEtTQtaOnOB3ErutohuTu10MwLSP/Z76T/waZJ8MxZtg4cxEhSgiIiKtQAlZOzJwVC/AsSfrRC5c3YvMUCbvbX6PBVsWQCAA5/7CK/jPKVC2N5GhioiISAtSQtaOpHdMpm8vhwuE+PyjMr494tsA/OHjP3gdxQ79mnc/WdkeePd3CY5WREREWooSsnbmzIl5AGxIPplLSk+la2pXluxc4j1xaQYX/NIr+MEM2PF54gIVERGRFqOErJ3pntONPum7iQVCLP7zYm465SbAu5csEotAn9Mg7xqIVsAL34ZwWYIjFhERkSOlhKwdOvPbo8HFWBfpx/nlufTN7MuafWt4duWzXoGvTYEuJ8D2ZfDGTxMbrIiIiBwxJWTtUI+R/emRug8XCLFy5lvcMfoOAKZ/Mp1tpdsgpQP8y+MQTPGeuPzkqcQGLCIiIkdECVk7dfLXvM5h1+1I5YxIf87udzYHIgeYunCqV6BXLozzh//6A1j/f4kJVERERI6YErJ2asjYIQSJsq9TDhsf+jN3jL6DtKQ03lj3htcNBsBpk+CMmyEWhucmwu61iQ1aREREDosSsnYqOTWJAcM6AbDq09103V7Od0Z8B4Bff/RrorGoV/D8X0LO+VC2G569Cir2JypkEREROUxKyNqxk746CICt2aex7de/ZtLwSfTK6MUXe77gzfVveoWCSfD1xyD7JNixEv724wRGLCIiIodDCVk71n94V1LSgpRm9mHbwlVE/m8BN+beCMDDnz58sJUstRNc+SQkpcGnz8CiJxIYtYiIiBwqJWTtWDApQE5BTwC29ihg2/1TGN9/HL0yerF632p++cEvvR78AbKHwsW/9YZf+yGsnZ+gqEVERORQKSFr54aO7gHAtt5nUrF2HSUv/IUpX5lCSjCF/y38X15e/fLBwqOugS/dArEIzLoWdq5KUNQiIiJyKJSQtXM9T+hEp+w0KpI6sK1HPjunT+eUtBx+cab3ovHfLPgNOw7sOLjAef8BQy+G8r3wzJVQsj0hcYuIiEjzKSFr58yM0742EIB1Qy8nXFzCnqef5v8N/n98pc9X2B/ezx8+/sPBBQJBuPwR6DkSdq+Gx85XS5mIiEg7p4TsKDD09B5k9UznQKAj6/tfwO7HnyC2fz93jL6DpEASL69+mXeK3jm4QEomXPMi9B4Fe9Z5SdnGjxIWv4iIiDROCdlRIBAM8NWrhwKwbuDXKA6nse3+KfTv2J+bcr2Xj/94/o9Zs3fNwYUys2HSqzDkAq+Psif+H6z8WyLCFxERkSYoITtK9Dkxi2Ff6Y2zICtPuoa9L77E/nnzuDH3Rs4fcD4l4RK+//b32Vu+9+BCKZlw1bMw6lqIlMGsifDRownbBhEREamfErKjyJcuO4H0Tsns6ziITb2/wtaf/wJXvJ/7vnwfJ3c5mY37N3LbvNsIx8IHFwomwfg/wtg7wcXgtX+HufdAVXcZIiIiknBKyI4iKekhvnqVd+ly9ZDLKCmOsO1XvyItKY1p50wjOy2bhdsW8vCnD9dc0AzG3gHjp4MF4d3fwUvfhUhlArZCREREalNCdpQZPCqbwaOyiVoyn590NXtffoX9b/+Dnhk9+c2Y32AYj3726MEXkMc79Vq4ehaEMuDTZ71uMUp3tf1GiIiISA1KyI5CY646keS0JHZlDaOoz1fZcvcviO7dS37PfL494ttEXZQfzPtBzZv8qww5Hya/ChnZsOYfMC0PPnpMlzBFREQSSAnZUSijUwpjJ3qXLlflfJ19B5LZet+vALhl1C2c2/9c9lfu57tzv8vOsp11K+hzKlw/F044FyqKvVct/fky2F1PAiciIiKtTgnZUWpIfg+Gj+mDswCrhnyd4r/+lX2vvEIwEOT+r9zPyG4j2Vy6me+/9X0OhA/UrSBrIFz7F/j6nyCti9daNn00/P0OOLC7zbdHRETkeJaQhMzMOpvZbDNbaWYrzOxMM+tiZnPMrND/zEpEbEeT08cPIjktid2dh7Kp15fZ8ou7Kf/8c9KS0vjjOX+kT2Yflu1axk/e+QnRWLT+SkZcDt/7EE652nsH5ocPwR/y4N3fQ7isLTdHRETkuJWoFrI/AK87504CTgFWAHcAbznnhgBv+ePSiLTMZL7yjSEAfDH0G+xKG0jRv91CZMcOuqZ15b/P+286Jndk3sZ5/Oaj3+Aauk8ssztc9hD86ztwwjlQsQ/m3g1/zIdPn4OGkjkRERFpEW2ekJlZJ2AM8BiAc67SObcXmAA84Rd7Ari0rWM7Gp10Ri9OvXAAjgBLR95I8fZS1n3jKsJbtjC402D+cPYfCAVCPLPyGZ5a8VTjlfUcCde+CNf8BXqMgOIiePEmeGA4vPM7qCxtm40SERE5ziSihWwQsAP4k5l9YmaPmlkG0MM5t8UvsxXoUd/CZnajmS00s4U7duxoo5DbtzMmDGbQKd2IBFL44rQbqdy8maIf/ABXWUl+z3x+edYvAZj60VT+94v/bbrCnHPhpvlw6UOQNQj2b4G37vEuZX4wA8LlrbtBIiIix5lEJGRJwKnAQ865UUAptS5POu/aWr3X15xzjzjn8p1z+dnZ2a0e7NHAAsbZ15xEakaIXaG+bBhxJeWfLmHbr38DwMWDL+a2027D4fiP9/+D/1783w1fvqwSCELe1XDLJ16LWe9ToXQ7vP4Tr6uMt++DrZ+puwwREZEWkIiErAgocs596I/PxkvQtplZLwD/c3sCYjtqpXVI5pxJJ4PB6m5fZWf3U9jz9NPse+01AK4bcR0/P+PnBCzAQ58+xC8/+GXDN/rHM/NazG5423svZo+RXovZ/N/AjC97ydmbd8HGjyAWa92NFBEROUZZky0lrbFSs3eA651zn5vZfwAZ/qxdzrkpZnYH0MU59+PG6snPz3cLFy5s5WiPLh+/sZ73X1xNenKU/Ld+TCglwKDnZ5GSkwPA2xve5vZ/3k5lrJIv9f4S93/lfrqkdmn+CpyDNfNg+Uuw8jUojbts3KEXnHQJDBsP/b/kvUdTREREADCzRc65/HrnJSghywMeBZKBNcC38Vrrngf6A+uBK51zjXaIpYSsrljM8b+/WcT2dcUMSlrPoLm/IXnwYAY+/zzBTC/v/WjrR/xw3g/ZW7GX7und+a+v/hd53fMOY2VR2PghLH8FVvzVewigSnpXGDoOTjgb+p8JHXu3zAaKiIgcpdpdQtZSlJDVb9emEp6/7yNiMUfBjufpsOyfpJ9+Ov0enkEgNRWAraVbuf2ft7N4x2KSLIl/Gfov3HzKzXRO7Xx4K3UONn8CK17xErTdq2vO7zESBn8V+hZAv9FK0ERE5LijhOw4tOCva/jotXVkdkpi9Ae/hG1FZH71q/T94zQsORmAcCzM7xf9nieXPwlAn8w+/OHsPzC0y9AjW7lzsGMlrHgVihbAuvcgXKvLjI59vJazE86GAV+CzgMhoBdHiIjIsUsJ2XEoGokx+9cL2bmxhJ59Uxj26k9g7y66fOc6etx+e42yX+z5gl+89wuW7VpGWlIaPy74MZcPuZyAtVCCFKmA9f8HGz7wErSiRV7ns/GSM6H7ydBjOHQfDj2GQfdhkH4I97eJiIi0Y0rIjlPFO8v4y9RFlO6rpG//JHKe+T6BaIQed95B1jXXYHEtUuWRcv7z/f/kr2v+CkBO5xxuyr2J8wecTzAQbNnAYjHY+TmsnQ+r/+Fd6izZWn/ZDr28RC37JO/9m1kDofMA6NwfktNbNi4REZFWpITsOLZ7cykv/tfHlJeG6d95Pye8dCeGo/O/fJ2e99xTIylzzvH6utf5r4X/xbYD2wAY1GkQN4y8ga8N+hpJgVZ8arJ0J2xbBtuWwrblsH25d9mzvhejV8ns4SVnWQOgUz8vSevcDzr1h059lbCJiEi7ooTsOLd9fTEvPfAJ4fIoQwbG6Pfsj6Cigg4XXEDvKfcTSK+ZuFRGK3lp1Us89tljbC7dDED/Dv256ZSbuHjQxS3fYtaQWAz2rIXtK2DnF7B3PexZ733u3eC9DL0x6d38BM1P1jr188f7QkoHCGV4HeBawBsPhtpmu0RE5LikhEzY9MUe/vrHT4mGY5x4YpB+z9yOK9lP+pln0G/GDAIpKXWWCcfCvLr6Vf7ns/9h4/6NgNdi9r2873H+gPNb7h6zwxGLQvHmg0navo2wdyPs2+B/FkEsfGh1htIhtdPBn5SOkJzh/YTS6w6ndPB+qqYnZ8aVyVA/bCIiUoMSMgFg/dJd/H3GZ0QjMQaflM6g/70Dt2Mbqafk0nfaHwn16F7vcpFYhL+t/Rv/vfi/2VSyCYCTu5zMD079AWf2PhMza8vNaJ5YDEq2+YnahriEbaOXyFXs9y6HupjX0lZRAq4Zby44FMEUCKV6n0kpEEz2PkPpkNrRT+g6Hkz8kpIhqap81bD/mZTsT48frqo3xUv+gskQCPmtfu3wmIiIHOeUkEm1TZ/v4bWHlhAuj9J3UConvXkPsU0bCGZ3o8+vf03Gl77U4LLhaJgXV73Iw58+zPYy781WQ7OGctVJV3FpzqWte49Za3MOKkuhfB+U74XyYqgohsoSb3rlAa/rjsoD/niJ91NRcnA8XDXPH3eJepWUHUzUqpM3P1kLhiCQ5H0Gk/3hZH885JdJrpng1U7uLOAlfYEkMP8zEDw4rerHAnHjwZqf5l8qNvPrCNSsy4IHLye7mP/OVOdtm5m/bCCubFLDddQoEzy4PhGRNqaETGrYvr6Yv0771LvRf2hHRnwynfIF3qtFO11+OT3u+AnBjh0bXL4sUsazK59l5tKZ7PO7r+iR3oPLhlzGZTmX0TtTnb7iHITLIFLudfsRrYBIpfdZecBroavY5yd++70kLr5MpKLucjWmVQ1X+p9h7xJttDKBieDRxOpJ1AK1kra4JK8qAaxOIgPUTA7jpwVqJYxWMwHFT3CrE91mjNdJiptTtoF624s6rbj1xNemZZqckOCY6ynXnDI1xP2+r/O7v7FcoJFzqcHzraFptZdrYF31zmrlZTK6wZnfa7iuFqCETOqIf/pywIgunBb4iH0zpuMqKwn16UOf3/0Xaaec0mgdldFK5qyfw0OfPsT64vUAGMboXqO5NOdSzu1/LmlJaW2xORIvFq2VzFX9+AlbLAzRSNxwOG5epNZwZd36qy7zxiLeumLRg+M15tWa76Jx0yIHW76cX8bFapWNep9VyYy3cn+ZWstWLxOruXx1TLViEBGprdtQ+P6CVl2FEjKp17Z1xfz1j4upKI2Q1iHEl87JIuWxeyhfuhSSkujyrW/R7V9varS1DCDmYizcupDZhbN5a/1bVMa8X+KZoUwuGnQRXxv4NfK655EcTG6LzRJpWixWM0GsTuCi9Sd01QmgP4yrNc3VmhaXYDp/Xc7FtUr4n80aP5Ky1D+ecM2J73guU99izWjRqrdMY61qzWhxa/C8q29afS1wjSzX4Lrqndn6y6R3hfxvN1LfkVNCJg3au+0Abz2xgq1rvEuPA0d0IWfXP4k9OwOApO7d6fmf99Bh7Nhm1VdcWczra1/npVUv8dnOz6qnpyWl8ZU+X+GCgRcwrMsw+nbo2z4fBhAREWklSsikUc45PptXxPsvriZS6d1/NCgnhT6fPEfyorkAdLzkEnr85MckZWc3u95Ve1bxyppXeHfTuxTuKawxr3tad0b3Gs1X+nyFM3qfQZdUvSJJRESObUrIpFlK91Xw8evrWfrOJmIR77zISq+g+9K/0mPT/5GcFiLrqm/Q+etfJ3ngwEOqe2vp1upWsyU7lrC3Ym+N+f069CM3O5eR3UaSl53HiV1OJBRQR60iInLsUEImh2T/7nI+nbuRlR9uoaLUuwE65Crou24OfTf9k1C0jA4XXkjXG64nbfjwQ64/5mKs2buG9za/x7ub3uXTHZ9SFimrUSYlmMLwrsM5JfsUhnUbxoAOAxjQcQDpIb0OSUREjk5KyOSwRMJR1izewWf/2FR9j1kSEXoX/ZP+6+eQHN5Pxlln0fXGG0kfXXDY94RFYhFW7V3Fkh1L+HTHpyzZsYR1xevqLds9rTsDOg1gYMeBDOjoJWk90nvQPb07WalZiX17gIiISCOUkMkRcc6xuXAvi/6+jo0r9gAQsBi9t/4f/de8TmrFHkL9+5Nx1pfo+LWvkV5w+MlZlb3le1my00vQCvcUsqF4Axv3b6x+grM+SZZEl9QupCSlkBnK5ITOJ9AzoyfZadn0yuhFr8xedEntQlZKFiG9t1JERNqYEjJpMdvWFbPo7+tY++lOAAxH712L6Ll+Hh32ryfgYoT69iVl6FAyzjiDTuP/H8FOnVpk3dFYlC2lW1hfvJ51xetYu28tRSVFbD+wnW2l2yiuLG52XR2SO1QnZ1mpWXRI7kBmKJOMUAaZyZlkhvwffzh+ekYo4+h+K4GIiCSEEjJpcbs2lbDo9fWsWritukuX1GCY3pveodeaOaTEJUfBzp1JL8inw3nnkZqbS8qgQa0SU0W0gj3le6o/1+xb4yVrB7axpXQL20q3sbt8N3sr9hI7wt7s05LSyAxlkhxMJhQIkRRIIhQI1RwOhkgOJJMSTCE5mExysOZw0II454gRI2hBMkIZZIQySEtKq1FP7brBu8yblpRGeiidlGAKZkb1f2YELUjAAgQD/qc/LiIiiaOETFrN3m0H+PStjWxcuZt9270b8y0A/buH6b72bTI++weBA/trLJM6fDgdL7mEDuefT3LfPm0ec8zF2Fexjz3le6oTtP2V+ykNl1ISLqGksoSScAml4VL2h/dTWulPD5dUD7tGXzPSflUnahasmbjFfwYCdaZX/dQ3vbHL0y3x/VK1vtrJZdCCOBzOuZqfccNVqhLVqv+8//1hODjP35b48lXj1cvUSn7rXUfcslX1V5eJW2ftZesbjq8jPp4aZSyunlr1N1iHGQECNY5jfTHXqaOBMvGaVc+RLNucempNb87y8dMbqqvFlm1uPXVG65631csZNcZbs6/Hpo5BU9tXYztrDNb/76MtpCalckp242+oOVJKyKTVOefY/MVelswrYu3iHdWtZsGkAD36ppJVUUSH7StJ+egNgsU7q5dL6tWL9Px8UocNIyXnBNLz8wmkte/XLcVcjLJIGfsr9xOOhgm7MOFomIiLeJ+xCOFYmHAsTGW0kopoBZXRyoPDMW84Eot4SQYBwrEwByIHKKksoSxSRiQWOVhfrXodjqRAEuWRcg5EDlARqahOPmIuhnOOqIsSc7EanyIi0rDBnQbz8qUvt+o6lJBJm9q/u5zl721m/We72LGhZutYIGj07h6l146FZH70V2zf7hrzLTmZ9PzTSMsbRVL3bIJdu5LUrZv307UrgXR1e3G4aiRosSgOP3GLxepN4Ko//fkNlW9KU3/dNtXCVrWe+NirptVukardShVfT+1WtOo3ulD/vKokt3bLW/UyjQxX10PNVsIa5WrVX2+ZuOHa29JUmXpjO7jROLx9G3Ox6mN7sIL4wbr11pneQJl4zaqngek16jmCGBIZh6u5U5tctvby9dXd1PFtLU1tY1Pb2uS+a2D/tLZeGb2498v3tuo6lJBJwpTtr2Tzqr1sLtzLjg372bp6X3XrmRlkdAjSJaWEvtG1ZBS+T3TpJ42+myyQnk6oTx/STjuV5P4DCPXpQ/LAAaSccAKWpBvtRUSk/VJCJu1G6b4KCj/axucfbmVXUUnN3MugS49UOgVLSC3fRZfIVpL3bSWweyuBnRuJ7dyFq6y/2wtLSyNtxAjS8k4h7ZRTSB0xgqQuXXDOEUhJaZuNExERaYQSMmmXopEY+7aXserj7WxcvovtG/ZXv7KptpT0JLr0ziC7VypZScVk7F5D0s4i3Kb1VK5eRXjjxgbXE+zUiaTevQn16kVS92wslIwFzHv6IBAgkJ5OICODQEY6wcxMfziDQO3h5OTW2hUiInIcUEImR4VIOMr2dcXs21HGrk2l7NiwnwPFlZTtr6TiQKTeZZJCAdI7JZOeESQlVkqoZCdJOzYQ3LyGpH3bSY6UkFG8iYCrf/lDEgoRTE+vkagRCEAk4rXEpaeT1CWLYFYXApmZWEoygZQULDkFS02pO5ySggtHwCCpWzbBzp2w5GTvJxRq1SekRESk7TWWkOmmG2k3kkJBeg/JoveQrBrTnXMcKK5k58YStq8vZvv6/ezZUkppcSWRiijFO8s5+OBmN0jqBv1PPViBQWqqkdUhSihWQTAQI8liBAMxgkQJRisIhMsJVB4gUFlGoLwUyg9QVhnElZcTKNuPHSgmWHmApEgZwV3lBHaUYC6CuRhJkXKsNW48DYUIhEJechafqCUnQzCIC1fiKsMQiUBSEhb3QygJC9YaTwrVLJMUBDPMAiRld8OSU7wb+wL+DfFJ/jIhf7nkkNeqaP6N+Oa3MprhysuI7i/xdndKMoHUVC9ZhRo36Na8Ru3qTosfDgSw5BQCqV7yaikpWCjZL+O8T//HuYM36VvAvHVbAAsG6gxbwJ8WCHplg/5+CAZrzK8uF/S6CKFquhJlEWkFSsik3TMzMjqlkNEphQEjutaYV1ke4cC+Skr3VlBaXHFweJ/Xsla6t4I92w5QXubYUhYAmtGlRhDI8H+aIRCAjMwASUEH0SiBWJiQRUgijMWixKIOF4uRHCsnOVaGRStIiZSSVFlCMOAIxCKwZyeB/buhsoJYNEawooRYIETMQsRiIaKVIaiE5MpiQpGdmIuSFCknkpRGJCkVHH5S6AjEooTCJRgOhxFJSsNZoPonFghV/0QDIW855wjEwgRiEQKxMEF/2FyEcJK3I7xp3o+5aBv1DNQOmR1M0hpM4gKYBWqVMywQ9JLb1FQsFPKT36oEserJ0Cb2bFMJYXWyjFd39bh5CWwshnMxL4GNxcDvKoVYPfNi3hOYVfOqk19/HjhcLC6JrkrWiVtnIHCwf6zAwXgsPi6zg+uOuUYf7Kn+oyEQrLGfqxP0qpirtqE5x7PGZ/zm1J7X0GfVaPyxad4yVeN11nVI62ug3OEsU2PZQ1ym1jY1b9mayxzOPmxwHc1ZJm7ZYNcudJ08mURRQiZHteTUJJJTk+jco+HuMKLRGKV7Kti1uZRwRYRIZYxwRZRIZdQb9j8j/rRwZZRoOEZ6pxQsAJEKr0xlWYTKsgiRcIxoxPuJRR3h8ij7i6u++ANAiv/TAANC/k+VZiZ/zWUGyclGZaVr9Hfb4XNe6yJ+Vwn+L2LDeT8uVj3slbY6PzFnxAjgCNSot2qZqgTTnD/NxQ4Ox9VUo3bnMGJey6ULEzbvvj9zMX/5mj9VdR5scfOPY6xqmj+/6oeDyYL58fp9DMS1kjp/mZjXemhBYhbEBZK84UDQ+3RBnAuCg2C4gmC0kkAs4tXSZGLmasQdv68OxhPzt90BsRpJuznvGFF1rPwyFr8/aqgbizOAQPX2VJdxeHXFDtbnzMA/zuaicT8x70j78129yaa/v13N8abi87an6lz0k8z4YwR1Wmyra4nbB9XHtXr5uONfZ3lXY9nq5WvEXns98efOwXXHddpSI/aD8dRaB3jna9wyVedAjfXEnS/NOMwH63Gu5vI1zrva8w+eR/Hn08Hh+PG2Ere/6pzj3nDKoEFKyERaUzAYoGO3NDp2a50OZ8OVUUp2lxONxHAOIpWx6uQtFnMEgl5rQOneCirLI0QqoxzYV0lFWYRoOEbE/6k4EMbFHBYwwhVRkkIBkpKDBJMCJCUHcDE4sL+SygMRIhEvgUxOSyI5Neh9kfq/C6KRGOUlYSoqvC+Z5LQkgklVrRJU15sUChAMeb8kXcwRjTiikRgRPyGNRGJEwzFS0kOY4SWifqwuBlGSiFZ9hdT3O7yl1PrjV0SkNXRMjzAkgetXQiZyhELJQbJ6tnATVxOc8y4VBYL1v58yGo5RURYhJT2JYFLLv8MyFvUTycqDl4S8mCAW8xI2F3PEYs6/ImVYwPwHW73hQNAIJgW8hNW/wHrw1jBvOBarmubqHY7FvP3g/YHuqsdjUUe4IkpKuvcVVz0/6s93B8t5BeIbMeJaFR014orf1gbn+8t7215zOwNBIxg8OBxICoDzkvpIRZRoJBZ32a8R/npddQPIwfvo4vefq9qWGsOuuox3vPx96A7ux+aqPo7BwMGEuUbd3h8pgYBV/9EQizpiUa912TlXfW5Ut7JWNbQ5qndqo41j1cHU2j9V2+Mfb+o5ptVVVu2XGvu31vprx1OrTK2GrHrPmzrrqFFRXL3xHafWt3yNel2NWFytel1cfd75cvD8djFX8/IedUbrPWfqnEtV50/8uReLW6a6QS5un7RB61id41m1QbWPPV5soS6dWz2mxighEzkKmRkWbPiXdjAUID3Uet10BIIBkoMBklNbbRUiIseVlv/TWUREREQOScISMjMLmtknZvaqPz7IzD40s1VmNsvM1AuniIiIHBcS2UJ2K7AibvzXwAPOuRxgD/CdhEQlIiIi0sYSkpCZWV/gYuBRf9yAc4DZfpEngEsTEZuIiIhIW0tUC9nvgR9DVScpdAX2Olf9fpsioE99C5rZjWa20MwW7tixo9UDFREREWltbZ6QmdklwHbn3KLDWd4594hzLt85l5+dnd3C0YmIiIi0vUR0e3EWMN7MxgGpQEfgD0BnM0vyW8n6ApsSEJuIiIhIm2vzFjLn3J3Oub7OuYHAVcDbzrmJwD+Ar/vFJgEvt3VsIiIiIonQnvoh+wnwQzNbhXdP2WMJjkdERESkTSS0p37n3Dxgnj+8BhidyHhEREREEqE9tZCJiIiIHJeUkImIiIgkmBIyERERkQQz51yiYzhsZrYDWN8Gq+oG7GyD9Ujz6Zi0Tzou7ZOOS/ujY9I+tfZxGeCcq7cT1aM6IWsrZrbQOZef6DjkIB2T9knHpX3ScWl/dEzap0QeF12yFBEREUkwJWQiIiIiCaaErHkeSXQAUoeOSfuk49I+6bi0Pzom7VPCjovuIRMRERFJMLWQiYiIiCSYErJGmNlFZva5ma0yszsSHc/xxMxmmtl2M1saN62Lmc0xs0L/M8ufbmY2zT9OS8zs1MRFfuwys35m9g8zW25my8zsVn+6jksCmVmqmS0ws0/943KPP32QmX3o7/9ZZpbsT0/xx1f58wcmdAOOYWYWNLNPzOxVf1zHJMHMbJ2ZfWZmi81soT+tXXyHKSFrgJkFgQeBrwHDgG+a2bDERnVceRy4qNa0O4C3nHNDgLf8cfCO0RD/50bgoTaK8XgTAf7dOTcMOAP4nv9vQsclsSqAc5xzpwB5wEVmdgbwa+AB51wOsAf4jl/+O8Aef/oDfjlpHbcCK+LGdUzah7Odc3lx3Vu0i+8wJWQNGw2scs6tcc5VAs8BExIc03HDOTcf2F1r8gTgCX/4CeDSuOlPOs8HQGcz69UmgR5HnHNbnHMf+8P78X7R9EHHJaH8/Vvij4b8HwecA8z2p9c+LlXHazZwrplZ20R7/DCzvsDFwKP+uKFj0l61i+8wJWQN6wNsjBsv8qdJ4vRwzm3xh7cCPfxhHas25l9SGQV8iI5LwvmXxhYD24E5wGpgr3Mu4heJ3/fVx8Wfvw/o2qYBHx9+D/wYiPnjXdExaQ8c8KaZLTKzG/1p7eI7LKm1KhZpTc45Z2Z6RDgBzCwT+F/gB8654vg/5HVcEsM5FwXyzKwz8CJwUmIjOr6Z2SXAdufcIjMbm+BwpKYvO+c2mVl3YI6ZrYyfmcjvMLWQNWwT0C9uvK8/TRJnW1Vzsf+53Z+uY9VGzCyEl4w97Zz7iz9Zx6WdcM7tBf4BnIl3eaXqj+74fV99XPz5nYBdbRvpMe8sYLyZrcO73eUc4A/omCScc26T/7kd74+X0bST7zAlZA37CBjiPxWTDFwFvJLgmI53rwCT/OFJwMtx07/lPxFzBrAvrvlZWoh/T8tjwArn3O/iZum4JJCZZfstY5hZGnA+3v19/wC+7herfVyqjtfXgbedOqRsUc65O51zfZ1zA/F+d7ztnJuIjklCmVmGmXWoGgYuAJbSTr7D1DFsI8xsHN59AEFgpnPuvsRGdPwws2eBsUA3YBtwN/AS8DzQH1gPXOmc2+0nCtPxnso8AHzbObcwAWEf08zsy8A7wGccvC/mp3j3kem4JIiZ5eLdiBzE+yP7eefcf5rZYLzWmS7AJ8A1zrkKM0sF/ox3D+Bu4Crn3JrERH/s8y9Z/sg5d4mOSWL5+/9FfzQJeMY5d5+ZdaUdfIcpIRMRERFJMF2yFBEREUkwJWQiIiIiCaaETERERCTBlJCJiIiIJJgSMhEREZEEU0ImIgKYmTOz/4ob/5GZ/UcCQxKR44gSMhERTwVwuZl1S3QgInL8UUImIuKJAI8AtyU6EBE5/ighExE56EFgopl1SnQgInJ8UUImIuJzzhUDTwK3JDoWETm+KCETEanp98B3gIwExyEixxElZCIicZxzu/FeNPydRMciIscPJWQiInX9F6CnLUWkzZhzLtExiIiIiBzX1EImIiIikmBKyEREREQSTAmZiIiISIIpIRMRERFJMCVkIiIiIgmmhExEREQkwZSQiYiIiCSYEjIRERGRBPv/rzs8AP25cocAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (10, 6))\n",
    "\n",
    "etaList = [1.0, 0.1, 0.1, 0.1, 0.1]\n",
    "subsampleList = [1.0, 1.0, 0.8, 0.5, 0.4]\n",
    "\n",
    "rmse_testList = []\n",
    "rmse_trainList = []\n",
    "\n",
    "for eta, subsample in zip(etaList, subsampleList):\n",
    "    gb = GradientBoostingRegressor(n_estimators = 500, max_depth = 5, \n",
    "                                   learning_rate = eta, subsample = subsample, random_state = 123)\n",
    "    gb.fit(X_train, Y_train)\n",
    "    \n",
    "    rmse_test = [np.sqrt(metrics.mean_squared_error(Y_test, predict)) for predict in gb.staged_predict(X_test)]\n",
    "    rmse_testList.append(rmse_test)\n",
    "    \n",
    "    rmse_trainList.append(np.sqrt(gb.train_score_[-1]))\n",
    "    \n",
    "    plt.plot(rmse_test, linewidth = 2, label = 'eta = {s[0]}, subsample = {s[1]}'.format(s = [eta, subsample]))\n",
    "\n",
    "plt.title(\"Зависимость ошибки GB на тестовой выборке от шага и объема выборки\", fontsize = 10)\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.xlabel(\"N\", fontsize = 10)\n",
    "plt.ylabel(\"RMSE\", fontsize = 10)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "eta = 1.0, subsample = 1.0: RMSE TRAIN = 0.0003   RMSE TEST = 117.4376   (RMSE TEST - RMSE TRAIN) = 117.43730000000001\n",
      "eta = 0.1, subsample = 1.0: RMSE TRAIN = 1.5171   RMSE TEST = 52.7967   (RMSE TEST - RMSE TRAIN) = 51.2796\n",
      "eta = 0.1, subsample = 0.8: RMSE TRAIN = 0.768   RMSE TEST = 47.3957   (RMSE TEST - RMSE TRAIN) = 46.6277\n",
      "eta = 0.1, subsample = 0.5: RMSE TRAIN = 1.1214   RMSE TEST = 43.6891   (RMSE TEST - RMSE TRAIN) = 42.5677\n",
      "eta = 0.1, subsample = 0.4: RMSE TRAIN = 1.3286   RMSE TEST = 42.9693   (RMSE TEST - RMSE TRAIN) = 41.640699999999995\n"
     ]
    }
   ],
   "source": [
    "for rmse_train, rmse_test, eta, subsample in zip(rmse_trainList, rmse_testList, etaList, subsampleList):\n",
    "    print('eta = {s[0]}, subsample = {s[1]}: RMSE TRAIN = {s[2]}   RMSE TEST = {s[3]}   (RMSE TEST - RMSE TRAIN) = {s[4]}'.\\\n",
    "         format(s = [eta, subsample, \n",
    "                     round(rmse_train, 4), \n",
    "                     round(rmse_test[-1], 4), \n",
    "                     round(rmse_test[-1], 4) - round(rmse_train, 4)\n",
    "                    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "По полученным данным видно, что при уменьшении объема выборки для обучения каждого базового алгоритма, ошибка на обучении становится выше. При этом, обобщающая способность модели возрастает - переобучение становится меньше."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Определение оптимальных гиперпараметров\n",
    "\n",
    "*Гиперпараметры* - это параметры модели, значения которых не оптимизируются в процессе построения. Они настраиваются заранее, а для того, чтобы подобрать оптимальные значения гиперпараметров, необходимо проводить многократное обучение модели на разных значениях таких параметров и выбирать оптимальные. \n",
    "\n",
    "Рассмотренные выше параметры - количество деревьев, размер шага, объем выборки для построения каждого нового алгоритма в композиции - всё это гиперпараметры модели. Также гиперпараметром является максимальная глубина дерева.\n",
    "\n",
    "Существует несколько способов определения оптимальных значений таких параметров. Подробнее о них мы поговорим позже. Сейчас же рассмотрим результат работы одного из них - [*полный поиск по сетке(GridSearchCV)*](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html). \n",
    "\n",
    "Суть метода следующая: \n",
    "- Определяется список гиперпараметров, которые нужно подобрать;\n",
    "- Для каждого гиперпараметра задается множество возможных значений;\n",
    "- Строится декартово произведение заданных множеств;\n",
    "- Для каждого элемента декартова произведения строится модель и оценивается ее качество по выбранной метрике;\n",
    "- После этого выбирается комбинация гиперпараметров, ошибка на которых минимальна.\n",
    "\n",
    "Недостаток данного подхода заключается в том, что он может быть очень времязатратным. \n",
    "\n",
    "На примере ниже отбираются оптимальные значения $4$ гиперпараметров, а соответствующие им множества имеют мощности $5, 5, 4$ и $7$ соответственно. Таким образом, в ходе отбор оптимальных значений параметров, будет построено $5 \\cdot 5 \\cdot 4 \\cdot 7 = 700$ моделей. Кроме того, качество полученных моделей проверяется на кросс - валидации. Поэтому, для получения итогового количества моделей, рассчитанное значение нужно умножить на количество фолдов. В данном случае, это $3$. В итоге, мы получили $2100$ моделей. Отсюда и следует описанный выше недостаток. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 700 candidates, totalling 2100 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    6.1s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   55.3s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  4.8min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  8.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed: 12.5min\n",
      "[Parallel(n_jobs=-1)]: Done 2100 out of 2100 | elapsed: 15.2min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=nan,\n",
       "             estimator=GradientBoostingRegressor(alpha=0.9, ccp_alpha=0.0,\n",
       "                                                 criterion='friedman_mse',\n",
       "                                                 init=None, learning_rate=0.1,\n",
       "                                                 loss='ls', max_depth=3,\n",
       "                                                 max_features=None,\n",
       "                                                 max_leaf_nodes=None,\n",
       "                                                 min_impurity_decrease=0.0,\n",
       "                                                 min_impurity_split=None,\n",
       "                                                 min_samples_leaf=1,\n",
       "                                                 min_samples_split=2,\n",
       "                                                 min_weight_fraction_leaf=0.0,\n",
       "                                                 n_estimators=100,\n",
       "                                                 n_iter_n...\n",
       "                                                 subsample=1.0, tol=0.0001,\n",
       "                                                 validation_fraction=0.1,\n",
       "                                                 verbose=0, warm_start=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'learning_rate': [0.2, 0.1, 0.05, 0.01],\n",
       "                         'max_depth': [4, 5, 6, 7, 8],\n",
       "                         'n_estimators': [100, 200, 300, 400, 500],\n",
       "                         'subsample': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='neg_mean_squared_error', verbose=1)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [100, 200, 300, 400, 500],\n",
    "    'max_depth': [4, 5, 6, 7, 8],    \n",
    "    'learning_rate': [0.2, 0.1, 0.05, 0.01], \n",
    "    'subsample': [0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1.0]\n",
    "}\n",
    "\n",
    "gb = GridSearchCV(GradientBoostingRegressor(random_state = 123),\n",
    "                   param_grid,\n",
    "                   n_jobs = -1, \n",
    "                   scoring = 'neg_mean_squared_error', \n",
    "                   cv = 3, \n",
    "                   verbose = 1)\n",
    "\n",
    "gb.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим на получившиеся оптимальные значения гиперпараметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'learning_rate': 0.05, 'max_depth': 4, 'n_estimators': 500, 'subsample': 0.4}"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gb.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним результат на оптимальных параметрах с полученным ранее"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_test_best = [np.sqrt(metrics.mean_squared_error(Y_test, predict)) for predict in gb.best_estimator_.staged_predict(X_test)]\n",
    "rmse_train_best = np.sqrt(gb.best_estimator_.train_score_[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x432 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.figure(figsize = (10, 6))\n",
    "\n",
    "plt.plot(rmse_test_best, linewidth = 2, label = 'best params'.format(s = [eta, subsample]), linestyle = '--')\n",
    "\n",
    "\n",
    "for rmse_test, eta, subsample in zip(rmse_testList, etaList, subsampleList):\n",
    "    plt.plot(rmse_test, linewidth = 2, label = 'eta = {s[0]}, subsample = {s[1]}'.format(s = [eta, subsample]))\n",
    "\n",
    "plt.legend(loc = 'upper right')\n",
    "plt.xlabel(\"N\", fontsize = 10)\n",
    "plt.ylabel(\"RMSE\", fontsize = 10)\n",
    "    \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "               best params: RMSE TRAIN = 7.9466   RMSE TEST = 33.0329   (RMSE TEST - RMSE TRAIN) = 25.0863\n",
      "eta = 1.0, subsample = 1.0: RMSE TRAIN = 0.0003   RMSE TEST = 117.4376   (RMSE TEST - RMSE TRAIN) = 117.4373\n",
      "eta = 0.1, subsample = 1.0: RMSE TRAIN = 1.5171   RMSE TEST = 52.7967   (RMSE TEST - RMSE TRAIN) = 51.2795\n",
      "eta = 0.1, subsample = 0.8: RMSE TRAIN = 0.768   RMSE TEST = 47.3957   (RMSE TEST - RMSE TRAIN) = 46.6277\n",
      "eta = 0.1, subsample = 0.5: RMSE TRAIN = 1.1214   RMSE TEST = 43.6891   (RMSE TEST - RMSE TRAIN) = 42.5677\n",
      "eta = 0.1, subsample = 0.4: RMSE TRAIN = 1.3286   RMSE TEST = 42.9693   (RMSE TEST - RMSE TRAIN) = 41.6407\n"
     ]
    }
   ],
   "source": [
    "print('               best params: RMSE TRAIN = {s[0]}   RMSE TEST = {s[1]}   (RMSE TEST - RMSE TRAIN) = {s[2]}'.\\\n",
    "     format(s = [round(rmse_train_best, 4), \n",
    "                 round(rmse_test_best[-1], 4), \n",
    "                 round(rmse_test_best[-1] - rmse_train_best, 4)\n",
    "                ]))\n",
    "\n",
    "for rmse_train, rmse_test, eta, subsample in zip(rmse_trainList, rmse_testList, etaList, subsampleList):\n",
    "    print('eta = {s[0]}, subsample = {s[1]}: RMSE TRAIN = {s[2]}   RMSE TEST = {s[3]}   (RMSE TEST - RMSE TRAIN) = {s[4]}'.\\\n",
    "         format(s = [eta, subsample, \n",
    "                     round(rmse_train, 4), \n",
    "                     round(rmse_test[-1], 4), \n",
    "                     round(rmse_test[-1] - rmse_train, 4)\n",
    "                    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получили, что качество на оптимальных параметрах заметно выше того, что мы получали при случайном подборе."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Сравнение бустинга со случаным лесом"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим какое качество дает *случайный лес* при значении параметров, оптимальных для *градиентного бустинга*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "rf = RandomForestRegressor(max_depth = gb.best_params_['max_depth'],\n",
    "                           n_estimators = gb.best_params_['n_estimators'],\n",
    "                           random_state = 123)\n",
    "\n",
    "rf.fit(X_train, Y_train)\n",
    "\n",
    "rmse_test_rf = np.sqrt(metrics.mean_squared_error(Y_test, rf.predict(X_test)))\n",
    "rmse_train_rf =np.sqrt(metrics.mean_squared_error(Y_train, rf.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       GB best params: RMSE TRAIN = 7.9466   RMSE TEST = 33.0329   (RMSE TEST - RMSE TRAIN) = 25.0863\n",
      "RF best params for GB: RMSE TRAIN = 112.4472   RMSE TEST = 114.3697   (RMSE TEST - RMSE TRAIN) = 1.9226\n"
     ]
    }
   ],
   "source": [
    "for rmse_train, rmse_test, name in zip([rmse_train_best, rmse_train_rf],\n",
    "                                       [rmse_test_best[-1], rmse_test_rf],\n",
    "                                       ['       GB best params:',\n",
    "                                        'RF best params for GB:']):\n",
    "\n",
    "    print('{s[0]} RMSE TRAIN = {s[1]}   RMSE TEST = {s[2]}   (RMSE TEST - RMSE TRAIN) = {s[3]}'.\\\n",
    "         format(s = [name,\n",
    "                     round(rmse_train, 4), \n",
    "                     round(rmse_test, 4), \n",
    "                     round(rmse_test - rmse_train, 4)\n",
    "                    ]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Случайные лес* на тех же параметрах, что и бустинг показывает качество значительно ниже. Найдем оптимальные параметры для *леса*."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 77 candidates, totalling 231 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  47 tasks      | elapsed:   13.0s\n",
      "[Parallel(n_jobs=-1)]: Done 200 tasks      | elapsed:  1.4min\n",
      "[Parallel(n_jobs=-1)]: Done 231 out of 231 | elapsed:  1.7min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=3, error_score=nan,\n",
       "             estimator=RandomForestRegressor(bootstrap=True, ccp_alpha=0.0,\n",
       "                                             criterion='mse', max_depth=None,\n",
       "                                             max_features='auto',\n",
       "                                             max_leaf_nodes=None,\n",
       "                                             max_samples=None,\n",
       "                                             min_impurity_decrease=0.0,\n",
       "                                             min_impurity_split=None,\n",
       "                                             min_samples_leaf=1,\n",
       "                                             min_samples_split=2,\n",
       "                                             min_weight_fraction_leaf=0.0,\n",
       "                                             n_estimators=100, n_jobs=None,\n",
       "                                             oob_score=False, random_state=123,\n",
       "                                             verbose=0, warm_start=False),\n",
       "             iid='deprecated', n_jobs=-1,\n",
       "             param_grid={'max_depth': [4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30],\n",
       "                         'n_estimators': [20, 50, 100, 200, 300, 400, 500]},\n",
       "             pre_dispatch='2*n_jobs', refit=True, return_train_score=False,\n",
       "             scoring='neg_mean_squared_error', verbose=1)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "param_grid = {\n",
    "    'n_estimators': [20, 50, 100, 200, 300, 400, 500],\n",
    "    'max_depth': [4, 5, 6, 7, 8, 9, 10, 15, 20, 25, 30]\n",
    "}\n",
    "\n",
    "rf = GridSearchCV(RandomForestRegressor(random_state = 123),\n",
    "                   param_grid,\n",
    "                   n_jobs = -1, \n",
    "                   scoring = 'neg_mean_squared_error', \n",
    "                   cv = 3, \n",
    "                   verbose = 1)\n",
    "\n",
    "rf.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'max_depth': 15, 'n_estimators': 500}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rmse_test_rf_best = np.sqrt(metrics.mean_squared_error(Y_test, rf.best_estimator_.predict(X_test)))\n",
    "rmse_train_rf_best = np.sqrt(metrics.mean_squared_error(Y_train, rf.best_estimator_.predict(X_train)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "       GB best params: RMSE TRAIN = 7.9466   RMSE TEST = 33.0329   (RMSE TEST - RMSE TRAIN) = 25.0863\n",
      "RF best params for GB: RMSE TRAIN = 112.4472   RMSE TEST = 114.3697   (RMSE TEST - RMSE TRAIN) = 1.9226\n",
      "       RF best params: RMSE TRAIN = 32.1185   RMSE TEST = 78.5415   (RMSE TEST - RMSE TRAIN) = 46.423\n"
     ]
    }
   ],
   "source": [
    "for rmse_train, rmse_test, name in zip([rmse_train_best, rmse_train_rf, rmse_train_rf_best],\n",
    "                                       [rmse_test_best[-1], rmse_test_rf, rmse_test_rf_best],\n",
    "                                       ['       GB best params:',\n",
    "                                        'RF best params for GB:',\n",
    "                                        '       RF best params:']):\n",
    "\n",
    "    print('{s[0]} RMSE TRAIN = {s[1]}   RMSE TEST = {s[2]}   (RMSE TEST - RMSE TRAIN) = {s[3]}'.\\\n",
    "         format(s = [name,\n",
    "                     round(rmse_train, 4), \n",
    "                     round(rmse_test, 4), \n",
    "                     round(rmse_test - rmse_train, 4)\n",
    "                    ]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Посмотрим сколько шагов потребовалось градиентному бустингу, чтобы достичь качества, полученное случайным лесом."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.where(rmse_test_best > rmse_test_rf_best)[0][-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Из полученных данных можно сделать следующие выводы:\n",
    "\n",
    "- Для получения оптимального качества с помощью случайного леса необходимо обучать базовые алгоритмы (деревья), глубина которых на порядок выше чем в случае градиентного бустинга;\n",
    "\n",
    "\n",
    "- Из - за того, что процесс построения композиции ненаправленный, для получения оптимального качества приходится обучать большое количество деревьев. При этом итоговое качество, в большинстве случаев, сильно ниже чем качество, полученное с помощью градиентного бустинга.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 7. Популярные алгоритмы градиентного бустинга\n",
    "\n",
    "На сегодняшний день наибольшей популярностью пользуются три метода: \n",
    "- [**XGBoost**](https://xgboost.readthedocs.io/en/latest/index.html) - самая ранняя из рассматриваемых библиотек. При разбиении каждой вершины дерева предварительно сортирует значения всех признаков участвующих в построении данного дерева. После этого для каждого признака определяет оптимальное разбиение на основе [*критерия информативности*](https://en.wikipedia.org/wiki/Information_gain_ratio). Далее выбирает признак с наилучшим разбиением. Не умеет работать с категориальными признаками. В случае, если они числовые, рассматривает их как вещественные переменные. Если передать текстовые данные - упадет. Дерево строится до указанной глубины (гиперпараметр *max_depth*), после чего происходит стрижка деревьев ([pruning](https://en.wikipedia.org/wiki/Decision_tree_pruning)).\n",
    "\n",
    "\n",
    "- [**LightGBM**](http://devdoc.net/bigdata/LightGBM-doc-2.2.2/index.html) - основан на двух эвристиках: $\\text{GOOS (Gradient-based One-Side Sampling)}$ и $\\text{ERF (Exclusive Feature Bundling)}$. Первая заключается в том, что перед построением очередного алгоритма (дерева) считается градиент на каждом объекте. После этого объекты сортируются по абсолютному значению расчитанного градиента от большего к меньшему. Затем для разбиения вершины дерева берется $N$ объектов с наибольшим значением градиента и какая - то часть от оставшихся наблюдений. Второе свойство заключается в склейке нескольких разреженных признаков в один. В результате признаков становится меньше, что увеличивает скорость разбиения вершины. Умеет работать с категориальными фичами, предварительно закодированными числовым типом. Для этого параметру *categorical_feature* нужно передать названия таких признаков.\n",
    "\n",
    "\n",
    "- [**CatBoost**](https://catboost.ai/) - это библиотека градиентного бустинга, созданная Яндексом. Использует симметричное построение деревьев, т.е. при разбиении узлов на одном уровне используются одинаковые условия. *CatBoost*  достаточно гибок при работе с категориальными признаками, позволяя использовать их без предварительной обработки. Более того, рекомендуется их не обрабатывать, так как это может повлиять на качество работы алгоритма. Для того, чтобы модель понимала какие признаки являются категориальными используется параметр *cat_features*, в который передаются индексы соответствующих столбцов. В отличие от *LightGBM* неважно каким типом данных представлен категориальный признак: числовым или текстовым. Глубина деревьев ограничена 16.\n",
    "\n",
    "Более подробно о реализации данных методов можно прочесть в документации по приведенным ссылкам. \n",
    "\n",
    "Названия основных гиперпараметров для *XGBoost* и *LightGBM* преимущественно совпадают. Для *CatBoost* могут немного отличаться.\n",
    " \n",
    "|**Интерпретация**                                 |**XGBoost**           |**LightGBM**                       |**Catboost** |\n",
    "|:------------------------------------------------:|:--------------------:|:---------------------------------:|:-----------:|\n",
    "|*Количество деревьев*                             |n_estimators          |n_estimators                       |iterations   |\n",
    "|*Максимальная глубина*                            |max_depth             |max_depth                          |depth        | \n",
    "|*Шаг (скорость) обучения*                         |learning_rate (eta)   |learning_rate (eta)                |learning_rate|\n",
    "|*Доля объектов для обучения одного дерева*        |subsample             |subsample (bagging_fraction)       |             |\n",
    "|*Доля признаков для обучения одного дерева*       |colsample_bytree      |colsample_bytree (feature_fraction)|rsm          |\n",
    "|*Минимальное количество наблюдений в листе дерева*|min_child_weight      |min_data_in_leaf                   |             |\n",
    "|*Минимальное уменьшение значения функции потерь*  |gamma (min_split_loss)|min_gain_to_split (min_split_gain) |             |\n",
    "|*L1 коэффициент регуляризации*                    |reg_alpha (alpha)     |reg_alpha (lambda_l1)              |             |\n",
    "|*L2 коэффициент регуляризации*                    |reg_lambda (lambda)   |reg_lambda (lambda_l2)             |reg_lambda   |\n",
    "|*Индексы категориальных признаков*                |                      |                                   |cat_features |\n",
    "|*Названия категориальных признаков*               |                      |categorical_feature                |             |\n",
    "\n",
    "\n",
    "Перед тем, как переходить к сравнению данных методов, опишем подробнее какие подходы существуют для отбора оптимальных значений гиперпараметров."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 8. Методы подбора гиперпараметров\n",
    "\n",
    "Выше мы уже начинали говорить о методах определения оптимальных значений гиперпараметров и рассмотрели один из возможных подходов. Обсудим подробнее этот и другие подходы, а также посмотрим какие преимущества и недостатки есть у каждого из них.\n",
    "\n",
    "- [**GridSearchCV**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html) - проходится по всем узлам сетки (декартово произведение заданных множеств гиперпараметров) и выбирает оптимальный узел в зависимости от заданной метрики качества.\n",
    "\n",
    "     **+** проход по всем узлам указанной сетки. Это гарантирует нахождение оптимального набора гиперпараметров на указанных подмножествах;\n",
    "     \n",
    "     **+** возможность параллельного вычисления.\n",
    "\n",
    "     **-** занимает очень много времени;\n",
    "\n",
    "     **-** не учитывает переобучение.\n",
    "\n",
    "\n",
    "- [**RandomizedSearchCV**](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) - устроен схожим образом, что и метод GridSearchCV, но, в данном случае, рассматриваются уже не все узлы сетки, а лишь их часть, отобранная случайным образом. Количество узлов задается параметром $\\text{n_iter}$ (по дефолту равен $10$).\n",
    "\n",
    "     **+** полезен, когда количество гиперпараметров велико и/или мощности указанных подмножеств для поиска оптимума большие.\n",
    "     \n",
    "     **+** возможность параллельного вычисления.\n",
    "\n",
    "     **-** существует вероятность пропустить оптимальную комбинацию настраиваемых параметров;\n",
    "\n",
    "     **-** несмотря на то, что данный метод гораздо быстрее вышеуказанного аналога, он так же может быть очень времязатратным;\n",
    "\n",
    "     **-** не учитывает переобучение.\n",
    "\n",
    "\n",
    "- **Последовательная настройка гиперпараметров** - заключается в последовательной фиксации всех гиперпараметров за исключением одного, подбора оптимального значения варьируемого параметра и переходу к исследованию следующего.\n",
    "\n",
    "     **+** время работы: количество комбинаций настраиваемых параметров, которые необходимо проверить, равно сумме размерностей множеств допустимых (заданных) значений гиперпараметров;\n",
    "\n",
    "     **+** как следствие из вышеуказанного преимущества - возможность рассматривать множества допустимых значений большей мощности.\n",
    "\n",
    "     **-**  небольшой набор проверяемых комбинаций гиперпараметров, тем самым, вероятность пропустить оптимальную комбинацию еще выше.\n",
    "\n",
    "\n",
    "- [**Генетический метод**](https://en.wikipedia.org/wiki/Genetic_algorithm) - идея алгоритма основана на эволюционном учении и триаде Ч. Дарвина: *наследственность, изменчивость и естественный отбор*. \n",
    "\n",
    "     В файле **\"GeneticBoost.py\"** реализован генетический алгоритм для решения задачи **классификации** и алгоритмов **XGBoost**, **LightGBM** и **CatBoost**. Суть метода заключается в следующих простых шагах:\n",
    "\n",
    "     1. Инициализация родителей \n",
    "     2. Оценка качества каждого родителя текущего поколения\n",
    "     3. Отбор лучших родителей из текущего поколения\n",
    "     4. Скрещивание лучших родителей текущего поколения - создание детей\n",
    "     5. Мутация детей\n",
    "     6. Получение нового поколения: лучшие родители текущего поколения + их дети\n",
    "     7. Переход к шагу 2 или завершение работы алгоритма\n",
    "\n",
    "   Ниже будет приведен пример использования данного алгоритма. \n",
    "     \n",
    "    **+** время работы: гораздо быстрее поиска по сетке;\n",
    "    \n",
    "    **+** в случае, если в качестве метрики выбран *roc_auc_score* - учитывает переобучение;\n",
    "\n",
    "    **-**  не гарантирует нахождения оптимума, во многом зависит от стартовых родителей.\n",
    "    \n",
    "    \n",
    "- [**Hyperopt**](https://github.com/hyperopt/hyperopt) - метод байесовской оптимизации гиперпараметров. На каждом шаге, проверяемая комбинация гиперпараметров определяется с учетом результатов, полученных на предыдущих шагах. Для этого решается задача поиска компромисса между *изучением* и *применением* (*explore-exploit dilemma*): выбрать точку из пространства параметров, где ожидаемое значение оптимизируемой функции оптимально (*применение*) или из пространства с большой неопределенностью (*изучение*). Оптимизируемая функция аппроксимируется посредством апостериорного гауссовского процесса.    \n",
    "\n",
    "\n",
    "\n",
    "\n",
    "|**Интерпретация**                                 |**Наиболее часто используемые значения**            |\n",
    "|:------------------------------------------------:|:--------------------------------------------------:|\n",
    "|*Количество деревьев*                             |$\\text{list(range(10,  121, 10))}$                  |\n",
    "|*Максимальная глубина*                            |$\\text{list(range(4,  10))}$                         | \n",
    "|*Шаг (скорость) обучения*                         |$\\text{[0.01, 0.03, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5]}$|\n",
    "|*Доля объектов для обучения одного дерева*        |$\\text{[0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]}$     |\n",
    "|*Доля признаков для обучения одного дерева*       |$\\text{[0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9, 1]}$     |\n",
    "|*Минимальное количество наблюдений в листе дерева*|$\\text{[1, 5, 10, 20, 30, 50, 100]}$                |\n",
    "|*Минимальное уменьшение значения функции потерь*  |$\\text{[0, 0.001, 0.005, 0.01, 0.02, 0.03, 0.05, 0.07, 0.1]}$|\n",
    "|*L1 коэффициент регуляризации*                    |$\\text{[0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5 ]}$     |\n",
    "|*L2 коэффициент регуляризации*                    |$\\text{[0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5 ]}$     |\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 9. Сравнение XGBoost, LightGBM и CatBoost"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Постановка задачи:**\n",
    "\n",
    "Предположим, что некоторый оператор сотовой связи решил поднять тариф, чтобы повысить свою прибыль. Поднимать тариф всем клиентам довольно рискованно, так как многие из них могут уйти к конкурентам. На основании исторических данных по повышению тарифа, необходимо определить группу клиентов, которые останутся после увеличения стоимости обслуживания. \n",
    "\n",
    "**Целевая переменная:**\n",
    "- $\\text{TARGET}$ - продолжит ли клиент пользоваться услугами связи в течение года после увеличения стоимости обслуживания.\n",
    "\n",
    "\n",
    "**Признаки:**\n",
    "- $\\text{AVG_PAYMENT_COMP_3_6}$ - сгруппированное некоторым образом отношение среднего платежа за последние 3 месяца к среднему платежу за предыдущие три (категориальный порядковый признак)\n",
    "\n",
    "\n",
    "- $\\text{SHARE_PAYMENT_MNT12(6, 3, 2)}$ - доля месяцев, в которые были совершены платежи среди последних 12 (6, 3, 2) \n",
    "\n",
    "\n",
    "- $\\text{AVG_PAYMENT_12}$ - среднемесячный платеж клиента, расчитанный по последнему году \n",
    "\n",
    "\n",
    "- $\\text{DURATION_INC}$ - во сколько раз увеличиваем продолжительность бесплатных минут разговора\n",
    "\n",
    "\n",
    "- $\\text{VALUE_INC}$ - на сколько увеличиваем стоимость тарифа\n",
    "\n",
    "\n",
    "- $\\text{VALUE_INC2}$ - отношение величины увеличения к текущей стоимости обслуживания\n",
    "\n",
    "\n",
    "- $\\text{SMS_INC}$ - на сколько % увеличится количество бесплатных SMS\n",
    "\n",
    "\n",
    "- $\\text{PAY_LAST_MNT}$ - наличие платежа за последние 30 дней (бинарный признак)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Загружаем данные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>TARGET</th>\n",
       "      <th>AVG_PAYMENT_COMP_3_6</th>\n",
       "      <th>PAY_LAST_MNT</th>\n",
       "      <th>SHARE_PAYMENT_MNT12</th>\n",
       "      <th>SHARE_PAYMENT_MNT6</th>\n",
       "      <th>SHARE_PAYMENT_MNT3</th>\n",
       "      <th>SHARE_PAYMENT_MNT2</th>\n",
       "      <th>AVG_PAYMENT_12</th>\n",
       "      <th>VALUE_INC</th>\n",
       "      <th>VALUE_INC2</th>\n",
       "      <th>SMS_INC</th>\n",
       "      <th>DURATION_INC</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0.250000</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2200.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.753769</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.683468</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.5</td>\n",
       "      <td>NaN</td>\n",
       "      <td>150.0</td>\n",
       "      <td>0.717703</td>\n",
       "      <td>10.0</td>\n",
       "      <td>0.737903</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>10</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>70.0</td>\n",
       "      <td>0.244727</td>\n",
       "      <td>13.0</td>\n",
       "      <td>0.552778</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>60.0</td>\n",
       "      <td>0.142857</td>\n",
       "      <td>12.4</td>\n",
       "      <td>0.900000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.0</td>\n",
       "      <td>4000.0</td>\n",
       "      <td>40.0</td>\n",
       "      <td>0.224490</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.610000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   TARGET  AVG_PAYMENT_COMP_3_6  PAY_LAST_MNT  SHARE_PAYMENT_MNT12  \\\n",
       "0       1                     2             0             0.250000   \n",
       "1       1                     8             0             0.416667   \n",
       "2       1                    10             1             1.000000   \n",
       "3       0                     1             0             0.166667   \n",
       "4       1                     9             1             1.000000   \n",
       "\n",
       "   SHARE_PAYMENT_MNT6  SHARE_PAYMENT_MNT3  SHARE_PAYMENT_MNT2  AVG_PAYMENT_12  \\\n",
       "0            0.166667            0.333333                 0.0          2200.0   \n",
       "1            0.333333            0.333333                 0.5             NaN   \n",
       "2            1.000000            1.000000                 1.0             NaN   \n",
       "3            0.166667            0.000000                 0.0          4000.0   \n",
       "4            1.000000            1.000000                 1.0          4000.0   \n",
       "\n",
       "   VALUE_INC  VALUE_INC2  SMS_INC  DURATION_INC  \n",
       "0      150.0    0.753769     10.0      0.683468  \n",
       "1      150.0    0.717703     10.0      0.737903  \n",
       "2       70.0    0.244727     13.0      0.552778  \n",
       "3       60.0    0.142857     12.4      0.900000  \n",
       "4       40.0    0.224490      7.0      0.610000  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "#Читаем данные и выводим первые пять строк на экран\n",
    "data = pd.read_csv('mobile.csv', sep = ',')\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 44200 entries, 0 to 44199\n",
      "Data columns (total 12 columns):\n",
      " #   Column                Non-Null Count  Dtype  \n",
      "---  ------                --------------  -----  \n",
      " 0   TARGET                44200 non-null  int64  \n",
      " 1   AVG_PAYMENT_COMP_3_6  44200 non-null  int64  \n",
      " 2   PAY_LAST_MNT          44200 non-null  int64  \n",
      " 3   SHARE_PAYMENT_MNT12   44200 non-null  float64\n",
      " 4   SHARE_PAYMENT_MNT6    44200 non-null  float64\n",
      " 5   SHARE_PAYMENT_MNT3    44200 non-null  float64\n",
      " 6   SHARE_PAYMENT_MNT2    44200 non-null  float64\n",
      " 7   AVG_PAYMENT_12        40546 non-null  float64\n",
      " 8   VALUE_INC             44200 non-null  float64\n",
      " 9   VALUE_INC2            44200 non-null  float64\n",
      " 10  SMS_INC               44200 non-null  float64\n",
      " 11  DURATION_INC          44200 non-null  float64\n",
      "dtypes: float64(9), int64(3)\n",
      "memory usage: 4.0 MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Заменим пропуски на медиану\n",
    "data['AVG_PAYMENT_12'].fillna(round(data['AVG_PAYMENT_12'].median()), inplace = True)\n",
    "\n",
    "X = data.drop(['TARGET'], 1)\n",
    "Y = data['TARGET']\n",
    "#Разделим выборку на две части \n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size = 0.3, random_state = 123)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### XGBoost\n",
    "Инициализация параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "numberOfParents = 10 # Количество родителей на старте\n",
    "numberOfGenerations = 10 # Количество поколений, которое будет создано\n",
    "numberOfParentsMating = 4 # Количество родителей, которые останутся после каждого этапа эволюции\n",
    "numberOfChildren = 6 # Количество детей от каждого поколения"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Метрика, по которой будем выбирать лучший набор гиперпараметров\n",
    "metric = 'roc_auc_adj'\n",
    "\n",
    "# Список метрик, на которые хотом смотреть (также можно добавить 'f1' и 'accuracy')\n",
    "metricList = ['roc_auc', 'recall', 'precision']\n",
    "\n",
    "# коэффициент регуляризации: roc_auc_adj = roc_auc(test) - overfitting / coef_adj\n",
    "coef_adj = 10.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# набор гиперпараметов, которые будем подбирать\n",
    "# каждому гиперпараметру соответствует 3 числа: мин. и макс. значение и шаг\n",
    "hyperparameterSettings = {'n_estimators': [10, 121, 5],\n",
    "                          'max_depth': [3, 11, 1],\n",
    "                          'learning_rate': [0.001, 0.3002, 0.002],\n",
    "                          'colsample_bytree': [0.4, 1.0, 0.1],\n",
    "                          'subsample': [0.4, 1.0, 0.1],\n",
    "                          'reg_lambda': [0.05, 0.35, 0.05]}\n",
    "\n",
    "# Словарь поколений: \n",
    "# ключ - номер поколения, \n",
    "# элемент - словарь, ключи которого - имена родителей/детей, а значения - набор гиперпараметров в лексикографическом порядке\n",
    "population = {}\n",
    "\n",
    "# Словарь метрик поколений: \n",
    "# ключ - номер поколения, \n",
    "# элемент - словарь, ключи которого - имена родителей/детей, а значения - результаты метрик в лексикографическом порядке\n",
    "scorePopulation = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Инициализация генетического алгоритма"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            colsample_bytree  learning_rate  max_depth  n_estimators  \\\n",
      "Parents 1                0.7          0.003        8.0          35.0   \n",
      "Parents 2                0.9          0.183        9.0         100.0   \n",
      "Parents 3                0.9          0.299        8.0          70.0   \n",
      "Parents 4                0.8          0.183        5.0          50.0   \n",
      "Parents 5                0.4          0.109        4.0         105.0   \n",
      "Parents 6                0.5          0.149        4.0          20.0   \n",
      "Parents 7                0.8          0.037        9.0          55.0   \n",
      "Parents 8                0.9          0.145        6.0          20.0   \n",
      "Parents 9                0.6          0.277        4.0          60.0   \n",
      "Parents 10               0.7          0.027        9.0          70.0   \n",
      "\n",
      "            reg_lambda  subsample  \n",
      "Parents 1         0.25        0.8  \n",
      "Parents 2         0.05        0.9  \n",
      "Parents 3         0.30        0.5  \n",
      "Parents 4         0.15        0.8  \n",
      "Parents 5         0.05        0.4  \n",
      "Parents 6         0.20        0.9  \n",
      "Parents 7         0.30        0.7  \n",
      "Parents 8         0.20        0.8  \n",
      "Parents 9         0.05        0.8  \n",
      "Parents 10        0.10        0.5  \n"
     ]
    }
   ],
   "source": [
    "import GeneticBoost as gboost\n",
    "import random\n",
    "\n",
    "# Водные данные:\n",
    "# Количество родителей и набор гиперпараметров\n",
    "random.seed(237)\n",
    "population['Gender 0'] = gboost.initilialize_poplulation(numberOfParents, hyperparameterSettings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отбор лучших родителей, построение нового поколения "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  This is number 0 generation\n",
      "\n",
      "           Train precision Train recall Train roc_auc Train roc_auc_adj  \\\n",
      "Parents 1         0.826058     0.891441      0.874847          0.873143   \n",
      "Parents 10        0.833452     0.893419      0.889641          0.886715   \n",
      "Parents 2         0.885955     0.910303      0.944085          0.934445   \n",
      "Parents 3         0.868755     0.895802      0.923602          0.915748   \n",
      "Parents 4         0.824924      0.89783      0.881639          0.879666   \n",
      "Parents 5         0.822128     0.895954        0.8755          0.874115   \n",
      "Parents 6         0.816942     0.886573      0.860834          0.859889   \n",
      "Parents 7         0.831357     0.899351      0.892036          0.888897   \n",
      "Parents 8         0.824446     0.892962       0.87635          0.874808   \n",
      "Parents 9         0.824371     0.897019      0.880119          0.878218   \n",
      "\n",
      "           Test precision Test recall Test roc_auc Test roc_auc_adj  \n",
      "Parents 1        0.820899    0.883201     0.857807         0.856103  \n",
      "Parents 10       0.820998     0.87729      0.86038         0.857454  \n",
      "Parents 2        0.822518     0.85412     0.847685         0.838045  \n",
      "Parents 3        0.822566    0.848918      0.84506         0.837206  \n",
      "Parents 4        0.817244    0.889703     0.861905         0.859931  \n",
      "Parents 5        0.817313    0.889585     0.861646         0.860261  \n",
      "Parents 6        0.814042     0.88131     0.851378         0.850433  \n",
      "Parents 7        0.818401    0.882256     0.860651         0.857513  \n",
      "Parents 8        0.818351    0.885684     0.860927         0.859384  \n",
      "Parents 9         0.81656    0.887221     0.861101         0.859199  \n",
      "\n",
      "Best parents in generation 0: Parents 5, Parents 4, Parents 8, Parents 9, \n",
      "\n",
      "New generation: \n",
      "                  colsample_bytree learning_rate max_depth n_estimators  \\\n",
      "Children 1 (gen0)              0.6         0.275         4           60   \n",
      "Children 2 (gen0)              0.6         0.183         8           60   \n",
      "Children 3 (gen0)              0.6         0.109         3          105   \n",
      "Children 4 (gen0)              0.9         0.145         4          100   \n",
      "Children 5 (gen0)              0.6         0.141         4           40   \n",
      "Children 6 (gen0)              0.8         0.183         4           50   \n",
      "Parents 4                      0.8         0.183         5           50   \n",
      "Parents 5                      0.4         0.109         4          105   \n",
      "Parents 8                      0.9         0.145         6           20   \n",
      "Parents 9                      0.6         0.277         4           60   \n",
      "\n",
      "                  reg_lambda subsample  \n",
      "Children 1 (gen0)       0.15       0.8  \n",
      "Children 2 (gen0)       0.15       0.4  \n",
      "Children 3 (gen0)       0.05       0.4  \n",
      "Children 4 (gen0)        0.2       0.5  \n",
      "Children 5 (gen0)       0.05       0.4  \n",
      "Children 6 (gen0)        0.2       0.5  \n",
      "Parents 4               0.15       0.8  \n",
      "Parents 5               0.05       0.4  \n",
      "Parents 8                0.2       0.8  \n",
      "Parents 9               0.05       0.8  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                  This is number 1 generation\n",
      "\n",
      "                  Train precision Train recall Train roc_auc  \\\n",
      "Children 1 (gen0)        0.823362     0.896004      0.880153   \n",
      "Children 2 (gen0)        0.842341     0.894534      0.901673   \n",
      "Children 3 (gen0)        0.820165      0.89423      0.871666   \n",
      "Children 4 (gen0)        0.826775     0.890579      0.880375   \n",
      "Children 5 (gen0)        0.821803      0.89139      0.869767   \n",
      "Children 6 (gen0)        0.821911     0.893926      0.875701   \n",
      "Parents 4                0.824924      0.89783      0.881639   \n",
      "Parents 5                0.822128     0.895954        0.8755   \n",
      "Parents 8                0.824446     0.892962       0.87635   \n",
      "Parents 9                0.824371     0.897019      0.880119   \n",
      "\n",
      "                  Train roc_auc_adj Test precision Test recall Test roc_auc  \\\n",
      "Children 1 (gen0)          0.878239       0.817467    0.885211     0.861016   \n",
      "Children 2 (gen0)          0.897087       0.821771    0.871025     0.855807   \n",
      "Children 3 (gen0)          0.870593       0.816889    0.889703     0.860931   \n",
      "Children 4 (gen0)          0.878471       0.819886    0.884147     0.861336   \n",
      "Children 5 (gen0)          0.868595       0.818588    0.887102     0.858047   \n",
      "Children 6 (gen0)          0.874215       0.817175    0.889821      0.86084   \n",
      "Parents 4                  0.879666       0.817244    0.889703     0.861905   \n",
      "Parents 5                  0.874115       0.817313    0.889585     0.861646   \n",
      "Parents 8                  0.874808       0.818351    0.885684     0.860927   \n",
      "Parents 9                  0.878218        0.81656    0.887221     0.861101   \n",
      "\n",
      "                  Test roc_auc_adj  \n",
      "Children 1 (gen0)         0.859102  \n",
      "Children 2 (gen0)         0.851221  \n",
      "Children 3 (gen0)         0.859858  \n",
      "Children 4 (gen0)         0.859432  \n",
      "Children 5 (gen0)         0.856875  \n",
      "Children 6 (gen0)         0.859353  \n",
      "Parents 4                 0.859931  \n",
      "Parents 5                 0.860261  \n",
      "Parents 8                 0.859384  \n",
      "Parents 9                 0.859199  \n",
      "\n",
      "Best parents in generation 1: Parents 5, Parents 4, Children 3 (gen0), Children 4 (gen0), \n",
      "\n",
      "New generation: \n",
      "                  colsample_bytree learning_rate max_depth n_estimators  \\\n",
      "Children 1 (gen1)              0.9         0.183         5          100   \n",
      "Children 2 (gen1)              0.4         0.145         4           25   \n",
      "Children 3 (gen0)              0.6         0.109         3          105   \n",
      "Children 3 (gen1)              0.9         0.109         4           80   \n",
      "Children 4 (gen0)              0.9         0.145         4          100   \n",
      "Children 4 (gen1)              0.7         0.183         5           75   \n",
      "Children 5 (gen1)              0.4         0.151         5          105   \n",
      "Children 6 (gen1)              0.6         0.249         3          105   \n",
      "Parents 4                      0.8         0.183         5           50   \n",
      "Parents 5                      0.4         0.109         4          105   \n",
      "\n",
      "                  reg_lambda subsample  \n",
      "Children 1 (gen1)        0.2       0.7  \n",
      "Children 2 (gen1)       0.05       0.9  \n",
      "Children 3 (gen0)       0.05       0.4  \n",
      "Children 3 (gen1)       0.05       0.5  \n",
      "Children 4 (gen0)        0.2       0.5  \n",
      "Children 4 (gen1)       0.15       0.4  \n",
      "Children 5 (gen1)       0.15       0.8  \n",
      "Children 6 (gen1)       0.05       0.5  \n",
      "Parents 4               0.15       0.8  \n",
      "Parents 5               0.05       0.4  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                  This is number 2 generation\n",
      "\n",
      "                  Train precision Train recall Train roc_auc  \\\n",
      "Children 1 (gen1)        0.833907     0.896106      0.892368   \n",
      "Children 2 (gen1)        0.816509     0.888754      0.862179   \n",
      "Children 3 (gen0)        0.820165      0.89423      0.871666   \n",
      "Children 3 (gen1)        0.822229     0.896106      0.876059   \n",
      "Children 4 (gen0)        0.826775     0.890579      0.880375   \n",
      "Children 4 (gen1)        0.826929     0.895903      0.883975   \n",
      "Children 5 (gen1)        0.826861      0.89499      0.884643   \n",
      "Children 6 (gen1)        0.823238     0.894534      0.877091   \n",
      "Parents 4                0.824924      0.89783      0.881639   \n",
      "Parents 5                0.822128     0.895954        0.8755   \n",
      "\n",
      "                  Train roc_auc_adj Test precision Test recall Test roc_auc  \\\n",
      "Children 1 (gen1)          0.889053       0.821681    0.878118     0.859217   \n",
      "Children 2 (gen1)          0.861193       0.814815    0.884265     0.852323   \n",
      "Children 3 (gen0)          0.870593       0.816889    0.889703     0.860931   \n",
      "Children 3 (gen1)          0.874605       0.818113    0.890649     0.861518   \n",
      "Children 4 (gen0)          0.878471       0.819886    0.884147     0.861336   \n",
      "Children 4 (gen1)          0.881471       0.816658    0.886748     0.858942   \n",
      "Children 5 (gen1)           0.88235       0.819541    0.887457     0.861719   \n",
      "Children 6 (gen1)          0.875504        0.81833    0.887694     0.861223   \n",
      "Parents 4                  0.879666       0.817244    0.889703     0.861905   \n",
      "Parents 5                  0.874115       0.817313    0.889585     0.861646   \n",
      "\n",
      "                  Test roc_auc_adj  \n",
      "Children 1 (gen1)         0.855901  \n",
      "Children 2 (gen1)         0.851337  \n",
      "Children 3 (gen0)         0.859858  \n",
      "Children 3 (gen1)         0.860064  \n",
      "Children 4 (gen0)         0.859432  \n",
      "Children 4 (gen1)         0.856439  \n",
      "Children 5 (gen1)         0.859427  \n",
      "Children 6 (gen1)         0.859636  \n",
      "Parents 4                 0.859931  \n",
      "Parents 5                 0.860261  \n",
      "\n",
      "Best parents in generation 2: Parents 5, Children 3 (gen1), Parents 4, Children 3 (gen0), \n",
      "\n",
      "New generation: \n",
      "                  colsample_bytree learning_rate max_depth n_estimators  \\\n",
      "Children 1 (gen2)              0.4         0.109         5          105   \n",
      "Children 2 (gen2)              0.8         0.183         5           45   \n",
      "Children 3 (gen0)              0.6         0.109         3          105   \n",
      "Children 3 (gen1)              0.9         0.109         4           80   \n",
      "Children 3 (gen2)              0.8         0.183         4           75   \n",
      "Children 4 (gen2)              0.9         0.279         4           80   \n",
      "Children 5 (gen2)              0.8         0.109        10           50   \n",
      "Children 6 (gen2)              0.4         0.109         4           95   \n",
      "Parents 4                      0.8         0.183         5           50   \n",
      "Parents 5                      0.4         0.109         4          105   \n",
      "\n",
      "                  reg_lambda subsample  \n",
      "Children 1 (gen2)        0.2       0.4  \n",
      "Children 2 (gen2)       0.05       0.8  \n",
      "Children 3 (gen0)       0.05       0.4  \n",
      "Children 3 (gen1)       0.05       0.5  \n",
      "Children 3 (gen2)       0.05       0.6  \n",
      "Children 4 (gen2)       0.05       0.6  \n",
      "Children 5 (gen2)        0.1       0.8  \n",
      "Children 6 (gen2)       0.05       0.8  \n",
      "Parents 4               0.15       0.8  \n",
      "Parents 5               0.05       0.4  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                  This is number 3 generation\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Train precision Train recall Train roc_auc  \\\n",
      "Children 1 (gen2)        0.823592     0.896004       0.88057   \n",
      "Children 2 (gen2)        0.824103     0.896309      0.880792   \n",
      "Children 3 (gen0)        0.820165      0.89423      0.871666   \n",
      "Children 3 (gen1)        0.822229     0.896106      0.876059   \n",
      "Children 3 (gen2)        0.825228     0.893976      0.879062   \n",
      "Children 4 (gen2)        0.827792     0.892557      0.883457   \n",
      "Children 5 (gen2)        0.852127      0.90579      0.918313   \n",
      "Children 6 (gen2)        0.821043     0.895396      0.875245   \n",
      "Parents 4                0.824924      0.89783      0.881639   \n",
      "Parents 5                0.822128     0.895954        0.8755   \n",
      "\n",
      "                  Train roc_auc_adj Test precision Test recall Test roc_auc  \\\n",
      "Children 1 (gen2)          0.878695       0.817608    0.888166     0.861819   \n",
      "Children 2 (gen2)          0.878947       0.815564    0.887102      0.86234   \n",
      "Children 3 (gen0)          0.870593       0.816889    0.889703     0.860931   \n",
      "Children 3 (gen1)          0.874605       0.818113    0.890649     0.861518   \n",
      "Children 3 (gen2)          0.877307       0.818172    0.885684     0.861515   \n",
      "Children 4 (gen2)          0.881156       0.820659    0.882847     0.860443   \n",
      "Children 5 (gen2)          0.912191       0.820111    0.872562     0.857093   \n",
      "Children 6 (gen2)          0.873897       0.815701    0.890531     0.861765   \n",
      "Parents 4                  0.879666       0.817244    0.889703     0.861905   \n",
      "Parents 5                  0.874115       0.817313    0.889585     0.861646   \n",
      "\n",
      "                  Test roc_auc_adj  \n",
      "Children 1 (gen2)         0.859944  \n",
      "Children 2 (gen2)         0.860495  \n",
      "Children 3 (gen0)         0.859858  \n",
      "Children 3 (gen1)         0.860064  \n",
      "Children 3 (gen2)          0.85976  \n",
      "Children 4 (gen2)         0.858142  \n",
      "Children 5 (gen2)          0.85097  \n",
      "Children 6 (gen2)         0.860418  \n",
      "Parents 4                 0.859931  \n",
      "Parents 5                 0.860261  \n",
      "\n",
      "Best parents in generation 3: Children 2 (gen2), Children 6 (gen2), Parents 5, Children 3 (gen1), \n",
      "\n",
      "New generation: \n",
      "                  colsample_bytree learning_rate max_depth n_estimators  \\\n",
      "Children 1 (gen3)              0.9         0.183         5           65   \n",
      "Children 2 (gen2)              0.8         0.183         5           45   \n",
      "Children 2 (gen3)              0.4         0.109         5          105   \n",
      "Children 3 (gen1)              0.9         0.109         4           80   \n",
      "Children 3 (gen3)              0.4         0.109         9           45   \n",
      "Children 4 (gen3)              0.9         0.135         5           45   \n",
      "Children 5 (gen3)              0.9         0.239         4          105   \n",
      "Children 6 (gen2)              0.4         0.109         4           95   \n",
      "Children 6 (gen3)              0.9         0.109         4           95   \n",
      "Parents 5                      0.4         0.109         4          105   \n",
      "\n",
      "                  reg_lambda subsample  \n",
      "Children 1 (gen3)       0.05       0.6  \n",
      "Children 2 (gen2)       0.05       0.8  \n",
      "Children 2 (gen3)       0.05       0.5  \n",
      "Children 3 (gen1)       0.05       0.5  \n",
      "Children 3 (gen3)       0.05       0.8  \n",
      "Children 4 (gen3)       0.05       0.8  \n",
      "Children 5 (gen3)       0.05       0.4  \n",
      "Children 6 (gen2)       0.05       0.8  \n",
      "Children 6 (gen3)       0.05       0.5  \n",
      "Parents 5               0.05       0.4  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                  This is number 4 generation\n",
      "\n",
      "                  Train precision Train recall Train roc_auc  \\\n",
      "Children 1 (gen3)        0.830964       0.8921      0.885553   \n",
      "Children 2 (gen2)        0.824103     0.896309      0.880792   \n",
      "Children 2 (gen3)        0.823581      0.89641      0.880885   \n",
      "Children 3 (gen1)        0.822229     0.896106      0.876059   \n",
      "Children 3 (gen3)        0.831769     0.898743      0.893638   \n",
      "Children 4 (gen3)        0.822857     0.894788      0.878579   \n",
      "Children 5 (gen3)         0.83023      0.89068      0.883252   \n",
      "Children 6 (gen2)        0.821043     0.895396      0.875245   \n",
      "Children 6 (gen3)        0.822701     0.894534      0.877736   \n",
      "Parents 5                0.822128     0.895954        0.8755   \n",
      "\n",
      "                  Train roc_auc_adj Test precision Test recall Test roc_auc  \\\n",
      "Children 1 (gen3)          0.883024       0.820425    0.880364     0.860259   \n",
      "Children 2 (gen2)          0.878947       0.815564    0.887102      0.86234   \n",
      "Children 2 (gen3)          0.878979       0.818291    0.889585     0.861826   \n",
      "Children 3 (gen1)          0.874605       0.818113    0.890649     0.861518   \n",
      "Children 3 (gen3)          0.890202       0.818112    0.882137     0.859282   \n",
      "Children 4 (gen3)          0.876949       0.817667    0.887457     0.862276   \n",
      "Children 5 (gen3)          0.880795         0.8207      0.8793     0.858685   \n",
      "Children 6 (gen2)          0.873897       0.815701    0.890531     0.861765   \n",
      "Children 6 (gen3)          0.876126       0.818469    0.888521     0.861629   \n",
      "Parents 5                  0.874115       0.817313    0.889585     0.861646   \n",
      "\n",
      "                  Test roc_auc_adj  \n",
      "Children 1 (gen3)         0.857729  \n",
      "Children 2 (gen2)         0.860495  \n",
      "Children 2 (gen3)          0.85992  \n",
      "Children 3 (gen1)         0.860064  \n",
      "Children 3 (gen3)         0.855846  \n",
      "Children 4 (gen3)         0.860646  \n",
      "Children 5 (gen3)         0.856228  \n",
      "Children 6 (gen2)         0.860418  \n",
      "Children 6 (gen3)         0.860018  \n",
      "Parents 5                 0.860261  \n",
      "\n",
      "Best parents in generation 4: Children 4 (gen3), Children 2 (gen2), Children 6 (gen2), Parents 5, \n",
      "\n",
      "New generation: \n",
      "                  colsample_bytree learning_rate max_depth n_estimators  \\\n",
      "Children 1 (gen4)              0.4         0.109         5           70   \n",
      "Children 2 (gen2)              0.8         0.183         5           45   \n",
      "Children 2 (gen4)              0.9         0.109         4           95   \n",
      "Children 3 (gen4)              0.5         0.109         4           15   \n",
      "Children 4 (gen3)              0.9         0.135         5           45   \n",
      "Children 4 (gen4)              0.4         0.109        10           45   \n",
      "Children 5 (gen4)              0.8         0.109         4           35   \n",
      "Children 6 (gen2)              0.4         0.109         4           95   \n",
      "Children 6 (gen4)              0.8         0.109         5           95   \n",
      "Parents 5                      0.4         0.109         4          105   \n",
      "\n",
      "                  reg_lambda subsample  \n",
      "Children 1 (gen4)        0.1       0.8  \n",
      "Children 2 (gen2)       0.05       0.8  \n",
      "Children 2 (gen4)        0.1       0.4  \n",
      "Children 3 (gen4)       0.05       0.8  \n",
      "Children 4 (gen3)       0.05       0.8  \n",
      "Children 4 (gen4)       0.05       0.8  \n",
      "Children 5 (gen4)       0.05       0.4  \n",
      "Children 6 (gen2)       0.05       0.8  \n",
      "Children 6 (gen4)       0.15       0.8  \n",
      "Parents 5               0.05       0.4  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                  This is number 5 generation\n",
      "\n",
      "                  Train precision Train recall Train roc_auc  \\\n",
      "Children 1 (gen4)        0.819497     0.898945      0.876706   \n",
      "Children 2 (gen2)        0.824103     0.896309      0.880792   \n",
      "Children 2 (gen4)        0.825966      0.89063      0.877099   \n",
      "Children 3 (gen4)        0.813702     0.877447      0.852089   \n",
      "Children 4 (gen3)        0.822857     0.894788      0.878579   \n",
      "Children 4 (gen4)        0.836776     0.899655      0.899642   \n",
      "Children 5 (gen4)        0.819939     0.890326      0.866777   \n",
      "Children 6 (gen2)        0.821043     0.895396      0.875245   \n",
      "Children 6 (gen4)        0.824512     0.898134      0.883318   \n",
      "Parents 5                0.822128     0.895954        0.8755   \n",
      "\n",
      "                  Train roc_auc_adj Test precision Test recall Test roc_auc  \\\n",
      "Children 1 (gen4)          0.875254       0.814655    0.893723     0.862187   \n",
      "Children 2 (gen2)          0.878947       0.815564    0.887102      0.86234   \n",
      "Children 2 (gen4)          0.875555       0.819507    0.884029     0.861651   \n",
      "Children 3 (gen4)          0.851065       0.813732    0.868661     0.841849   \n",
      "Children 4 (gen3)          0.876949       0.817667    0.887457     0.862276   \n",
      "Children 4 (gen4)          0.895551       0.820584    0.877527     0.858729   \n",
      "Children 5 (gen4)          0.865719       0.817726    0.886748     0.856192   \n",
      "Children 6 (gen2)          0.873897       0.815701    0.890531     0.861765   \n",
      "Children 6 (gen4)          0.881187       0.817807    0.890413     0.862009   \n",
      "Parents 5                  0.874115       0.817313    0.889585     0.861646   \n",
      "\n",
      "                  Test roc_auc_adj  \n",
      "Children 1 (gen4)         0.860735  \n",
      "Children 2 (gen2)         0.860495  \n",
      "Children 2 (gen4)         0.860107  \n",
      "Children 3 (gen4)         0.840825  \n",
      "Children 4 (gen3)         0.860646  \n",
      "Children 4 (gen4)         0.854637  \n",
      "Children 5 (gen4)         0.855134  \n",
      "Children 6 (gen2)         0.860418  \n",
      "Children 6 (gen4)         0.859878  \n",
      "Parents 5                 0.860261  \n",
      "\n",
      "Best parents in generation 5: Children 1 (gen4), Children 4 (gen3), Children 2 (gen2), Children 6 (gen2), \n",
      "\n",
      "New generation: \n",
      "                  colsample_bytree learning_rate max_depth n_estimators  \\\n",
      "Children 1 (gen4)              0.4         0.109         5           70   \n",
      "Children 1 (gen5)              0.4         0.221         5           70   \n",
      "Children 2 (gen2)              0.8         0.183         5           45   \n",
      "Children 2 (gen5)              0.5         0.183         9           70   \n",
      "Children 3 (gen5)              0.8         0.005         5           75   \n",
      "Children 4 (gen3)              0.9         0.135         5           45   \n",
      "Children 4 (gen5)              0.4         0.109         4           45   \n",
      "Children 5 (gen5)              0.4         0.183         5           45   \n",
      "Children 6 (gen2)              0.4         0.109         4           95   \n",
      "Children 6 (gen5)              0.4         0.237         5           95   \n",
      "\n",
      "                  reg_lambda subsample  \n",
      "Children 1 (gen4)        0.1       0.8  \n",
      "Children 1 (gen5)       0.05       0.9  \n",
      "Children 2 (gen2)       0.05       0.8  \n",
      "Children 2 (gen5)       0.05       0.8  \n",
      "Children 3 (gen5)       0.05       0.8  \n",
      "Children 4 (gen3)       0.05       0.8  \n",
      "Children 4 (gen5)       0.05       0.7  \n",
      "Children 5 (gen5)       0.05       0.8  \n",
      "Children 6 (gen2)       0.05       0.8  \n",
      "Children 6 (gen5)        0.1       0.8  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                  This is number 6 generation\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Train precision Train recall Train roc_auc  \\\n",
      "Children 1 (gen4)        0.819497     0.898945      0.876706   \n",
      "Children 1 (gen5)        0.825967      0.89641      0.884118   \n",
      "Children 2 (gen2)        0.824103     0.896309      0.880792   \n",
      "Children 2 (gen5)        0.855823     0.901734      0.919015   \n",
      "Children 3 (gen5)        0.817812     0.877649      0.860151   \n",
      "Children 4 (gen3)        0.822857     0.894788      0.878579   \n",
      "Children 4 (gen5)        0.820676     0.894788      0.868242   \n",
      "Children 5 (gen5)        0.822126      0.89641      0.877209   \n",
      "Children 6 (gen2)        0.821043     0.895396      0.875245   \n",
      "Children 6 (gen5)        0.831069     0.893266      0.889287   \n",
      "\n",
      "                  Train roc_auc_adj Test precision Test recall Test roc_auc  \\\n",
      "Children 1 (gen4)          0.875254       0.814655    0.893723     0.862187   \n",
      "Children 1 (gen5)          0.881795       0.818866    0.887694     0.860885   \n",
      "Children 2 (gen2)          0.878947       0.815564    0.887102      0.86234   \n",
      "Children 2 (gen5)          0.912584       0.820181    0.868661     0.854704   \n",
      "Children 3 (gen5)          0.859096       0.817414    0.869015     0.849597   \n",
      "Children 4 (gen3)          0.876949       0.817667    0.887457     0.862276   \n",
      "Children 4 (gen5)           0.86724       0.816023    0.889821     0.858223   \n",
      "Children 5 (gen5)          0.875752       0.817135    0.889585     0.862641   \n",
      "Children 6 (gen2)          0.873897       0.815701    0.890531     0.861765   \n",
      "Children 6 (gen5)          0.886333       0.821393    0.879655     0.859747   \n",
      "\n",
      "                  Test roc_auc_adj  \n",
      "Children 1 (gen4)         0.860735  \n",
      "Children 1 (gen5)         0.858561  \n",
      "Children 2 (gen2)         0.860495  \n",
      "Children 2 (gen5)         0.848273  \n",
      "Children 3 (gen5)         0.848542  \n",
      "Children 4 (gen3)         0.860646  \n",
      "Children 4 (gen5)         0.857221  \n",
      "Children 5 (gen5)         0.861184  \n",
      "Children 6 (gen2)         0.860418  \n",
      "Children 6 (gen5)         0.856793  \n",
      "\n",
      "Best parents in generation 6: Children 5 (gen5), Children 1 (gen4), Children 4 (gen3), Children 2 (gen2), \n",
      "\n",
      "New generation: \n",
      "                  colsample_bytree learning_rate max_depth n_estimators  \\\n",
      "Children 1 (gen4)              0.4         0.109         5           70   \n",
      "Children 1 (gen6)              0.6         0.183         8           45   \n",
      "Children 2 (gen2)              0.8         0.183         5           45   \n",
      "Children 2 (gen6)              0.4         0.183         5           40   \n",
      "Children 3 (gen6)              0.9         0.183         5          100   \n",
      "Children 4 (gen3)              0.9         0.135         5           45   \n",
      "Children 4 (gen6)              0.9         0.137         5           45   \n",
      "Children 5 (gen5)              0.4         0.183         5           45   \n",
      "Children 5 (gen6)              0.8         0.273         9           45   \n",
      "Children 6 (gen6)              0.9         0.135         5           45   \n",
      "\n",
      "                  reg_lambda subsample  \n",
      "Children 1 (gen4)        0.1       0.8  \n",
      "Children 1 (gen6)       0.05       0.8  \n",
      "Children 2 (gen2)       0.05       0.8  \n",
      "Children 2 (gen6)       0.05       0.5  \n",
      "Children 3 (gen6)       0.05       0.5  \n",
      "Children 4 (gen3)       0.05       0.8  \n",
      "Children 4 (gen6)        0.1       0.8  \n",
      "Children 5 (gen5)       0.05       0.8  \n",
      "Children 5 (gen6)       0.05       0.8  \n",
      "Children 6 (gen6)       0.05       0.7  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                  This is number 7 generation\n",
      "\n",
      "                  Train precision Train recall Train roc_auc  \\\n",
      "Children 1 (gen4)        0.819497     0.898945      0.876706   \n",
      "Children 1 (gen6)         0.83914     0.898793      0.901013   \n",
      "Children 2 (gen2)        0.824103     0.896309      0.880792   \n",
      "Children 2 (gen6)        0.821819     0.896866      0.875838   \n",
      "Children 3 (gen6)          0.8314     0.897627      0.891093   \n",
      "Children 4 (gen3)        0.822857     0.894788      0.878579   \n",
      "Children 4 (gen6)        0.821626     0.896157      0.878113   \n",
      "Children 5 (gen5)        0.822126      0.89641      0.877209   \n",
      "Children 5 (gen6)         0.86643     0.904168       0.92702   \n",
      "Children 6 (gen6)        0.824204     0.893368      0.878169   \n",
      "\n",
      "                  Train roc_auc_adj Test precision Test recall Test roc_auc  \\\n",
      "Children 1 (gen4)          0.875254       0.814655    0.893723     0.862187   \n",
      "Children 1 (gen6)          0.896773       0.821318    0.878118     0.858613   \n",
      "Children 2 (gen2)          0.878947       0.815564    0.887102      0.86234   \n",
      "Children 2 (gen6)           0.87437       0.817049    0.891713     0.861157   \n",
      "Children 3 (gen6)          0.887838       0.817169    0.884502      0.85854   \n",
      "Children 4 (gen3)          0.876949       0.817667    0.887457     0.862276   \n",
      "Children 4 (gen6)          0.876514       0.818419    0.889821     0.862127   \n",
      "Children 5 (gen5)          0.875752       0.817135    0.889585     0.862641   \n",
      "Children 5 (gen6)          0.919435       0.821497    0.862868     0.851171   \n",
      "Children 6 (gen6)          0.876542       0.818549    0.886866     0.861897   \n",
      "\n",
      "                  Test roc_auc_adj  \n",
      "Children 1 (gen4)         0.860735  \n",
      "Children 1 (gen6)         0.854373  \n",
      "Children 2 (gen2)         0.860495  \n",
      "Children 2 (gen6)         0.859689  \n",
      "Children 3 (gen6)         0.855285  \n",
      "Children 4 (gen3)         0.860646  \n",
      "Children 4 (gen6)         0.860528  \n",
      "Children 5 (gen5)         0.861184  \n",
      "Children 5 (gen6)         0.843586  \n",
      "Children 6 (gen6)         0.860269  \n",
      "\n",
      "Best parents in generation 7: Children 5 (gen5), Children 1 (gen4), Children 4 (gen3), Children 4 (gen6), \n",
      "\n",
      "New generation: \n",
      "                  colsample_bytree learning_rate max_depth n_estimators  \\\n",
      "Children 1 (gen4)              0.4         0.109         5           70   \n",
      "Children 1 (gen7)              0.8         0.173         5           45   \n",
      "Children 2 (gen7)              0.4         0.109         5           85   \n",
      "Children 3 (gen7)              0.4         0.183         5          100   \n",
      "Children 4 (gen3)              0.9         0.135         5           45   \n",
      "Children 4 (gen6)              0.9         0.137         5           45   \n",
      "Children 4 (gen7)              0.9         0.137         5           70   \n",
      "Children 5 (gen5)              0.4         0.183         5           45   \n",
      "Children 5 (gen7)              0.4         0.135         6           45   \n",
      "Children 6 (gen7)              0.9         0.265         5           45   \n",
      "\n",
      "                  reg_lambda subsample  \n",
      "Children 1 (gen4)        0.1       0.8  \n",
      "Children 1 (gen7)       0.05       0.8  \n",
      "Children 2 (gen7)        0.1       0.5  \n",
      "Children 3 (gen7)        0.1       0.5  \n",
      "Children 4 (gen3)       0.05       0.8  \n",
      "Children 4 (gen6)        0.1       0.8  \n",
      "Children 4 (gen7)        0.3       0.5  \n",
      "Children 5 (gen5)       0.05       0.8  \n",
      "Children 5 (gen7)       0.25       0.8  \n",
      "Children 6 (gen7)        0.1       0.8  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                  This is number 8 generation\n",
      "\n",
      "                  Train precision Train recall Train roc_auc  \\\n",
      "Children 1 (gen4)        0.819497     0.898945      0.876706   \n",
      "Children 1 (gen7)        0.825925     0.890376      0.880568   \n",
      "Children 2 (gen7)        0.823483     0.896512      0.878329   \n",
      "Children 3 (gen7)        0.828103     0.896461      0.885003   \n",
      "Children 4 (gen3)        0.822857     0.894788      0.878579   \n",
      "Children 4 (gen6)        0.821626     0.896157      0.878113   \n",
      "Children 4 (gen7)        0.824848      0.89712      0.882456   \n",
      "Children 5 (gen5)        0.822126      0.89641      0.877209   \n",
      "Children 5 (gen7)        0.821467     0.896816      0.878704   \n",
      "Children 6 (gen7)        0.827856     0.896613      0.885367   \n",
      "\n",
      "                  Train roc_auc_adj Test precision Test recall Test roc_auc  \\\n",
      "Children 1 (gen4)          0.875254       0.814655    0.893723     0.862187   \n",
      "Children 1 (gen7)          0.878715       0.820538    0.883201     0.862037   \n",
      "Children 2 (gen7)          0.876675       0.817274    0.890413     0.861795   \n",
      "Children 3 (gen7)           0.88253       0.819555    0.884856     0.860269   \n",
      "Children 4 (gen3)          0.876949       0.817667    0.887457     0.862276   \n",
      "Children 4 (gen6)          0.876514       0.818419    0.889821     0.862127   \n",
      "Children 4 (gen7)          0.880286       0.818142    0.890294     0.860758   \n",
      "Children 5 (gen5)          0.875752       0.817135    0.889585     0.862641   \n",
      "Children 5 (gen7)          0.877016       0.817402    0.889585     0.861824   \n",
      "Children 6 (gen7)          0.882864       0.819654    0.885447     0.860337   \n",
      "\n",
      "                  Test roc_auc_adj  \n",
      "Children 1 (gen4)         0.860735  \n",
      "Children 1 (gen7)         0.860184  \n",
      "Children 2 (gen7)         0.860141  \n",
      "Children 3 (gen7)         0.857795  \n",
      "Children 4 (gen3)         0.860646  \n",
      "Children 4 (gen6)         0.860528  \n",
      "Children 4 (gen7)         0.858588  \n",
      "Children 5 (gen5)         0.861184  \n",
      "Children 5 (gen7)         0.860137  \n",
      "Children 6 (gen7)         0.857834  \n",
      "\n",
      "Best parents in generation 8: Children 5 (gen5), Children 1 (gen4), Children 4 (gen3), Children 4 (gen6), \n",
      "\n",
      "New generation: \n",
      "                  colsample_bytree learning_rate max_depth n_estimators  \\\n",
      "Children 1 (gen4)              0.4         0.109         5           70   \n",
      "Children 1 (gen8)              0.7         0.137        10           70   \n",
      "Children 2 (gen8)              0.4         0.135         5           45   \n",
      "Children 3 (gen8)              0.4         0.109         9           45   \n",
      "Children 4 (gen3)              0.9         0.135         5           45   \n",
      "Children 4 (gen6)              0.9         0.137         5           45   \n",
      "Children 4 (gen8)              0.4         0.109         5           55   \n",
      "Children 5 (gen5)              0.4         0.183         5           45   \n",
      "Children 5 (gen8)              0.5         0.183         3           45   \n",
      "Children 6 (gen8)              0.5         0.183         5           20   \n",
      "\n",
      "                  reg_lambda subsample  \n",
      "Children 1 (gen4)        0.1       0.8  \n",
      "Children 1 (gen8)        0.1       0.8  \n",
      "Children 2 (gen8)       0.05       0.8  \n",
      "Children 3 (gen8)        0.1       0.8  \n",
      "Children 4 (gen3)       0.05       0.8  \n",
      "Children 4 (gen6)        0.1       0.8  \n",
      "Children 4 (gen8)       0.05       0.9  \n",
      "Children 5 (gen5)       0.05       0.8  \n",
      "Children 5 (gen8)       0.05       0.8  \n",
      "Children 6 (gen8)       0.05       0.8  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                  This is number 9 generation\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Train precision Train recall Train roc_auc  \\\n",
      "Children 1 (gen4)        0.819497     0.898945      0.876706   \n",
      "Children 1 (gen8)        0.865279     0.906652      0.929036   \n",
      "Children 2 (gen8)         0.82037     0.896866      0.874793   \n",
      "Children 3 (gen8)        0.831887     0.898996      0.893583   \n",
      "Children 4 (gen3)        0.822857     0.894788      0.878579   \n",
      "Children 4 (gen6)        0.821626     0.896157      0.878113   \n",
      "Children 4 (gen8)        0.821943     0.893419      0.874812   \n",
      "Children 5 (gen5)        0.822126      0.89641      0.877209   \n",
      "Children 5 (gen8)        0.819729     0.893672      0.868248   \n",
      "Children 6 (gen8)         0.82571     0.883278       0.86949   \n",
      "\n",
      "                  Train roc_auc_adj Test precision Test recall Test roc_auc  \\\n",
      "Children 1 (gen4)          0.875254       0.814655    0.893723     0.862187   \n",
      "Children 1 (gen8)          0.921433       0.822461    0.867478     0.853005   \n",
      "Children 2 (gen8)          0.873509       0.816782    0.890649     0.861955   \n",
      "Children 3 (gen8)          0.890192       0.818691     0.88131     0.859674   \n",
      "Children 4 (gen3)          0.876949       0.817667    0.887457     0.862276   \n",
      "Children 4 (gen6)          0.876514       0.818419    0.889821     0.862127   \n",
      "Children 4 (gen8)          0.873433       0.817568    0.886866     0.861027   \n",
      "Children 5 (gen5)          0.875752       0.817135    0.889585     0.862641   \n",
      "Children 5 (gen8)           0.86728       0.815932    0.888758     0.858566   \n",
      "Children 6 (gen8)          0.868359        0.82366    0.877409      0.85818   \n",
      "\n",
      "                  Test roc_auc_adj  \n",
      "Children 1 (gen4)         0.860735  \n",
      "Children 1 (gen8)         0.845402  \n",
      "Children 2 (gen8)         0.860672  \n",
      "Children 3 (gen8)         0.856283  \n",
      "Children 4 (gen3)         0.860646  \n",
      "Children 4 (gen6)         0.860528  \n",
      "Children 4 (gen8)         0.859648  \n",
      "Children 5 (gen5)         0.861184  \n",
      "Children 5 (gen8)         0.857598  \n",
      "Children 6 (gen8)         0.857049  \n",
      "\n",
      "Best parents in generation 9: Children 5 (gen5), Children 1 (gen4), Children 2 (gen8), Children 4 (gen3), \n",
      "\n",
      "New generation: \n",
      "                  colsample_bytree learning_rate max_depth n_estimators  \\\n",
      "Children 1 (gen4)              0.4         0.109         5           70   \n",
      "Children 1 (gen9)              0.9         0.135         5           45   \n",
      "Children 2 (gen8)              0.4         0.135         5           45   \n",
      "Children 2 (gen9)              0.4         0.135         5           70   \n",
      "Children 3 (gen9)              0.4         0.109         5           35   \n",
      "Children 4 (gen3)              0.9         0.135         5           45   \n",
      "Children 4 (gen9)              0.4         0.135        10           65   \n",
      "Children 5 (gen5)              0.4         0.183         5           45   \n",
      "Children 5 (gen9)              0.6         0.135         5           70   \n",
      "Children 6 (gen9)              0.7         0.135         5           50   \n",
      "\n",
      "                  reg_lambda subsample  \n",
      "Children 1 (gen4)        0.1       0.8  \n",
      "Children 1 (gen9)       0.05       0.8  \n",
      "Children 2 (gen8)       0.05       0.8  \n",
      "Children 2 (gen9)        0.3       0.7  \n",
      "Children 3 (gen9)        0.1       0.7  \n",
      "Children 4 (gen3)       0.05       0.8  \n",
      "Children 4 (gen9)        0.1       0.8  \n",
      "Children 5 (gen5)       0.05       0.8  \n",
      "Children 5 (gen9)       0.25       0.8  \n",
      "Children 6 (gen9)       0.05       0.8  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                  This is number 10 generation\n",
      "\n",
      "                  Train precision Train recall Train roc_auc  \\\n",
      "Children 1 (gen4)        0.819497     0.898945      0.876706   \n",
      "Children 1 (gen9)        0.822857     0.894788      0.878579   \n",
      "Children 2 (gen8)         0.82037     0.896866      0.874793   \n",
      "Children 2 (gen9)        0.822122     0.897323      0.878236   \n",
      "Children 3 (gen9)        0.820464     0.893266      0.867958   \n",
      "Children 4 (gen3)        0.822857     0.894788      0.878579   \n",
      "Children 4 (gen9)        0.846671     0.900162      0.909682   \n",
      "Children 5 (gen5)        0.822126      0.89641      0.877209   \n",
      "Children 5 (gen9)        0.823658      0.89783      0.881164   \n",
      "Children 6 (gen9)        0.820776     0.899807      0.877853   \n",
      "\n",
      "                  Train roc_auc_adj Test precision Test recall Test roc_auc  \\\n",
      "Children 1 (gen4)          0.875254       0.814655    0.893723     0.862187   \n",
      "Children 1 (gen9)          0.876949       0.817667    0.887457     0.862276   \n",
      "Children 2 (gen8)          0.873509       0.816782    0.890649     0.861955   \n",
      "Children 2 (gen9)          0.876621       0.816548    0.891358     0.862091   \n",
      "Children 3 (gen9)            0.8668       0.816136    0.887339     0.856377   \n",
      "Children 4 (gen3)          0.876949       0.817667    0.887457     0.862276   \n",
      "Children 4 (gen9)          0.904401       0.820999    0.870789     0.856873   \n",
      "Children 5 (gen5)          0.875752       0.817135    0.889585     0.862641   \n",
      "Children 5 (gen9)          0.879294       0.817856    0.890176      0.86246   \n",
      "Children 6 (gen9)          0.876286       0.816677    0.892659     0.862182   \n",
      "\n",
      "                  Test roc_auc_adj  \n",
      "Children 1 (gen4)         0.860735  \n",
      "Children 1 (gen9)         0.860646  \n",
      "Children 2 (gen8)         0.860672  \n",
      "Children 2 (gen9)         0.860477  \n",
      "Children 3 (gen9)         0.855219  \n",
      "Children 4 (gen3)         0.860646  \n",
      "Children 4 (gen9)         0.851592  \n",
      "Children 5 (gen5)         0.861184  \n",
      "Children 5 (gen9)          0.86059  \n",
      "Children 6 (gen9)         0.860615  \n",
      "\n",
      "Best parents in generation 10: Children 5 (gen5), Children 1 (gen4), Children 2 (gen8), Children 1 (gen9), \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Best Algorithm: Children 5 (gen5)\n",
      "roc_auc_adj train = 0.8757521190391626,  roc_auc_adj test = 0.86118431483775\n",
      "\n",
      "colsample_bytree = 0.4\n",
      "learning_rate = 0.183\n",
      "max_depth = 5.0\n",
      "n_estimators = 45.0\n",
      "reg_lambda = 0.05\n",
      "subsample = 0.8\n",
      "\n",
      "\n",
      "Time = 0.9076868136723836\n"
     ]
    }
   ],
   "source": [
    "import time\n",
    "start = time.time()\n",
    "\n",
    "for generation in range(numberOfGenerations + 1):\n",
    "    \n",
    "    print(\"                                  This is number %s generation\" % (generation))\n",
    "    print()\n",
    "    \n",
    "    # Строим модель на каждом родителе и считаем метрики\n",
    "    # Output: словарь с метриками - DICT(ключ - имя родителя, значение - numpy массив с метриками в лексикографическом порядке)\n",
    "    scorePopulation['Gender ' + str(generation)] =\\\n",
    "        gboost.train_population(population['Gender ' + str(generation)], #Данные по текущему поколению\n",
    "                                hyperparameterSettings, #Набор гиперпараметров с допустимыми значениями\n",
    "                                X_train, Y_train, #Данные для обучения\n",
    "                                X_test, Y_test, #Данные для теста\n",
    "                                metric, # Основная метрика (на основе которой будут выбираться \"лучшие\" родители)\n",
    "                                metricList, #Список метрик на которые также хотим смотреть\n",
    "                                alg = 'XGB',\n",
    "                                coef_adj = coef_adj) \n",
    "    \n",
    "    print()\n",
    "    print('Best parents in generation %s: ' % (generation), end = '')\n",
    "         \n",
    "    # Отбор \"лучших\" родителей  \n",
    "    # Output: список лучших родителей - LIST\n",
    "    bestParents =\\\n",
    "        gboost.new_parents_selection(scorePopulation['Gender ' + str(generation)], # Расчитанные метрики для текущего поколения \n",
    "                                    numberOfParentsMating, #Количество родителей, которые останутся\n",
    "                                    metricList, #Список метрик на которые также хотим смотреть\n",
    "                                    metric)# Основная метрика\n",
    "    \n",
    "    if(generation < numberOfGenerations):\n",
    "        \n",
    "        # Скрещивание \"лучших\" родителей\n",
    "        # Output: словарь детей - DICT(ключ - имя ребенка, значение - numpy массив с гиперпараметрами в лексикографическом порядке)\n",
    "        children =\\\n",
    "            gboost.crossover_uniform(bestParents, #\"лучшие\" родители \n",
    "                                     population['Gender ' + str(generation)], #Словарь родителей текущего поколения \n",
    "                                     numberOfChildren, #Количество детей\n",
    "                                     generation) # Номер текущего поколения (нужен для именования детей)\n",
    "\n",
    "        # Мутация детей\n",
    "        gboost.mutation(children, #словарь детей\n",
    "                        hyperparameterSettings, #Множество допустимых значений для каждого гиперпараметра\n",
    "                        2) #Количество гиперпараметров для мутации\n",
    "\n",
    "        print()\n",
    "        print()\n",
    "        print('New generation: ')\n",
    "\n",
    "        # Получение нового поколения: \"лучшие\" родители + дети\n",
    "        # Output: Словарь родителей нового поколения - DICT(ключ - имя родителя, значение - numpy массив с гиперпараметрами \n",
    "                                                                                       #в лексикографическом порядке)\n",
    "        population['Gender ' + str(generation + 1)] =\\\n",
    "            gboost.getNewPopulation(population['Gender ' + str(generation)], # Текущее поколение со значениями их параметров\n",
    "                                    children, # Дети со значениями их параметров\n",
    "                                    bestParents, # Имена \"лучших\" родителей\n",
    "                                    list(sorted(hyperparameterSettings.keys()))) # Названия гиперпараметров\n",
    "\n",
    "        print('\\n\\n\\n\\n\\n\\n')\n",
    "       \n",
    "    # Печать на экран лучшего алгоритам\n",
    "    if(generation == numberOfGenerations):\n",
    "        gboost.printBestAlgorithm(bestParents[0],\n",
    "                                scorePopulation['Gender ' + str(generation)][bestParents[0]],\n",
    "                                metricList, metric,\n",
    "                                list(sorted(hyperparameterSettings.keys())),\n",
    "                                population['Gender ' + str(generation)][bestParents[0]])\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "time_genetic_xgb = (time.time() - start) / 60.0\n",
    "print(f\"Time = {time_genetic_xgb}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Замечание.** Данные метод **не возвращает** модель на оптимальных значениях гиперпараметров. По полученным значениям ее нужно будет построить."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### LightGBM\n",
    "Инициализация параметров \n",
    "\n",
    "- Количество родителей, детей, \"лучших\" родителей и поколений - оставляем теми же. \n",
    "- Также не изменяем метрики и набор допустимых значений гиперпараметров.\n",
    "- Исходное поколение берем то же самое"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Словарь поколений: \n",
    "# ключ - номер поколения, \n",
    "# элемент - словарь, ключи которого - имена родителей/детей, а значения - набор гиперпараметров в лексикографическом порядке\n",
    "population = {'Gender 0': population['Gender 0']}\n",
    "\n",
    "# Словарь метрик поколений: \n",
    "# ключ - номер поколения, \n",
    "# элемент - словарь, ключи которого - имена родителей/детей, а значения - результаты метрик в лексикографическом порядке\n",
    "scorePopulation = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отбор лучших родителей, построение нового поколения "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  This is number 0 generation\n",
      "\n",
      "           Train precision Train recall Train roc_auc Train roc_auc_adj  \\\n",
      "Parents 1         0.637427            1      0.873936          0.872467   \n",
      "Parents 10        0.822404     0.904219      0.887421          0.884671   \n",
      "Parents 2         0.869025      0.90432      0.932087          0.923978   \n",
      "Parents 3         0.863902     0.897982      0.924974          0.917544   \n",
      "Parents 4         0.821925     0.898692      0.880248          0.878361   \n",
      "Parents 5         0.823708     0.892455      0.876454          0.875093   \n",
      "Parents 6         0.817337      0.88779      0.862932          0.861959   \n",
      "Parents 7         0.822964     0.903458      0.887849          0.884995   \n",
      "Parents 8         0.819435     0.897881      0.873389           0.87191   \n",
      "Parents 9          0.82583     0.892911      0.880135          0.878192   \n",
      "\n",
      "           Test precision Test recall Test roc_auc Test roc_auc_adj  \n",
      "Parents 1        0.637934           1     0.859251         0.857783  \n",
      "Parents 10       0.812016    0.893132     0.859916         0.857166  \n",
      "Parents 2        0.819281    0.864996        0.851         0.842891  \n",
      "Parents 3        0.823476    0.860858     0.850672         0.843242  \n",
      "Parents 4        0.815195    0.891713     0.861378         0.859491  \n",
      "Parents 5         0.82085    0.886157     0.862843         0.861482  \n",
      "Parents 6        0.814807    0.882137     0.853199         0.852226  \n",
      "Parents 7        0.811807    0.890885     0.859311         0.856457  \n",
      "Parents 8        0.814076    0.892895     0.858599          0.85712  \n",
      "Parents 9        0.818918    0.884265     0.860709         0.858766  \n",
      "\n",
      "Best parents in generation 0: Parents 5, Parents 4, Parents 9, Parents 1, \n",
      "\n",
      "New generation: \n",
      "                  colsample_bytree learning_rate max_depth n_estimators  \\\n",
      "Children 1 (gen0)              0.4         0.109        10           60   \n",
      "Children 2 (gen0)              0.7         0.181         3          105   \n",
      "Children 3 (gen0)              0.8         0.277         5          100   \n",
      "Children 4 (gen0)              0.7         0.003         8           45   \n",
      "Children 5 (gen0)              0.4         0.277         4           60   \n",
      "Children 6 (gen0)              0.8         0.069         5           35   \n",
      "Parents 1                      0.7         0.003         8           35   \n",
      "Parents 4                      0.8         0.183         5           50   \n",
      "Parents 5                      0.4         0.109         4          105   \n",
      "Parents 9                      0.6         0.277         4           60   \n",
      "\n",
      "                  reg_lambda subsample  \n",
      "Children 1 (gen0)        0.1       0.4  \n",
      "Children 2 (gen0)       0.25       0.8  \n",
      "Children 3 (gen0)       0.15       0.8  \n",
      "Children 4 (gen0)       0.05       0.5  \n",
      "Children 5 (gen0)       0.05       0.8  \n",
      "Children 6 (gen0)       0.15       0.7  \n",
      "Parents 1               0.25       0.8  \n",
      "Parents 4               0.15       0.8  \n",
      "Parents 5               0.05       0.4  \n",
      "Parents 9               0.05       0.8  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                  This is number 1 generation\n",
      "\n",
      "                  Train precision Train recall Train roc_auc  \\\n",
      "Children 1 (gen0)        0.839424     0.898033      0.905947   \n",
      "Children 2 (gen0)        0.822691     0.893064      0.876059   \n",
      "Children 3 (gen0)        0.835872     0.896055      0.896422   \n",
      "Children 4 (gen0)        0.637427            1      0.874203   \n",
      "Children 5 (gen0)        0.825697     0.891847      0.878999   \n",
      "Children 6 (gen0)        0.810293     0.900517      0.866622   \n",
      "Parents 1                0.637427            1      0.873936   \n",
      "Parents 4                0.821925     0.898692      0.880248   \n",
      "Parents 5                0.823708     0.892455      0.876454   \n",
      "Parents 9                 0.82583     0.892911      0.880135   \n",
      "\n",
      "                  Train roc_auc_adj Test precision Test recall Test roc_auc  \\\n",
      "Children 1 (gen0)          0.901152       0.820892    0.878827     0.858003   \n",
      "Children 2 (gen0)          0.874581       0.819226    0.885566      0.86128   \n",
      "Children 3 (gen0)          0.892718       0.818872    0.880246     0.859379   \n",
      "Children 4 (gen0)          0.872679       0.637934           1     0.858962   \n",
      "Children 5 (gen0)          0.877316       0.821107    0.883911     0.862175   \n",
      "Children 6 (gen0)          0.865453       0.805786    0.895614     0.854938   \n",
      "Parents 1                  0.872467       0.637934           1     0.859251   \n",
      "Parents 4                  0.878361       0.815195    0.891713     0.861378   \n",
      "Parents 5                  0.875093        0.82085    0.886157     0.862843   \n",
      "Parents 9                  0.878192       0.818918    0.884265     0.860709   \n",
      "\n",
      "                  Test roc_auc_adj  \n",
      "Children 1 (gen0)         0.853209  \n",
      "Children 2 (gen0)         0.859802  \n",
      "Children 3 (gen0)         0.855675  \n",
      "Children 4 (gen0)         0.857437  \n",
      "Children 5 (gen0)         0.860493  \n",
      "Children 6 (gen0)          0.85377  \n",
      "Parents 1                 0.857783  \n",
      "Parents 4                 0.859491  \n",
      "Parents 5                 0.861482  \n",
      "Parents 9                 0.858766  \n",
      "\n",
      "Best parents in generation 1: Parents 5, Children 5 (gen0), Children 2 (gen0), Parents 4, \n",
      "\n",
      "New generation: \n",
      "                  colsample_bytree learning_rate max_depth n_estimators  \\\n",
      "Children 1 (gen1)              0.4         0.109         7          105   \n",
      "Children 2 (gen0)              0.7         0.181         3          105   \n",
      "Children 2 (gen1)              0.8         0.277         5           90   \n",
      "Children 3 (gen1)              0.4         0.109         3          110   \n",
      "Children 4 (gen1)              0.8         0.109         5          105   \n",
      "Children 5 (gen0)              0.4         0.277         4           60   \n",
      "Children 5 (gen1)              0.6         0.277         3           15   \n",
      "Children 6 (gen1)              0.4         0.109         4           60   \n",
      "Parents 4                      0.8         0.183         5           50   \n",
      "Parents 5                      0.4         0.109         4          105   \n",
      "\n",
      "                  reg_lambda subsample  \n",
      "Children 1 (gen1)        0.1       0.8  \n",
      "Children 2 (gen0)       0.25       0.8  \n",
      "Children 2 (gen1)       0.15       0.8  \n",
      "Children 3 (gen1)       0.05       0.5  \n",
      "Children 4 (gen1)       0.15       0.4  \n",
      "Children 5 (gen0)       0.05       0.8  \n",
      "Children 5 (gen1)       0.05       0.8  \n",
      "Children 6 (gen1)        0.3       0.6  \n",
      "Parents 4               0.15       0.8  \n",
      "Parents 5               0.05       0.4  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                  This is number 2 generation\n",
      "\n",
      "                  Train precision Train recall Train roc_auc  \\\n",
      "Children 1 (gen1)        0.831715     0.894889      0.894651   \n",
      "Children 2 (gen0)        0.822691     0.893064      0.876059   \n",
      "Children 2 (gen1)        0.833734     0.896765      0.894443   \n",
      "Children 3 (gen1)        0.820042     0.896258      0.871805   \n",
      "Children 4 (gen1)        0.824819     0.894788      0.883539   \n",
      "Children 5 (gen0)        0.825697     0.891847      0.878999   \n",
      "Children 5 (gen1)        0.809083      0.88703      0.854583   \n",
      "Children 6 (gen1)        0.818342     0.896765      0.871216   \n",
      "Parents 4                0.821925     0.898692      0.880248   \n",
      "Parents 5                0.823708     0.892455      0.876454   \n",
      "\n",
      "                  Train roc_auc_adj Test precision Test recall Test roc_auc  \\\n",
      "Children 1 (gen1)          0.891284       0.820612    0.882019     0.860983   \n",
      "Children 2 (gen0)          0.874581       0.819226    0.885566      0.86128   \n",
      "Children 2 (gen1)          0.890965       0.818022    0.882137     0.859671   \n",
      "Children 3 (gen1)           0.87068       0.815511     0.89254     0.860553   \n",
      "Children 4 (gen1)          0.881248       0.817201    0.886275     0.860629   \n",
      "Children 5 (gen0)          0.877316       0.821107    0.883911     0.862175   \n",
      "Children 5 (gen1)          0.853629       0.808359     0.88261     0.845045   \n",
      "Children 6 (gen1)          0.870077       0.815818    0.893841     0.859822   \n",
      "Parents 4                  0.878361       0.815195    0.891713     0.861378   \n",
      "Parents 5                  0.875093        0.82085    0.886157     0.862843   \n",
      "\n",
      "                  Test roc_auc_adj  \n",
      "Children 1 (gen1)         0.857617  \n",
      "Children 2 (gen0)         0.859802  \n",
      "Children 2 (gen1)         0.856194  \n",
      "Children 3 (gen1)         0.859428  \n",
      "Children 4 (gen1)         0.858338  \n",
      "Children 5 (gen0)         0.860493  \n",
      "Children 5 (gen1)         0.844091  \n",
      "Children 6 (gen1)         0.858682  \n",
      "Parents 4                 0.859491  \n",
      "Parents 5                 0.861482  \n",
      "\n",
      "Best parents in generation 2: Parents 5, Children 5 (gen0), Children 2 (gen0), Parents 4, \n",
      "\n",
      "New generation: \n",
      "                  colsample_bytree learning_rate max_depth n_estimators  \\\n",
      "Children 1 (gen2)              0.6         0.285         4           60   \n",
      "Children 2 (gen0)              0.7         0.181         3          105   \n",
      "Children 2 (gen2)              0.9         0.181         8          105   \n",
      "Children 3 (gen2)              0.4         0.277         4           50   \n",
      "Children 4 (gen2)              0.4         0.277         4          105   \n",
      "Children 5 (gen0)              0.4         0.277         4           60   \n",
      "Children 5 (gen2)              0.5         0.091         3          105   \n",
      "Children 6 (gen2)              0.8         0.105         7           50   \n",
      "Parents 4                      0.8         0.183         5           50   \n",
      "Parents 5                      0.4         0.109         4          105   \n",
      "\n",
      "                  reg_lambda subsample  \n",
      "Children 1 (gen2)       0.05       0.8  \n",
      "Children 2 (gen0)       0.25       0.8  \n",
      "Children 2 (gen2)       0.25       0.8  \n",
      "Children 3 (gen2)       0.05       0.6  \n",
      "Children 4 (gen2)       0.05       0.9  \n",
      "Children 5 (gen0)       0.05       0.8  \n",
      "Children 5 (gen2)       0.25       0.8  \n",
      "Children 6 (gen2)       0.25       0.8  \n",
      "Parents 4               0.15       0.8  \n",
      "Parents 5               0.05       0.4  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                  This is number 3 generation\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Train precision Train recall Train roc_auc  \\\n",
      "Children 1 (gen2)        0.826264     0.892962      0.880367   \n",
      "Children 2 (gen0)        0.822691     0.893064      0.876059   \n",
      "Children 2 (gen2)        0.859985     0.900974      0.924713   \n",
      "Children 3 (gen2)        0.825213     0.890528       0.87767   \n",
      "Children 4 (gen2)        0.828534      0.89281      0.884097   \n",
      "Children 5 (gen0)        0.825697     0.891847      0.878999   \n",
      "Children 5 (gen2)        0.821495      0.89139      0.870863   \n",
      "Children 6 (gen2)         0.82542     0.897323      0.885387   \n",
      "Parents 4                0.821925     0.898692      0.880248   \n",
      "Parents 5                0.823708     0.892455      0.876454   \n",
      "\n",
      "                  Train roc_auc_adj Test precision Test recall Test roc_auc  \\\n",
      "Children 1 (gen2)          0.878486       0.821922    0.885566     0.861557   \n",
      "Children 2 (gen0)          0.874581       0.819226    0.885566      0.86128   \n",
      "Children 2 (gen2)          0.917607       0.820493    0.869961     0.853653   \n",
      "Children 3 (gen2)          0.876127       0.821268    0.883792     0.862248   \n",
      "Children 4 (gen2)          0.881834       0.821711    0.883201     0.861473   \n",
      "Children 5 (gen0)          0.877316       0.821107    0.883911     0.862175   \n",
      "Children 5 (gen2)          0.869791       0.818112    0.886393     0.860143   \n",
      "Children 6 (gen2)          0.882881       0.817171     0.88663     0.860332   \n",
      "Parents 4                  0.878361       0.815195    0.891713     0.861378   \n",
      "Parents 5                  0.875093        0.82085    0.886157     0.862843   \n",
      "\n",
      "                  Test roc_auc_adj  \n",
      "Children 1 (gen2)         0.859676  \n",
      "Children 2 (gen0)         0.859802  \n",
      "Children 2 (gen2)         0.846548  \n",
      "Children 3 (gen2)         0.860706  \n",
      "Children 4 (gen2)         0.859211  \n",
      "Children 5 (gen0)         0.860493  \n",
      "Children 5 (gen2)         0.859071  \n",
      "Children 6 (gen2)         0.857827  \n",
      "Parents 4                 0.859491  \n",
      "Parents 5                 0.861482  \n",
      "\n",
      "Best parents in generation 3: Parents 5, Children 3 (gen2), Children 5 (gen0), Children 2 (gen0), \n",
      "\n",
      "New generation: \n",
      "                  colsample_bytree learning_rate max_depth n_estimators  \\\n",
      "Children 1 (gen3)              0.7         0.277         8          105   \n",
      "Children 2 (gen0)              0.7         0.181         3          105   \n",
      "Children 2 (gen3)              0.4         0.109         4          105   \n",
      "Children 3 (gen2)              0.4         0.277         4           50   \n",
      "Children 3 (gen3)              0.4         0.181         3          105   \n",
      "Children 4 (gen3)              0.4         0.277        10           60   \n",
      "Children 5 (gen0)              0.4         0.277         4           60   \n",
      "Children 5 (gen3)              0.5         0.051         4          105   \n",
      "Children 6 (gen3)              0.8         0.277         4           50   \n",
      "Parents 5                      0.4         0.109         4          105   \n",
      "\n",
      "                  reg_lambda subsample  \n",
      "Children 1 (gen3)        0.3       0.8  \n",
      "Children 2 (gen0)       0.25       0.8  \n",
      "Children 2 (gen3)       0.25       0.6  \n",
      "Children 3 (gen2)       0.05       0.6  \n",
      "Children 3 (gen3)        0.3       0.6  \n",
      "Children 4 (gen3)       0.05       0.6  \n",
      "Children 5 (gen0)       0.05       0.8  \n",
      "Children 5 (gen3)       0.05       0.4  \n",
      "Children 6 (gen3)       0.05       0.9  \n",
      "Parents 5               0.05       0.4  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                  This is number 4 generation\n",
      "\n",
      "                  Train precision Train recall Train roc_auc  \\\n",
      "Children 1 (gen3)        0.872729     0.898793      0.932588   \n",
      "Children 2 (gen0)        0.822691     0.893064      0.876059   \n",
      "Children 2 (gen3)        0.822794     0.893926      0.876453   \n",
      "Children 3 (gen2)        0.825213     0.890528       0.87767   \n",
      "Children 3 (gen3)         0.82163     0.895244      0.875183   \n",
      "Children 4 (gen3)        0.861772     0.899351      0.924414   \n",
      "Children 5 (gen0)        0.825697     0.891847      0.878999   \n",
      "Children 5 (gen3)        0.821514     0.891035      0.869935   \n",
      "Children 6 (gen3)        0.825062     0.894382      0.878968   \n",
      "Parents 5                0.823708     0.892455      0.876454   \n",
      "\n",
      "                  Train roc_auc_adj Test precision Test recall Test roc_auc  \\\n",
      "Children 1 (gen3)          0.924362       0.822915    0.857548     0.850335   \n",
      "Children 2 (gen0)          0.874581       0.819226    0.885566      0.86128   \n",
      "Children 2 (gen3)          0.875067       0.820096    0.888639     0.862591   \n",
      "Children 3 (gen2)          0.876127       0.821268    0.883792     0.862248   \n",
      "Children 3 (gen3)          0.873912       0.817876    0.890294     0.862467   \n",
      "Children 4 (gen3)          0.917148       0.822477    0.864287     0.851752   \n",
      "Children 5 (gen0)          0.877316       0.821107    0.883911     0.862175   \n",
      "Children 5 (gen3)          0.868836       0.818152    0.885566     0.858943   \n",
      "Children 6 (gen3)          0.877312       0.818638     0.88793     0.862402   \n",
      "Parents 5                  0.875093        0.82085    0.886157     0.862843   \n",
      "\n",
      "                  Test roc_auc_adj  \n",
      "Children 1 (gen3)         0.842109  \n",
      "Children 2 (gen0)         0.859802  \n",
      "Children 2 (gen3)         0.861205  \n",
      "Children 3 (gen2)         0.860706  \n",
      "Children 3 (gen3)         0.861196  \n",
      "Children 4 (gen3)         0.844486  \n",
      "Children 5 (gen0)         0.860493  \n",
      "Children 5 (gen3)         0.857844  \n",
      "Children 6 (gen3)         0.860746  \n",
      "Parents 5                 0.861482  \n",
      "\n",
      "Best parents in generation 4: Parents 5, Children 2 (gen3), Children 3 (gen3), Children 6 (gen3), \n",
      "\n",
      "New generation: \n",
      "                  colsample_bytree learning_rate max_depth n_estimators  \\\n",
      "Children 1 (gen4)              0.8         0.109         3           90   \n",
      "Children 2 (gen3)              0.4         0.109         4          105   \n",
      "Children 2 (gen4)              0.6         0.109         4           50   \n",
      "Children 3 (gen3)              0.4         0.181         3          105   \n",
      "Children 3 (gen4)              0.4         0.109         7          105   \n",
      "Children 4 (gen4)              0.4         0.079         5          105   \n",
      "Children 5 (gen4)              0.6         0.109         3          105   \n",
      "Children 6 (gen3)              0.8         0.277         4           50   \n",
      "Children 6 (gen4)              0.7         0.295         4          105   \n",
      "Parents 5                      0.4         0.109         4          105   \n",
      "\n",
      "                  reg_lambda subsample  \n",
      "Children 1 (gen4)       0.05       0.9  \n",
      "Children 2 (gen3)       0.25       0.6  \n",
      "Children 2 (gen4)        0.1       0.9  \n",
      "Children 3 (gen3)        0.3       0.6  \n",
      "Children 3 (gen4)       0.05       0.9  \n",
      "Children 4 (gen4)       0.05       0.4  \n",
      "Children 5 (gen4)       0.05       0.6  \n",
      "Children 6 (gen3)       0.05       0.9  \n",
      "Children 6 (gen4)        0.3       0.6  \n",
      "Parents 5               0.05       0.4  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                  This is number 5 generation\n",
      "\n",
      "                  Train precision Train recall Train roc_auc  \\\n",
      "Children 1 (gen4)        0.821659     0.892151      0.871342   \n",
      "Children 2 (gen3)        0.822794     0.893926      0.876453   \n",
      "Children 2 (gen4)        0.823706     0.888652      0.871257   \n",
      "Children 3 (gen3)         0.82163     0.895244      0.875183   \n",
      "Children 3 (gen4)        0.832397     0.893976      0.895074   \n",
      "Children 4 (gen4)         0.82351     0.894078      0.879216   \n",
      "Children 5 (gen4)        0.822116     0.891897      0.872866   \n",
      "Children 6 (gen3)        0.825062     0.894382      0.878968   \n",
      "Children 6 (gen4)        0.829846      0.89494      0.887689   \n",
      "Parents 5                0.823708     0.892455      0.876454   \n",
      "\n",
      "                  Train roc_auc_adj Test precision Test recall Test roc_auc  \\\n",
      "Children 1 (gen4)          0.870255       0.818539    0.886275     0.860469   \n",
      "Children 2 (gen3)          0.875067       0.820096    0.888639     0.862591   \n",
      "Children 2 (gen4)          0.870057        0.82093    0.882847      0.85926   \n",
      "Children 3 (gen3)          0.873912       0.817876    0.890294     0.862467   \n",
      "Children 3 (gen4)          0.891703       0.821204    0.881783      0.86136   \n",
      "Children 4 (gen4)          0.877481       0.818202    0.889585     0.861867   \n",
      "Children 5 (gen4)          0.871694       0.818509     0.88663     0.861144   \n",
      "Children 6 (gen3)          0.877312       0.818638     0.88793     0.862402   \n",
      "Children 6 (gen4)          0.884995       0.819579    0.882847     0.860753   \n",
      "Parents 5                  0.875093        0.82085    0.886157     0.862843   \n",
      "\n",
      "                  Test roc_auc_adj  \n",
      "Children 1 (gen4)         0.859381  \n",
      "Children 2 (gen3)         0.861205  \n",
      "Children 2 (gen4)          0.85806  \n",
      "Children 3 (gen3)         0.861196  \n",
      "Children 3 (gen4)         0.857989  \n",
      "Children 4 (gen4)         0.860132  \n",
      "Children 5 (gen4)         0.859972  \n",
      "Children 6 (gen3)         0.860746  \n",
      "Children 6 (gen4)          0.85806  \n",
      "Parents 5                 0.861482  \n",
      "\n",
      "Best parents in generation 5: Parents 5, Children 2 (gen3), Children 3 (gen3), Children 6 (gen3), \n",
      "\n",
      "New generation: \n",
      "                  colsample_bytree learning_rate max_depth n_estimators  \\\n",
      "Children 1 (gen5)              0.8         0.177         4           50   \n",
      "Children 2 (gen3)              0.4         0.109         4          105   \n",
      "Children 2 (gen5)              0.7         0.277         6           50   \n",
      "Children 3 (gen3)              0.4         0.181         3          105   \n",
      "Children 3 (gen5)              0.4         0.005         3           50   \n",
      "Children 4 (gen5)              0.5         0.109         3          105   \n",
      "Children 5 (gen5)              0.4         0.109         3           95   \n",
      "Children 6 (gen3)              0.8         0.277         4           50   \n",
      "Children 6 (gen5)              0.4         0.181         5          105   \n",
      "Parents 5                      0.4         0.109         4          105   \n",
      "\n",
      "                  reg_lambda subsample  \n",
      "Children 1 (gen5)       0.25       0.6  \n",
      "Children 2 (gen3)       0.25       0.6  \n",
      "Children 2 (gen5)       0.05       0.9  \n",
      "Children 3 (gen3)        0.3       0.6  \n",
      "Children 3 (gen5)       0.05       0.9  \n",
      "Children 4 (gen5)        0.2       0.6  \n",
      "Children 5 (gen5)       0.25       0.4  \n",
      "Children 6 (gen3)       0.05       0.9  \n",
      "Children 6 (gen5)        0.3       0.6  \n",
      "Parents 5               0.05       0.4  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                  This is number 6 generation\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Train precision Train recall Train roc_auc  \\\n",
      "Children 1 (gen5)        0.821715     0.892962      0.874793   \n",
      "Children 2 (gen3)        0.822794     0.893926      0.876453   \n",
      "Children 2 (gen5)        0.830658     0.897373      0.893472   \n",
      "Children 3 (gen3)         0.82163     0.895244      0.875183   \n",
      "Children 3 (gen5)        0.637427            1      0.814419   \n",
      "Children 4 (gen5)         0.82077     0.893266      0.872035   \n",
      "Children 5 (gen5)        0.820415     0.894128      0.870147   \n",
      "Children 6 (gen3)        0.825062     0.894382      0.878968   \n",
      "Children 6 (gen5)        0.828489     0.893266      0.886813   \n",
      "Parents 5                0.823708     0.892455      0.876454   \n",
      "\n",
      "                  Train roc_auc_adj Test precision Test recall Test roc_auc  \\\n",
      "Children 1 (gen5)          0.873405       0.818608    0.888285     0.860908   \n",
      "Children 2 (gen3)          0.875067       0.820096    0.888639     0.862591   \n",
      "Children 2 (gen5)          0.890061       0.819019    0.882728     0.859362   \n",
      "Children 3 (gen3)          0.873912       0.817876    0.890294     0.862467   \n",
      "Children 3 (gen5)          0.813395       0.637934           1     0.804179   \n",
      "Children 4 (gen5)           0.87095       0.817559    0.888403     0.861188   \n",
      "Children 5 (gen5)           0.86905       0.817274    0.890413     0.859176   \n",
      "Children 6 (gen3)          0.877312       0.818638     0.88793     0.862402   \n",
      "Children 6 (gen5)           0.88428       0.821084    0.884856     0.861485   \n",
      "Parents 5                  0.875093        0.82085    0.886157     0.862843   \n",
      "\n",
      "                  Test roc_auc_adj  \n",
      "Children 1 (gen5)         0.859519  \n",
      "Children 2 (gen3)         0.861205  \n",
      "Children 2 (gen5)         0.855951  \n",
      "Children 3 (gen3)         0.861196  \n",
      "Children 3 (gen5)         0.803156  \n",
      "Children 4 (gen5)         0.860104  \n",
      "Children 5 (gen5)         0.858078  \n",
      "Children 6 (gen3)         0.860746  \n",
      "Children 6 (gen5)         0.858953  \n",
      "Parents 5                 0.861482  \n",
      "\n",
      "Best parents in generation 6: Parents 5, Children 2 (gen3), Children 3 (gen3), Children 6 (gen3), \n",
      "\n",
      "New generation: \n",
      "                  colsample_bytree learning_rate max_depth n_estimators  \\\n",
      "Children 1 (gen6)              0.4         0.077         4           20   \n",
      "Children 2 (gen3)              0.4         0.109         4          105   \n",
      "Children 2 (gen6)              0.4         0.109         5          105   \n",
      "Children 3 (gen3)              0.4         0.181         3          105   \n",
      "Children 3 (gen6)              0.4         0.109         5          105   \n",
      "Children 4 (gen6)              0.4         0.109         6          105   \n",
      "Children 5 (gen6)              0.8         0.123         8          105   \n",
      "Children 6 (gen3)              0.8         0.277         4           50   \n",
      "Children 6 (gen6)              0.4         0.109         4          105   \n",
      "Parents 5                      0.4         0.109         4          105   \n",
      "\n",
      "                  reg_lambda subsample  \n",
      "Children 1 (gen6)       0.25       0.6  \n",
      "Children 2 (gen3)       0.25       0.6  \n",
      "Children 2 (gen6)       0.05       0.9  \n",
      "Children 3 (gen3)        0.3       0.6  \n",
      "Children 3 (gen6)        0.2       0.4  \n",
      "Children 4 (gen6)        0.2       0.4  \n",
      "Children 5 (gen6)       0.25       0.6  \n",
      "Children 6 (gen3)       0.05       0.9  \n",
      "Children 6 (gen6)       0.05       0.6  \n",
      "Parents 5               0.05       0.4  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                  This is number 7 generation\n",
      "\n",
      "                  Train precision Train recall Train roc_auc  \\\n",
      "Children 1 (gen6)        0.810828     0.885407      0.849835   \n",
      "Children 2 (gen3)        0.822794     0.893926      0.876453   \n",
      "Children 2 (gen6)        0.824189      0.89494      0.881741   \n",
      "Children 3 (gen3)         0.82163     0.895244      0.875183   \n",
      "Children 3 (gen6)        0.823914     0.895142      0.881974   \n",
      "Children 4 (gen6)        0.828112     0.895295      0.888302   \n",
      "Children 5 (gen6)        0.843428     0.900264      0.909596   \n",
      "Children 6 (gen3)        0.825062     0.894382      0.878968   \n",
      "Children 6 (gen6)        0.823708     0.892455      0.876454   \n",
      "Parents 5                0.823708     0.892455      0.876454   \n",
      "\n",
      "                  Train roc_auc_adj Test precision Test recall Test roc_auc  \\\n",
      "Children 1 (gen6)          0.848927       0.810758    0.876699     0.840759   \n",
      "Children 2 (gen3)          0.875067       0.820096    0.888639     0.862591   \n",
      "Children 2 (gen6)          0.879781       0.818747    0.888048     0.862139   \n",
      "Children 3 (gen3)          0.873912       0.817876    0.890294     0.862467   \n",
      "Children 3 (gen6)          0.879999       0.818677     0.88923     0.862221   \n",
      "Children 4 (gen6)          0.885604       0.820628    0.886984     0.861321   \n",
      "Children 5 (gen6)          0.904351       0.818543    0.876699     0.857144   \n",
      "Children 6 (gen3)          0.877312       0.818638     0.88793     0.862402   \n",
      "Children 6 (gen6)          0.875093        0.82085    0.886157     0.862843   \n",
      "Parents 5                  0.875093        0.82085    0.886157     0.862843   \n",
      "\n",
      "                  Test roc_auc_adj  \n",
      "Children 1 (gen6)         0.839852  \n",
      "Children 2 (gen3)         0.861205  \n",
      "Children 2 (gen6)         0.860179  \n",
      "Children 3 (gen3)         0.861196  \n",
      "Children 3 (gen6)         0.860246  \n",
      "Children 4 (gen6)         0.858622  \n",
      "Children 5 (gen6)         0.851898  \n",
      "Children 6 (gen3)         0.860746  \n",
      "Children 6 (gen6)         0.861482  \n",
      "Parents 5                 0.861482  \n",
      "\n",
      "Best parents in generation 7: Children 6 (gen6), Parents 5, Children 2 (gen3), Children 3 (gen3), \n",
      "\n",
      "New generation: \n",
      "                  colsample_bytree learning_rate max_depth n_estimators  \\\n",
      "Children 1 (gen7)              0.4         0.109         3          105   \n",
      "Children 2 (gen3)              0.4         0.109         4          105   \n",
      "Children 2 (gen7)              0.5         0.109         4          105   \n",
      "Children 3 (gen3)              0.4         0.181         3          105   \n",
      "Children 3 (gen7)              0.4         0.181         4          110   \n",
      "Children 4 (gen7)              0.4         0.109         4          105   \n",
      "Children 5 (gen7)              0.4         0.109         6           80   \n",
      "Children 6 (gen6)              0.4         0.109         4          105   \n",
      "Children 6 (gen7)              0.9         0.109         4          105   \n",
      "Parents 5                      0.4         0.109         4          105   \n",
      "\n",
      "                  reg_lambda subsample  \n",
      "Children 1 (gen7)        0.3       0.9  \n",
      "Children 2 (gen3)       0.25       0.6  \n",
      "Children 2 (gen7)       0.05       0.6  \n",
      "Children 3 (gen3)        0.3       0.6  \n",
      "Children 3 (gen7)        0.2       0.4  \n",
      "Children 4 (gen7)       0.25       0.8  \n",
      "Children 5 (gen7)       0.05       0.6  \n",
      "Children 6 (gen6)       0.05       0.6  \n",
      "Children 6 (gen7)        0.3       0.8  \n",
      "Parents 5               0.05       0.4  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                  This is number 8 generation\n",
      "\n",
      "                  Train precision Train recall Train roc_auc  \\\n",
      "Children 1 (gen7)        0.820833     0.893419      0.871061   \n",
      "Children 2 (gen3)        0.822794     0.893926      0.876453   \n",
      "Children 2 (gen7)        0.823027     0.894179      0.877066   \n",
      "Children 3 (gen3)         0.82163     0.895244      0.875183   \n",
      "Children 3 (gen7)         0.82697     0.891796      0.881175   \n",
      "Children 4 (gen7)        0.822794     0.893926      0.876453   \n",
      "Children 5 (gen7)        0.826384     0.894433      0.885483   \n",
      "Children 6 (gen6)        0.823708     0.892455      0.876454   \n",
      "Children 6 (gen7)        0.823153     0.893773       0.87752   \n",
      "Parents 5                0.823708     0.892455      0.876454   \n",
      "\n",
      "                  Train roc_auc_adj Test precision Test recall Test roc_auc  \\\n",
      "Children 1 (gen7)          0.869962       0.818202    0.888521     0.860069   \n",
      "Children 2 (gen3)          0.875067       0.820096    0.888639     0.862591   \n",
      "Children 2 (gen7)          0.875517       0.818558    0.887457     0.861572   \n",
      "Children 3 (gen3)          0.873912       0.817876    0.890294     0.862467   \n",
      "Children 3 (gen7)           0.87927       0.820358    0.883201     0.862127   \n",
      "Children 4 (gen7)          0.875067       0.820096    0.888639     0.862591   \n",
      "Children 5 (gen7)          0.883145       0.820236    0.888403     0.862105   \n",
      "Children 6 (gen6)          0.875093        0.82085    0.886157     0.862843   \n",
      "Children 6 (gen7)          0.875952       0.818954    0.888758     0.861836   \n",
      "Parents 5                  0.875093        0.82085    0.886157     0.862843   \n",
      "\n",
      "                  Test roc_auc_adj  \n",
      "Children 1 (gen7)          0.85897  \n",
      "Children 2 (gen3)         0.861205  \n",
      "Children 2 (gen7)         0.860022  \n",
      "Children 3 (gen3)         0.861196  \n",
      "Children 3 (gen7)         0.860222  \n",
      "Children 4 (gen7)         0.861205  \n",
      "Children 5 (gen7)         0.859767  \n",
      "Children 6 (gen6)         0.861482  \n",
      "Children 6 (gen7)         0.860268  \n",
      "Parents 5                 0.861482  \n",
      "\n",
      "Best parents in generation 8: Children 6 (gen6), Parents 5, Children 2 (gen3), Children 4 (gen7), \n",
      "\n",
      "New generation: \n",
      "                  colsample_bytree learning_rate max_depth n_estimators  \\\n",
      "Children 1 (gen8)              0.5         0.109         4          105   \n",
      "Children 2 (gen3)              0.4         0.109         4          105   \n",
      "Children 2 (gen8)              0.4         0.109         9           45   \n",
      "Children 3 (gen8)              0.4         0.109         6          105   \n",
      "Children 4 (gen7)              0.4         0.109         4          105   \n",
      "Children 4 (gen8)              0.9         0.109         7          105   \n",
      "Children 5 (gen8)              0.4         0.051         4          105   \n",
      "Children 6 (gen6)              0.4         0.109         4          105   \n",
      "Children 6 (gen8)              0.4         0.109         4          105   \n",
      "Parents 5                      0.4         0.109         4          105   \n",
      "\n",
      "                  reg_lambda subsample  \n",
      "Children 1 (gen8)       0.25       0.7  \n",
      "Children 2 (gen3)       0.25       0.6  \n",
      "Children 2 (gen8)       0.05       0.6  \n",
      "Children 3 (gen8)       0.25       0.9  \n",
      "Children 4 (gen7)       0.25       0.8  \n",
      "Children 4 (gen8)       0.05       0.6  \n",
      "Children 5 (gen8)        0.3       0.6  \n",
      "Children 6 (gen6)       0.05       0.6  \n",
      "Children 6 (gen8)        0.3       0.9  \n",
      "Parents 5               0.05       0.4  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                  This is number 9 generation\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Train precision Train recall Train roc_auc  \\\n",
      "Children 1 (gen8)        0.821668      0.89641      0.877107   \n",
      "Children 2 (gen3)        0.822794     0.893926      0.876453   \n",
      "Children 2 (gen8)        0.830708     0.895954      0.893818   \n",
      "Children 3 (gen8)        0.827399     0.895447       0.88805   \n",
      "Children 4 (gen7)        0.822794     0.893926      0.876453   \n",
      "Children 4 (gen8)        0.833819     0.899097       0.89859   \n",
      "Children 5 (gen8)        0.819584     0.894635       0.86968   \n",
      "Children 6 (gen6)        0.823708     0.892455      0.876454   \n",
      "Children 6 (gen8)        0.823689      0.89281      0.876639   \n",
      "Parents 5                0.823708     0.892455      0.876454   \n",
      "\n",
      "                  Train roc_auc_adj Test precision Test recall Test roc_auc  \\\n",
      "Children 1 (gen8)           0.87557       0.817816     0.88994     0.861737   \n",
      "Children 2 (gen3)          0.875067       0.820096    0.888639     0.862591   \n",
      "Children 2 (gen8)           0.89016        0.81971    0.882019     0.857236   \n",
      "Children 3 (gen8)          0.885399       0.819699    0.888403     0.861535   \n",
      "Children 4 (gen7)          0.875067       0.820096    0.888639     0.862591   \n",
      "Children 4 (gen8)          0.894666       0.817703     0.88131     0.859355   \n",
      "Children 5 (gen8)           0.86855       0.815581    0.889821     0.858379   \n",
      "Children 6 (gen6)          0.875093        0.82085    0.886157     0.862843   \n",
      "Children 6 (gen8)          0.875245        0.81962     0.88793       0.8627   \n",
      "Parents 5                  0.875093        0.82085    0.886157     0.862843   \n",
      "\n",
      "                  Test roc_auc_adj  \n",
      "Children 1 (gen8)           0.8602  \n",
      "Children 2 (gen3)         0.861205  \n",
      "Children 2 (gen8)         0.853578  \n",
      "Children 3 (gen8)         0.858883  \n",
      "Children 4 (gen7)         0.861205  \n",
      "Children 4 (gen8)         0.855431  \n",
      "Children 5 (gen8)         0.857249  \n",
      "Children 6 (gen6)         0.861482  \n",
      "Children 6 (gen8)         0.861307  \n",
      "Parents 5                 0.861482  \n",
      "\n",
      "Best parents in generation 9: Children 6 (gen6), Parents 5, Children 6 (gen8), Children 2 (gen3), \n",
      "\n",
      "New generation: \n",
      "                  colsample_bytree learning_rate max_depth n_estimators  \\\n",
      "Children 1 (gen9)              0.6         0.109         4          105   \n",
      "Children 2 (gen3)              0.4         0.109         4          105   \n",
      "Children 2 (gen9)              0.4         0.109         3          105   \n",
      "Children 3 (gen9)              0.4         0.109         6          105   \n",
      "Children 4 (gen9)              0.4         0.109         5          105   \n",
      "Children 5 (gen9)              0.4         0.109         5          105   \n",
      "Children 6 (gen6)              0.4         0.109         4          105   \n",
      "Children 6 (gen8)              0.4         0.109         4          105   \n",
      "Children 6 (gen9)              0.5         0.109         4           30   \n",
      "Parents 5                      0.4         0.109         4          105   \n",
      "\n",
      "                  reg_lambda subsample  \n",
      "Children 1 (gen9)       0.25       0.9  \n",
      "Children 2 (gen3)       0.25       0.6  \n",
      "Children 2 (gen9)        0.3       0.6  \n",
      "Children 3 (gen9)        0.2       0.6  \n",
      "Children 4 (gen9)        0.3       0.9  \n",
      "Children 5 (gen9)        0.3       0.9  \n",
      "Children 6 (gen6)       0.05       0.6  \n",
      "Children 6 (gen8)        0.3       0.9  \n",
      "Children 6 (gen9)       0.05       0.6  \n",
      "Parents 5               0.05       0.4  \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                  This is number 10 generation\n",
      "\n",
      "                  Train precision Train recall Train roc_auc  \\\n",
      "Children 1 (gen9)        0.822922     0.893064      0.877136   \n",
      "Children 2 (gen3)        0.822794     0.893926      0.876453   \n",
      "Children 2 (gen9)        0.820833     0.893419      0.871061   \n",
      "Children 3 (gen9)        0.828112     0.895295      0.888302   \n",
      "Children 4 (gen9)        0.824533     0.894686      0.881842   \n",
      "Children 5 (gen9)        0.824533     0.894686      0.881842   \n",
      "Children 6 (gen6)        0.823708     0.892455      0.876454   \n",
      "Children 6 (gen8)        0.823689      0.89281      0.876639   \n",
      "Children 6 (gen9)        0.812752      0.89068      0.860977   \n",
      "Parents 5                0.823708     0.892455      0.876454   \n",
      "\n",
      "                  Train roc_auc_adj Test precision Test recall Test roc_auc  \\\n",
      "Children 1 (gen9)          0.875594       0.818946    0.887102     0.861722   \n",
      "Children 2 (gen3)          0.875067       0.820096    0.888639     0.862591   \n",
      "Children 2 (gen9)          0.869962       0.818202    0.888521     0.860069   \n",
      "Children 3 (gen9)          0.885604       0.820628    0.886984     0.861321   \n",
      "Children 4 (gen9)          0.879909       0.819133    0.888758     0.862514   \n",
      "Children 5 (gen9)          0.879909       0.819133    0.888758     0.862514   \n",
      "Children 6 (gen6)          0.875093        0.82085    0.886157     0.862843   \n",
      "Children 6 (gen8)          0.875245        0.81962     0.88793       0.8627   \n",
      "Children 6 (gen9)          0.859917       0.810132    0.884738     0.850375   \n",
      "Parents 5                  0.875093        0.82085    0.886157     0.862843   \n",
      "\n",
      "                  Test roc_auc_adj  \n",
      "Children 1 (gen9)         0.860181  \n",
      "Children 2 (gen3)         0.861205  \n",
      "Children 2 (gen9)          0.85897  \n",
      "Children 3 (gen9)         0.858622  \n",
      "Children 4 (gen9)         0.860581  \n",
      "Children 5 (gen9)         0.860581  \n",
      "Children 6 (gen6)         0.861482  \n",
      "Children 6 (gen8)         0.861307  \n",
      "Children 6 (gen9)         0.849314  \n",
      "Parents 5                 0.861482  \n",
      "\n",
      "Best parents in generation 10: Children 6 (gen6), Parents 5, Children 6 (gen8), Children 2 (gen3), \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Best Algorithm: Children 6 (gen6)\n",
      "roc_auc_adj train = 0.8750925051493156,  roc_auc_adj test = 0.8614816241355175\n",
      "\n",
      "colsample_bytree = 0.4\n",
      "learning_rate = 0.109\n",
      "max_depth = 4.0\n",
      "n_estimators = 105.0\n",
      "reg_lambda = 0.05\n",
      "subsample = 0.6000000000000001\n",
      "\n",
      "\n",
      "Time = 0.7067520022392273\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "for generation in range(numberOfGenerations + 1):\n",
    "    \n",
    "    print(\"                                  This is number %s generation\" % (generation))\n",
    "    print()\n",
    "    \n",
    "    # Строим модель на каждом родителе и считаем метрики\n",
    "    # Output: словарь с метриками - DICT(ключ - имя родителя, значение - numpy массив с метриками в лексикографическом порядке)\n",
    "    scorePopulation['Gender ' + str(generation)] =\\\n",
    "        gboost.train_population(population['Gender ' + str(generation)], #Данные по текущему поколению\n",
    "                                hyperparameterSettings, #Набор гиперпараметров с допустимыми значениями\n",
    "                                X_train, Y_train, #Данные для обучения\n",
    "                                X_test, Y_test, #Данные для теста\n",
    "                                metric, # Основная метрика (на основе которой будут выбираться \"лучшие\" родители)\n",
    "                                metricList, #Список метрик на которые также хотим смотреть\n",
    "                                alg = 'LightGBM',\n",
    "                                coef_adj = coef_adj,\n",
    "                                cat_feat = ['AVG_PAYMENT_COMP_3_6']) \n",
    "    \n",
    "    print()\n",
    "    print('Best parents in generation %s: ' % (generation), end = '')\n",
    "         \n",
    "    # Отбор \"лучших\" родителей  \n",
    "    # Output: список лучших родителей - LIST\n",
    "    bestParents =\\\n",
    "        gboost.new_parents_selection(scorePopulation['Gender ' + str(generation)], # Расчитанные метрики для текущего поколения \n",
    "                                    numberOfParentsMating, #Количество родителей, которые останутся\n",
    "                                    metricList, #Список метрик на которые также хотим смотреть\n",
    "                                    metric)# Основная метрика\n",
    "    \n",
    "    if(generation < numberOfGenerations):\n",
    "        \n",
    "        # Скрещивание \"лучших\" родителей\n",
    "        # Output: словарь детей - DICT(ключ - имя ребенка, значение - numpy массив с гиперпараметрами в лексикографическом порядке)\n",
    "        children =\\\n",
    "            gboost.crossover_uniform(bestParents, #\"лучшие\" родители \n",
    "                                     population['Gender ' + str(generation)], #Словарь родителей текущего поколения \n",
    "                                     numberOfChildren, #Количество детей\n",
    "                                     generation) # Номер текущего поколения (нужен для именования детей)\n",
    "\n",
    "        # Мутация детей\n",
    "        gboost.mutation(children, #словарь детей\n",
    "                        hyperparameterSettings, #Множество допустимых значений для каждого гиперпараметра\n",
    "                        2) #Количество гиперпараметров для мутации\n",
    "\n",
    "        print()\n",
    "        print()\n",
    "        print('New generation: ')\n",
    "\n",
    "        # Получение нового поколения: \"лучшие\" родители + дети\n",
    "        # Output: Словарь родителей нового поколения - DICT(ключ - имя родителя, значение - numpy массив с гиперпараметрами \n",
    "                                                                                       #в лексикографическом порядке)\n",
    "        population['Gender ' + str(generation + 1)] =\\\n",
    "            gboost.getNewPopulation(population['Gender ' + str(generation)], # Текущее поколение со значениями их параметров\n",
    "                                    children, # Дети со значениями их параметров\n",
    "                                    bestParents, # Имена \"лучших\" родителей\n",
    "                                    list(sorted(hyperparameterSettings.keys()))) # Названия гиперпараметров\n",
    "\n",
    "        print('\\n\\n\\n\\n\\n\\n')\n",
    "       \n",
    "    # Печать на экран лучшего алгоритам\n",
    "    if(generation == numberOfGenerations):\n",
    "        gboost.printBestAlgorithm(bestParents[0],\n",
    "                                scorePopulation['Gender ' + str(generation)][bestParents[0]],\n",
    "                                metricList, metric,\n",
    "                                list(sorted(hyperparameterSettings.keys())),\n",
    "                                population['Gender ' + str(generation)][bestParents[0]])\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "time_genetic_lgbm = (time.time() - start) / 60.0\n",
    "print(f\"Time = {time_genetic_lgbm}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### CatBoost\n",
    "Инициализация параметров"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "hyperparameterSettings = {'iterations': [10, 121, 5],\n",
    "                          'depth': [3, 11, 1],\n",
    "                          'learning_rate': [0.001, 0.3002, 0.002],\n",
    "                          'rsm': [0.4, 1.0, 0.1],\n",
    "                          'reg_lambda': [0.05, 0.35, 0.05]}\n",
    "\n",
    "# Словарь поколений: \n",
    "# ключ - номер поколения, \n",
    "# элемент - словарь, ключи которого - имена родителей/детей, а значения - набор гиперпараметров в лексикографическом порядке\n",
    "population = {}\n",
    "\n",
    "# Словарь метрик поколений: \n",
    "# ключ - номер поколения, \n",
    "# элемент - словарь, ключи которого - имена родителей/детей, а значения - результаты метрик в лексикографическом порядке\n",
    "scorePopulation = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            depth  iterations  learning_rate  reg_lambda  rsm\n",
      "Parents 1     9.0        25.0          0.281        0.05  0.8\n",
      "Parents 2     3.0        40.0          0.261        0.15  0.4\n",
      "Parents 3     8.0        10.0          0.195        0.20  0.6\n",
      "Parents 4     3.0        55.0          0.199        0.10  0.8\n",
      "Parents 5     4.0       115.0          0.255        0.25  0.4\n",
      "Parents 6     9.0        35.0          0.291        0.20  0.9\n",
      "Parents 7     4.0        25.0          0.253        0.30  0.5\n",
      "Parents 8     7.0       110.0          0.211        0.10  0.4\n",
      "Parents 9     7.0        55.0          0.027        0.30  0.4\n",
      "Parents 10    3.0        35.0          0.107        0.05  0.8\n"
     ]
    }
   ],
   "source": [
    "random.seed(238)\n",
    "population['Gender 0'] = gboost.initilialize_poplulation(numberOfParents, hyperparameterSettings)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Отбор лучших родителей, построение нового поколения "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                  This is number 0 generation\n",
      "\n",
      "           Train precision Train recall Train roc_auc Train roc_auc_adj  \\\n",
      "Parents 1         0.825781     0.892607      0.879185          0.877441   \n",
      "Parents 10        0.813195     0.883125       0.85024          0.849425   \n",
      "Parents 2         0.822832     0.886624      0.865759          0.864943   \n",
      "Parents 3         0.816019     0.890579        0.8644          0.863536   \n",
      "Parents 4         0.823552     0.885813      0.867176          0.866361   \n",
      "Parents 5         0.823047     0.895954      0.876249          0.874822   \n",
      "Parents 6         0.830303      0.89139      0.883878           0.88153   \n",
      "Parents 7         0.812647     0.896004      0.864417          0.863629   \n",
      "Parents 8         0.832876     0.893266      0.888853          0.886063   \n",
      "Parents 9           0.8135     0.889109      0.860761          0.859948   \n",
      "\n",
      "           Test precision Test recall Test roc_auc Test roc_auc_adj  \n",
      "Parents 1        0.819466    0.884856     0.861746         0.860002  \n",
      "Parents 10       0.812815     0.87729     0.842088         0.841272  \n",
      "Parents 2        0.821586    0.881901     0.857601         0.856786  \n",
      "Parents 3        0.815294    0.886039     0.855766         0.854902  \n",
      "Parents 4        0.820787    0.880364     0.859028         0.858213  \n",
      "Parents 5        0.817409    0.886984     0.861987          0.86056  \n",
      "Parents 6        0.821157    0.880955     0.860394         0.858045  \n",
      "Parents 7        0.811145    0.893132      0.85654         0.855752  \n",
      "Parents 8        0.821894    0.880482     0.860948         0.858158  \n",
      "Parents 9        0.813888    0.884029     0.852628         0.851815  \n",
      "\n",
      "Best parents in generation 0: Parents 5, Parents 1, Parents 4, Parents 8, \n",
      "\n",
      "New generation: \n",
      "                  depth iterations learning_rate reg_lambda  rsm\n",
      "Children 1 (gen0)     4        115         0.281       0.25  0.9\n",
      "Children 2 (gen0)     4         45         0.255        0.1  0.5\n",
      "Children 3 (gen0)     4        115         0.101       0.05  0.8\n",
      "Children 4 (gen0)     4        115         0.211        0.2  0.5\n",
      "Children 5 (gen0)     6         25         0.281       0.25  0.4\n",
      "Children 6 (gen0)     4         75         0.011        0.1  0.4\n",
      "Parents 1             9         25         0.281       0.05  0.8\n",
      "Parents 4             3         55         0.199        0.1  0.8\n",
      "Parents 5             4        115         0.255       0.25  0.4\n",
      "Parents 8             7        110         0.211        0.1  0.4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                  This is number 1 generation\n",
      "\n",
      "                  Train precision Train recall Train roc_auc  \\\n",
      "Children 1 (gen0)        0.827814     0.889768       0.87754   \n",
      "Children 2 (gen0)        0.817757     0.895295      0.869965   \n",
      "Children 3 (gen0)        0.824442     0.886979      0.870444   \n",
      "Children 4 (gen0)        0.823106     0.893723      0.875463   \n",
      "Children 5 (gen0)        0.821892     0.892404      0.869961   \n",
      "Children 6 (gen0)        0.816112     0.875824      0.840567   \n",
      "Parents 1                0.825781     0.892607      0.879185   \n",
      "Parents 4                0.823552     0.885813      0.867176   \n",
      "Parents 5                0.823047     0.895954      0.876249   \n",
      "Parents 8                0.832876     0.893266      0.888853   \n",
      "\n",
      "                  Train roc_auc_adj Test precision Test recall Test roc_auc  \\\n",
      "Children 1 (gen0)          0.875938       0.821681    0.880837     0.861518   \n",
      "Children 2 (gen0)          0.869085       0.815562    0.893368     0.861174   \n",
      "Children 3 (gen0)          0.869498       0.821425    0.882019     0.860986   \n",
      "Children 4 (gen0)           0.87414       0.819203    0.887575     0.862236   \n",
      "Children 5 (gen0)           0.86892       0.819255    0.886275     0.859548   \n",
      "Children 6 (gen0)          0.839751       0.815807    0.867597     0.832407   \n",
      "Parents 1                  0.877441       0.819466    0.884856     0.861746   \n",
      "Parents 4                  0.866361       0.820787    0.880364     0.859028   \n",
      "Parents 5                  0.874822       0.817409    0.886984     0.861987   \n",
      "Parents 8                  0.886063       0.821894    0.880482     0.860948   \n",
      "\n",
      "                  Test roc_auc_adj  \n",
      "Children 1 (gen0)         0.859915  \n",
      "Children 2 (gen0)         0.860295  \n",
      "Children 3 (gen0)          0.86004  \n",
      "Children 4 (gen0)         0.860914  \n",
      "Children 5 (gen0)         0.858507  \n",
      "Children 6 (gen0)         0.831591  \n",
      "Parents 1                 0.860002  \n",
      "Parents 4                 0.858213  \n",
      "Parents 5                  0.86056  \n",
      "Parents 8                 0.858158  \n",
      "\n",
      "Best parents in generation 1: Children 4 (gen0), Parents 5, Children 2 (gen0), Children 3 (gen0), \n",
      "\n",
      "New generation: \n",
      "                  depth iterations learning_rate reg_lambda  rsm\n",
      "Children 1 (gen1)     4         65         0.101       0.05  0.5\n",
      "Children 2 (gen0)     4         45         0.255        0.1  0.5\n",
      "Children 2 (gen1)     4         45         0.085        0.2  0.8\n",
      "Children 3 (gen0)     4        115         0.101       0.05  0.8\n",
      "Children 3 (gen1)     4        115         0.255       0.05  0.8\n",
      "Children 4 (gen0)     4        115         0.211        0.2  0.5\n",
      "Children 4 (gen1)     6         45         0.255       0.25  0.9\n",
      "Children 5 (gen1)     4        115         0.159       0.15  0.4\n",
      "Children 6 (gen1)     4        115         0.255        0.3  0.4\n",
      "Parents 5             4        115         0.255       0.25  0.4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                  This is number 2 generation\n",
      "\n",
      "                  Train precision Train recall Train roc_auc  \\\n",
      "Children 1 (gen1)        0.820151     0.886979      0.865789   \n",
      "Children 2 (gen0)        0.817757     0.895295      0.869965   \n",
      "Children 2 (gen1)        0.814608     0.886725      0.859481   \n",
      "Children 3 (gen0)        0.824442     0.886979      0.870444   \n",
      "Children 3 (gen1)        0.826827      0.89139      0.876981   \n",
      "Children 4 (gen0)        0.823106     0.893723      0.875463   \n",
      "Children 4 (gen1)        0.822712     0.894128       0.87401   \n",
      "Children 5 (gen1)        0.822691     0.893064       0.87266   \n",
      "Children 6 (gen1)        0.823463     0.894027      0.875951   \n",
      "Parents 5                0.823047     0.895954      0.876249   \n",
      "\n",
      "                  Train roc_auc_adj Test precision Test recall Test roc_auc  \\\n",
      "Children 1 (gen1)          0.864979       0.817993    0.882492     0.857691   \n",
      "Children 2 (gen0)          0.869085       0.815562    0.893368     0.861174   \n",
      "Children 2 (gen1)          0.858686       0.812732    0.881428     0.851536   \n",
      "Children 3 (gen0)          0.869498       0.821425    0.882019     0.860986   \n",
      "Children 3 (gen1)          0.875526       0.820136    0.884029     0.862424   \n",
      "Children 4 (gen0)           0.87414       0.819203    0.887575     0.862236   \n",
      "Children 4 (gen1)          0.872773        0.81834    0.887221     0.861642   \n",
      "Children 5 (gen1)          0.871595       0.819629    0.888521     0.862003   \n",
      "Children 6 (gen1)          0.874547       0.816934    0.886275     0.861909   \n",
      "Parents 5                  0.874822       0.817409    0.886984     0.861987   \n",
      "\n",
      "                  Test roc_auc_adj  \n",
      "Children 1 (gen1)         0.856881  \n",
      "Children 2 (gen0)         0.860295  \n",
      "Children 2 (gen1)         0.850741  \n",
      "Children 3 (gen0)          0.86004  \n",
      "Children 3 (gen1)         0.860969  \n",
      "Children 4 (gen0)         0.860914  \n",
      "Children 4 (gen1)         0.860405  \n",
      "Children 5 (gen1)         0.860937  \n",
      "Children 6 (gen1)         0.860505  \n",
      "Parents 5                  0.86056  \n",
      "\n",
      "Best parents in generation 2: Children 3 (gen1), Children 5 (gen1), Children 4 (gen0), Parents 5, \n",
      "\n",
      "New generation: \n",
      "                  depth iterations learning_rate reg_lambda  rsm\n",
      "Children 1 (gen2)     4        115         0.137       0.15  0.4\n",
      "Children 2 (gen2)     4         85         0.005        0.2  0.5\n",
      "Children 3 (gen1)     4        115         0.255       0.05  0.8\n",
      "Children 3 (gen2)     4        115         0.255       0.15  0.6\n",
      "Children 4 (gen0)     4        115         0.211        0.2  0.5\n",
      "Children 4 (gen2)     4        115         0.271       0.05  0.5\n",
      "Children 5 (gen1)     4        115         0.159       0.15  0.4\n",
      "Children 5 (gen2)     4         30         0.255       0.15  0.4\n",
      "Children 6 (gen2)     4        115         0.255       0.25  0.5\n",
      "Parents 5             4        115         0.255       0.25  0.4\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                  This is number 3 generation\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Train precision Train recall Train roc_auc  \\\n",
      "Children 1 (gen2)        0.822692     0.891188      0.871704   \n",
      "Children 2 (gen2)        0.816416     0.871514      0.839502   \n",
      "Children 3 (gen1)        0.826827      0.89139      0.876981   \n",
      "Children 3 (gen2)        0.825785     0.892151      0.876453   \n",
      "Children 4 (gen0)        0.823106     0.893723      0.875463   \n",
      "Children 4 (gen2)        0.824499     0.895193      0.877179   \n",
      "Children 5 (gen1)        0.822691     0.893064       0.87266   \n",
      "Children 5 (gen2)        0.822026     0.887131      0.866343   \n",
      "Children 6 (gen2)        0.822136     0.896004      0.875969   \n",
      "Parents 5                0.823047     0.895954      0.876249   \n",
      "\n",
      "                  Train roc_auc_adj Test precision Test recall Test roc_auc  \\\n",
      "Children 1 (gen2)          0.870665       0.819982    0.885802     0.861315   \n",
      "Children 2 (gen2)           0.83877        0.81527    0.863459     0.832178   \n",
      "Children 3 (gen1)          0.875526       0.820136    0.884029     0.862424   \n",
      "Children 3 (gen2)          0.874981        0.82019    0.886511     0.861734   \n",
      "Children 4 (gen0)           0.87414       0.819203    0.887575     0.862236   \n",
      "Children 4 (gen2)          0.875617         0.8184    0.887575     0.861561   \n",
      "Children 5 (gen1)          0.871595       0.819629    0.888521     0.862003   \n",
      "Children 5 (gen2)          0.865475        0.81928    0.882137     0.857659   \n",
      "Children 6 (gen2)          0.874631       0.817333    0.889703     0.862591   \n",
      "Parents 5                  0.874822       0.817409    0.886984     0.861987   \n",
      "\n",
      "                  Test roc_auc_adj  \n",
      "Children 1 (gen2)         0.860276  \n",
      "Children 2 (gen2)         0.831445  \n",
      "Children 3 (gen1)         0.860969  \n",
      "Children 3 (gen2)         0.860262  \n",
      "Children 4 (gen0)         0.860914  \n",
      "Children 4 (gen2)         0.859999  \n",
      "Children 5 (gen1)         0.860937  \n",
      "Children 5 (gen2)         0.856791  \n",
      "Children 6 (gen2)         0.861253  \n",
      "Parents 5                  0.86056  \n",
      "\n",
      "Best parents in generation 3: Children 6 (gen2), Children 3 (gen1), Children 5 (gen1), Children 4 (gen0), \n",
      "\n",
      "New generation: \n",
      "                  depth iterations learning_rate reg_lambda  rsm\n",
      "Children 1 (gen3)     5        115         0.143       0.15  0.4\n",
      "Children 2 (gen3)     4        115         0.277        0.2  0.4\n",
      "Children 3 (gen1)     4        115         0.255       0.05  0.8\n",
      "Children 3 (gen3)     6         40         0.211       0.15  0.4\n",
      "Children 4 (gen0)     4        115         0.211        0.2  0.5\n",
      "Children 4 (gen3)     4        115         0.269       0.05  0.4\n",
      "Children 5 (gen1)     4        115         0.159       0.15  0.4\n",
      "Children 5 (gen3)     4         80         0.211        0.1  0.5\n",
      "Children 6 (gen2)     4        115         0.255       0.25  0.5\n",
      "Children 6 (gen3)     4        115         0.039        0.2  0.5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                  This is number 4 generation\n",
      "\n",
      "                  Train precision Train recall Train roc_auc  \\\n",
      "Children 1 (gen3)        0.823789     0.891289      0.875074   \n",
      "Children 2 (gen3)        0.825136     0.893165      0.876257   \n",
      "Children 3 (gen1)        0.826827      0.89139      0.876981   \n",
      "Children 3 (gen3)        0.824201     0.886218      0.871149   \n",
      "Children 4 (gen0)        0.823106     0.893723      0.875463   \n",
      "Children 4 (gen3)        0.824878     0.893723      0.877073   \n",
      "Children 5 (gen1)        0.822691     0.893064       0.87266   \n",
      "Children 5 (gen3)        0.823416     0.890427      0.872273   \n",
      "Children 6 (gen2)        0.822136     0.896004      0.875969   \n",
      "Children 6 (gen3)        0.815642     0.886776      0.860217   \n",
      "\n",
      "                  Train roc_auc_adj Test precision Test recall Test roc_auc  \\\n",
      "Children 1 (gen3)          0.873772       0.820614    0.884738      0.86206   \n",
      "Children 2 (gen3)          0.874773       0.820294    0.884975     0.861421   \n",
      "Children 3 (gen1)          0.875526       0.820136    0.884029     0.862424   \n",
      "Children 3 (gen3)           0.87009       0.822185    0.880601     0.860561   \n",
      "Children 4 (gen0)           0.87414       0.819203    0.887575     0.862236   \n",
      "Children 4 (gen3)          0.875487       0.819373    0.886984     0.861216   \n",
      "Children 5 (gen1)          0.871595       0.819629    0.888521     0.862003   \n",
      "Children 5 (gen3)          0.871213       0.819893    0.885802     0.861669   \n",
      "Children 6 (gen2)          0.874631       0.817333    0.889703     0.862591   \n",
      "Children 6 (gen3)          0.859377       0.814007    0.882137     0.851817   \n",
      "\n",
      "                  Test roc_auc_adj  \n",
      "Children 1 (gen3)         0.860759  \n",
      "Children 2 (gen3)         0.859937  \n",
      "Children 3 (gen1)         0.860969  \n",
      "Children 3 (gen3)         0.859502  \n",
      "Children 4 (gen0)         0.860914  \n",
      "Children 4 (gen3)          0.85963  \n",
      "Children 5 (gen1)         0.860937  \n",
      "Children 5 (gen3)         0.860609  \n",
      "Children 6 (gen2)         0.861253  \n",
      "Children 6 (gen3)         0.850977  \n",
      "\n",
      "Best parents in generation 4: Children 6 (gen2), Children 3 (gen1), Children 5 (gen1), Children 4 (gen0), \n",
      "\n",
      "New generation: \n",
      "                  depth iterations learning_rate reg_lambda  rsm\n",
      "Children 1 (gen4)     3        115         0.255       0.05  0.9\n",
      "Children 2 (gen4)     4        105         0.159       0.15  0.5\n",
      "Children 3 (gen1)     4        115         0.255       0.05  0.8\n",
      "Children 3 (gen4)     7        115         0.211       0.25  0.8\n",
      "Children 4 (gen0)     4        115         0.211        0.2  0.5\n",
      "Children 4 (gen4)     4        120         0.039        0.2  0.5\n",
      "Children 5 (gen1)     4        115         0.159       0.15  0.4\n",
      "Children 5 (gen4)     8        115         0.113       0.15  0.4\n",
      "Children 6 (gen2)     4        115         0.255       0.25  0.5\n",
      "Children 6 (gen4)     8        115         0.159        0.2  0.8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                  This is number 5 generation\n",
      "\n",
      "                  Train precision Train recall Train roc_auc  \\\n",
      "Children 1 (gen4)        0.824362     0.891492      0.873545   \n",
      "Children 2 (gen4)        0.825298     0.887942      0.872686   \n",
      "Children 3 (gen1)        0.826827      0.89139      0.876981   \n",
      "Children 3 (gen4)        0.831334     0.895954      0.888314   \n",
      "Children 4 (gen0)        0.823106     0.893723      0.875463   \n",
      "Children 4 (gen4)        0.815988     0.886573      0.861105   \n",
      "Children 5 (gen1)        0.822691     0.893064       0.87266   \n",
      "Children 5 (gen4)        0.827227     0.892911      0.882485   \n",
      "Children 6 (gen2)        0.822136     0.896004      0.875969   \n",
      "Children 6 (gen4)        0.831584     0.896055      0.889667   \n",
      "\n",
      "                  Train roc_auc_adj Test precision Test recall Test roc_auc  \\\n",
      "Children 1 (gen4)          0.872404       0.819805    0.884738     0.862129   \n",
      "Children 2 (gen4)          0.871586        0.82196    0.881428     0.861684   \n",
      "Children 3 (gen1)          0.875526       0.820136    0.884029     0.862424   \n",
      "Children 3 (gen4)          0.885611        0.81855    0.883674     0.861289   \n",
      "Children 4 (gen0)           0.87414       0.819203    0.887575     0.862236   \n",
      "Children 4 (gen4)          0.860273       0.814685    0.881428     0.852783   \n",
      "Children 5 (gen1)          0.871595       0.819629    0.888521     0.862003   \n",
      "Children 5 (gen4)            0.8804       0.819356    0.884738     0.861638   \n",
      "Children 6 (gen2)          0.874631       0.817333    0.889703     0.862591   \n",
      "Children 6 (gen4)          0.886757       0.818212    0.881664     0.860564   \n",
      "\n",
      "                  Test roc_auc_adj  \n",
      "Children 1 (gen4)         0.860988  \n",
      "Children 2 (gen4)         0.860584  \n",
      "Children 3 (gen1)         0.860969  \n",
      "Children 3 (gen4)         0.858586  \n",
      "Children 4 (gen0)         0.860914  \n",
      "Children 4 (gen4)         0.851951  \n",
      "Children 5 (gen1)         0.860937  \n",
      "Children 5 (gen4)         0.859553  \n",
      "Children 6 (gen2)         0.861253  \n",
      "Children 6 (gen4)         0.857653  \n",
      "\n",
      "Best parents in generation 5: Children 6 (gen2), Children 1 (gen4), Children 3 (gen1), Children 5 (gen1), \n",
      "\n",
      "New generation: \n",
      "                  depth iterations learning_rate reg_lambda  rsm\n",
      "Children 1 (gen4)     3        115         0.255       0.05  0.9\n",
      "Children 1 (gen5)     8        115         0.159       0.05  0.8\n",
      "Children 2 (gen5)     4        115         0.063        0.3  0.5\n",
      "Children 3 (gen1)     4        115         0.255       0.05  0.8\n",
      "Children 3 (gen5)     4         65         0.011       0.25  0.4\n",
      "Children 4 (gen5)     3        115         0.229       0.25  0.8\n",
      "Children 5 (gen1)     4        115         0.159       0.15  0.4\n",
      "Children 5 (gen5)     3        115         0.069       0.25  0.9\n",
      "Children 6 (gen2)     4        115         0.255       0.25  0.5\n",
      "Children 6 (gen5)     8        115         0.255        0.3  0.8\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                  This is number 6 generation\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Train precision Train recall Train roc_auc  \\\n",
      "Children 1 (gen4)        0.824362     0.891492      0.873545   \n",
      "Children 1 (gen5)        0.833255     0.896714      0.891613   \n",
      "Children 2 (gen5)        0.821911     0.884799      0.866634   \n",
      "Children 3 (gen1)        0.826827      0.89139      0.876981   \n",
      "Children 3 (gen5)        0.812776     0.876077      0.838137   \n",
      "Children 4 (gen5)        0.824303     0.889464      0.872546   \n",
      "Children 5 (gen1)        0.822691     0.893064       0.87266   \n",
      "Children 5 (gen5)        0.818819     0.882466      0.863768   \n",
      "Children 6 (gen2)        0.822136     0.896004      0.875969   \n",
      "Children 6 (gen5)        0.836909     0.897931      0.896651   \n",
      "\n",
      "                  Train roc_auc_adj Test precision Test recall Test roc_auc  \\\n",
      "Children 1 (gen4)          0.872404       0.819805    0.884738     0.862129   \n",
      "Children 1 (gen5)          0.888485       0.819996    0.884265     0.860336   \n",
      "Children 2 (gen5)          0.865823       0.819392    0.880128      0.85852   \n",
      "Children 3 (gen1)          0.875526       0.820136    0.884029     0.862424   \n",
      "Children 3 (gen5)          0.837361       0.812258    0.867951     0.830376   \n",
      "Children 4 (gen5)          0.871485       0.820367    0.883792     0.861941   \n",
      "Children 5 (gen1)          0.871595       0.819629    0.888521     0.862003   \n",
      "Children 5 (gen5)          0.862961        0.81678    0.876936     0.855702   \n",
      "Children 6 (gen2)          0.874631       0.817333    0.889703     0.862591   \n",
      "Children 6 (gen5)          0.892862       0.819513    0.879773     0.858757   \n",
      "\n",
      "                  Test roc_auc_adj  \n",
      "Children 1 (gen4)         0.860988  \n",
      "Children 1 (gen5)         0.857209  \n",
      "Children 2 (gen5)         0.857709  \n",
      "Children 3 (gen1)         0.860969  \n",
      "Children 3 (gen5)         0.829599  \n",
      "Children 4 (gen5)         0.860881  \n",
      "Children 5 (gen1)         0.860937  \n",
      "Children 5 (gen5)         0.854896  \n",
      "Children 6 (gen2)         0.861253  \n",
      "Children 6 (gen5)         0.854967  \n",
      "\n",
      "Best parents in generation 6: Children 6 (gen2), Children 1 (gen4), Children 3 (gen1), Children 5 (gen1), \n",
      "\n",
      "New generation: \n",
      "                  depth iterations learning_rate reg_lambda  rsm\n",
      "Children 1 (gen4)     3        115         0.255       0.05  0.9\n",
      "Children 1 (gen6)     4        110         0.159       0.25  0.4\n",
      "Children 2 (gen6)     5        115         0.255       0.15  0.5\n",
      "Children 3 (gen1)     4        115         0.255       0.05  0.8\n",
      "Children 3 (gen6)     3         75         0.255       0.05  0.4\n",
      "Children 4 (gen6)     3        115         0.161       0.25  0.9\n",
      "Children 5 (gen1)     4        115         0.159       0.15  0.4\n",
      "Children 5 (gen6)    10        115         0.159       0.15  0.4\n",
      "Children 6 (gen2)     4        115         0.255       0.25  0.5\n",
      "Children 6 (gen6)     3        115         0.163       0.05  0.7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                  This is number 7 generation\n",
      "\n",
      "                  Train precision Train recall Train roc_auc  \\\n",
      "Children 1 (gen4)        0.824362     0.891492      0.873545   \n",
      "Children 1 (gen6)        0.824246     0.889109      0.872073   \n",
      "Children 2 (gen6)        0.825831     0.894838      0.880997   \n",
      "Children 3 (gen1)        0.826827      0.89139      0.876981   \n",
      "Children 3 (gen6)        0.822625     0.890072      0.870561   \n",
      "Children 4 (gen6)        0.823992      0.88779      0.870411   \n",
      "Children 5 (gen1)        0.822691     0.893064       0.87266   \n",
      "Children 5 (gen6)        0.842486     0.896866      0.903063   \n",
      "Children 6 (gen2)        0.822136     0.896004      0.875969   \n",
      "Children 6 (gen6)        0.823629     0.887942      0.870386   \n",
      "\n",
      "                  Train roc_auc_adj Test precision Test recall Test roc_auc  \\\n",
      "Children 1 (gen4)          0.872404       0.819805    0.884738     0.862129   \n",
      "Children 1 (gen6)          0.870974       0.820465    0.884383     0.861085   \n",
      "Children 2 (gen6)           0.87904        0.81843    0.887221     0.861436   \n",
      "Children 3 (gen1)          0.875526       0.820136    0.884029     0.862424   \n",
      "Children 3 (gen6)          0.869629       0.822212    0.885684     0.861243   \n",
      "Children 4 (gen6)           0.86946       0.821574    0.882374     0.860908   \n",
      "Children 5 (gen1)          0.871595       0.819629    0.888521     0.862003   \n",
      "Children 5 (gen6)          0.898569         0.8203    0.874217     0.858125   \n",
      "Children 6 (gen2)          0.874631       0.817333    0.889703     0.862591   \n",
      "Children 6 (gen6)          0.869492       0.820891     0.88261     0.861443   \n",
      "\n",
      "                  Test roc_auc_adj  \n",
      "Children 1 (gen4)         0.860988  \n",
      "Children 1 (gen6)         0.859986  \n",
      "Children 2 (gen6)          0.85948  \n",
      "Children 3 (gen1)         0.860969  \n",
      "Children 3 (gen6)         0.860311  \n",
      "Children 4 (gen6)         0.859957  \n",
      "Children 5 (gen1)         0.860937  \n",
      "Children 5 (gen6)         0.853631  \n",
      "Children 6 (gen2)         0.861253  \n",
      "Children 6 (gen6)         0.860549  \n",
      "\n",
      "Best parents in generation 7: Children 6 (gen2), Children 1 (gen4), Children 3 (gen1), Children 5 (gen1), \n",
      "\n",
      "New generation: \n",
      "                  depth iterations learning_rate reg_lambda  rsm\n",
      "Children 1 (gen4)     3        115         0.255       0.05  0.9\n",
      "Children 1 (gen7)     4        100         0.255       0.05  0.8\n",
      "Children 2 (gen7)     4         55         0.075       0.05  0.9\n",
      "Children 3 (gen1)     4        115         0.255       0.05  0.8\n",
      "Children 3 (gen7)     4        115         0.255       0.25  0.6\n",
      "Children 4 (gen7)     3        115         0.087        0.2  0.9\n",
      "Children 5 (gen1)     4        115         0.159       0.15  0.4\n",
      "Children 5 (gen7)     4        115         0.175        0.1  0.9\n",
      "Children 6 (gen2)     4        115         0.255       0.25  0.5\n",
      "Children 6 (gen7)     4         25         0.255       0.25  0.7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                  This is number 8 generation\n",
      "\n",
      "                  Train precision Train recall Train roc_auc  \\\n",
      "Children 1 (gen4)        0.824362     0.891492      0.873545   \n",
      "Children 1 (gen7)        0.827376       0.8885      0.875783   \n",
      "Children 2 (gen7)        0.815775     0.885762      0.860975   \n",
      "Children 3 (gen1)        0.826827      0.89139      0.876981   \n",
      "Children 3 (gen7)        0.826324     0.889717      0.876257   \n",
      "Children 4 (gen7)         0.82423     0.883785      0.866279   \n",
      "Children 5 (gen1)        0.822691     0.893064       0.87266   \n",
      "Children 5 (gen7)        0.824089     0.891238       0.87405   \n",
      "Children 6 (gen2)        0.822136     0.896004      0.875969   \n",
      "Children 6 (gen7)        0.821318     0.884951      0.866169   \n",
      "\n",
      "                  Train roc_auc_adj Test precision Test recall Test roc_auc  \\\n",
      "Children 1 (gen4)          0.872404       0.819805    0.884738     0.862129   \n",
      "Children 1 (gen7)          0.874444       0.821058    0.880364     0.862392   \n",
      "Children 2 (gen7)          0.860129        0.81437    0.881664     0.852516   \n",
      "Children 3 (gen1)          0.875526       0.820136    0.884029     0.862424   \n",
      "Children 3 (gen7)          0.874822       0.821968    0.882019     0.861904   \n",
      "Children 4 (gen7)          0.865498        0.82176    0.878591     0.858469   \n",
      "Children 5 (gen1)          0.871595       0.819629    0.888521     0.862003   \n",
      "Children 5 (gen7)          0.872859       0.821338    0.883674     0.862149   \n",
      "Children 6 (gen2)          0.874631       0.817333    0.889703     0.862591   \n",
      "Children 6 (gen7)          0.865332       0.818332     0.88131     0.857802   \n",
      "\n",
      "                  Test roc_auc_adj  \n",
      "Children 1 (gen4)         0.860988  \n",
      "Children 1 (gen7)         0.861053  \n",
      "Children 2 (gen7)          0.85167  \n",
      "Children 3 (gen1)         0.860969  \n",
      "Children 3 (gen7)         0.860469  \n",
      "Children 4 (gen7)         0.857688  \n",
      "Children 5 (gen1)         0.860937  \n",
      "Children 5 (gen7)         0.860958  \n",
      "Children 6 (gen2)         0.861253  \n",
      "Children 6 (gen7)         0.856965  \n",
      "\n",
      "Best parents in generation 8: Children 6 (gen2), Children 1 (gen7), Children 1 (gen4), Children 3 (gen1), \n",
      "\n",
      "New generation: \n",
      "                  depth iterations learning_rate reg_lambda  rsm\n",
      "Children 1 (gen4)     3        115         0.255       0.05  0.9\n",
      "Children 1 (gen7)     4        100         0.255       0.05  0.8\n",
      "Children 1 (gen8)     3        115         0.295        0.3  0.8\n",
      "Children 2 (gen8)     8         15         0.255       0.05  0.9\n",
      "Children 3 (gen1)     4        115         0.255       0.05  0.8\n",
      "Children 3 (gen8)     4        115         0.255       0.25  0.4\n",
      "Children 4 (gen8)     3        115         0.001       0.05  0.8\n",
      "Children 5 (gen8)     7        115         0.103       0.05  0.5\n",
      "Children 6 (gen2)     4        115         0.255       0.25  0.5\n",
      "Children 6 (gen8)     4        115         0.145        0.1  0.5\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                  This is number 9 generation\n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                  Train precision Train recall Train roc_auc  \\\n",
      "Children 1 (gen4)        0.824362     0.891492      0.873545   \n",
      "Children 1 (gen7)        0.827376       0.8885      0.875783   \n",
      "Children 1 (gen8)        0.821189     0.894889      0.873908   \n",
      "Children 2 (gen8)        0.828336     0.882517      0.870855   \n",
      "Children 3 (gen1)        0.826827      0.89139      0.876981   \n",
      "Children 3 (gen8)        0.823047     0.895954      0.876249   \n",
      "Children 4 (gen8)        0.815994     0.866596      0.822805   \n",
      "Children 5 (gen8)         0.82651     0.891593      0.878476   \n",
      "Children 6 (gen2)        0.822136     0.896004      0.875969   \n",
      "Children 6 (gen8)        0.824138     0.889159      0.872275   \n",
      "\n",
      "                  Train roc_auc_adj Test precision Test recall Test roc_auc  \\\n",
      "Children 1 (gen4)          0.872404       0.819805    0.884738     0.862129   \n",
      "Children 1 (gen7)          0.874444       0.821058    0.880364     0.862392   \n",
      "Children 1 (gen8)          0.872704       0.817252    0.888166     0.861863   \n",
      "Children 2 (gen8)          0.869759       0.824591    0.875281     0.859895   \n",
      "Children 3 (gen1)          0.875526       0.820136    0.884029     0.862424   \n",
      "Children 3 (gen8)          0.874822       0.817409    0.886984     0.861987   \n",
      "Children 4 (gen8)          0.822227       0.813875    0.857075     0.817032   \n",
      "Children 5 (gen8)          0.876815       0.820375    0.884383     0.861866   \n",
      "Children 6 (gen2)          0.874631       0.817333    0.889703     0.862591   \n",
      "Children 6 (gen8)          0.871214        0.82115    0.883083     0.861663   \n",
      "\n",
      "                  Test roc_auc_adj  \n",
      "Children 1 (gen4)         0.860988  \n",
      "Children 1 (gen7)         0.861053  \n",
      "Children 1 (gen8)         0.860658  \n",
      "Children 2 (gen8)         0.858799  \n",
      "Children 3 (gen1)         0.860969  \n",
      "Children 3 (gen8)          0.86056  \n",
      "Children 4 (gen8)         0.816455  \n",
      "Children 5 (gen8)         0.860205  \n",
      "Children 6 (gen2)         0.861253  \n",
      "Children 6 (gen8)         0.860602  \n",
      "\n",
      "Best parents in generation 9: Children 6 (gen2), Children 1 (gen7), Children 1 (gen4), Children 3 (gen1), \n",
      "\n",
      "New generation: \n",
      "                  depth iterations learning_rate reg_lambda  rsm\n",
      "Children 1 (gen4)     3        115         0.255       0.05  0.9\n",
      "Children 1 (gen7)     4        100         0.255       0.05  0.8\n",
      "Children 1 (gen9)     4        115         0.173       0.25  0.7\n",
      "Children 2 (gen9)     6         90         0.255       0.05  0.8\n",
      "Children 3 (gen1)     4        115         0.255       0.05  0.8\n",
      "Children 3 (gen9)     6        115         0.255       0.05  0.6\n",
      "Children 4 (gen9)     6        110         0.255       0.25  0.5\n",
      "Children 5 (gen9)     4         80         0.055       0.05  0.8\n",
      "Children 6 (gen2)     4        115         0.255       0.25  0.5\n",
      "Children 6 (gen9)     3         40         0.255       0.05  0.7\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "                                  This is number 10 generation\n",
      "\n",
      "                  Train precision Train recall Train roc_auc  \\\n",
      "Children 1 (gen4)        0.824362     0.891492      0.873545   \n",
      "Children 1 (gen7)        0.827376       0.8885      0.875783   \n",
      "Children 1 (gen9)        0.823684     0.891593       0.87394   \n",
      "Children 2 (gen9)        0.829841     0.893672      0.883418   \n",
      "Children 3 (gen1)        0.826827      0.89139      0.876981   \n",
      "Children 3 (gen9)         0.83239     0.895193       0.88708   \n",
      "Children 4 (gen9)        0.830169      0.89352      0.884092   \n",
      "Children 5 (gen9)          0.8164     0.884951      0.860935   \n",
      "Children 6 (gen2)        0.822136     0.896004      0.875969   \n",
      "Children 6 (gen9)        0.823387     0.882213      0.865727   \n",
      "\n",
      "                  Train roc_auc_adj Test precision Test recall Test roc_auc  \\\n",
      "Children 1 (gen4)          0.872404       0.819805    0.884738     0.862129   \n",
      "Children 1 (gen7)          0.874444       0.821058    0.880364     0.862392   \n",
      "Children 1 (gen9)          0.872778       0.819925    0.884383     0.862325   \n",
      "Children 2 (gen9)          0.881119       0.819469    0.882728     0.860433   \n",
      "Children 3 (gen1)          0.875526       0.820136    0.884029     0.862424   \n",
      "Children 3 (gen9)          0.884433       0.820392    0.881783     0.860601   \n",
      "Children 4 (gen9)          0.881804       0.822053    0.883083     0.861217   \n",
      "Children 5 (gen9)          0.860129       0.813908    0.880009     0.852874   \n",
      "Children 6 (gen2)          0.874631       0.817333    0.889703     0.862591   \n",
      "Children 6 (gen9)           0.86495       0.821875    0.877645     0.857958   \n",
      "\n",
      "                  Test roc_auc_adj  \n",
      "Children 1 (gen4)         0.860988  \n",
      "Children 1 (gen7)         0.861053  \n",
      "Children 1 (gen9)         0.861164  \n",
      "Children 2 (gen9)         0.858134  \n",
      "Children 3 (gen1)         0.860969  \n",
      "Children 3 (gen9)         0.857953  \n",
      "Children 4 (gen9)          0.85893  \n",
      "Children 5 (gen9)         0.852068  \n",
      "Children 6 (gen2)         0.861253  \n",
      "Children 6 (gen9)         0.857181  \n",
      "\n",
      "Best parents in generation 10: Children 6 (gen2), Children 1 (gen9), Children 1 (gen7), Children 1 (gen4), \n",
      "\n",
      "\n",
      "\n",
      "\n",
      "Best Algorithm: Children 6 (gen2)\n",
      "roc_auc_adj train = 0.874631493341061,  roc_auc_adj test = 0.8612529198457572\n",
      "\n",
      "depth = 4.0\n",
      "iterations = 115.0\n",
      "learning_rate = 0.255\n",
      "reg_lambda = 0.25\n",
      "rsm = 0.5\n",
      "\n",
      "\n",
      "Time = 3.0548635880152384\n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "for generation in range(numberOfGenerations + 1):\n",
    "    \n",
    "    print(\"                                  This is number %s generation\" % (generation))\n",
    "    print()\n",
    "    \n",
    "    # Строим модель на каждом родителе и считаем метрики\n",
    "    # Output: словарь с метриками - DICT(ключ - имя родителя, значение - numpy массив с метриками в лексикографическом порядке)\n",
    "    scorePopulation['Gender ' + str(generation)] =\\\n",
    "        gboost.train_population(population['Gender ' + str(generation)], #Данные по текущему поколению\n",
    "                                hyperparameterSettings, #Набор гиперпараметров с допустимыми значениями\n",
    "                                X_train, Y_train, #Данные для обучения\n",
    "                                X_test, Y_test, #Данные для теста\n",
    "                                metric, # Основная метрика (на основе которой будут выбираться \"лучшие\" родители)\n",
    "                                metricList, #Список метрик на которые также хотим смотреть\n",
    "                                alg = 'CatBoost',\n",
    "                                coef_adj = coef_adj,\n",
    "                                cat_feat = [0]) \n",
    "    \n",
    "    print()\n",
    "    print('Best parents in generation %s: ' % (generation), end = '')\n",
    "         \n",
    "    # Отбор \"лучших\" родителей  \n",
    "    # Output: список лучших родителей - LIST\n",
    "    bestParents =\\\n",
    "        gboost.new_parents_selection(scorePopulation['Gender ' + str(generation)], # Расчитанные метрики для текущего поколения \n",
    "                                    numberOfParentsMating, #Количество родителей, которые останутся\n",
    "                                    metricList, #Список метрик на которые также хотим смотреть\n",
    "                                    metric)# Основная метрика\n",
    "    \n",
    "    if(generation < numberOfGenerations):\n",
    "        \n",
    "        # Скрещивание \"лучших\" родителей\n",
    "        # Output: словарь детей - DICT(ключ - имя ребенка, значение - numpy массив с гиперпараметрами в лексикографическом порядке)\n",
    "        children =\\\n",
    "            gboost.crossover_uniform(bestParents, #\"лучшие\" родители \n",
    "                                     population['Gender ' + str(generation)], #Словарь родителей текущего поколения \n",
    "                                     numberOfChildren, #Количество детей\n",
    "                                     generation) # Номер текущего поколения (нужен для именования детей)\n",
    "\n",
    "        # Мутация детей\n",
    "        gboost.mutation(children, #словарь детей\n",
    "                        hyperparameterSettings, #Множество допустимых значений для каждого гиперпараметра\n",
    "                        2) #Количество гиперпараметров для мутации\n",
    "\n",
    "        print()\n",
    "        print()\n",
    "        print('New generation: ')\n",
    "\n",
    "        # Получение нового поколения: \"лучшие\" родители + дети\n",
    "        # Output: Словарь родителей нового поколения - DICT(ключ - имя родителя, значение - numpy массив с гиперпараметрами \n",
    "                                                                                       #в лексикографическом порядке)\n",
    "        population['Gender ' + str(generation + 1)] =\\\n",
    "            gboost.getNewPopulation(population['Gender ' + str(generation)], # Текущее поколение со значениями их параметров\n",
    "                                    children, # Дети со значениями их параметров\n",
    "                                    bestParents, # Имена \"лучших\" родителей\n",
    "                                    list(sorted(hyperparameterSettings.keys()))) # Названия гиперпараметров\n",
    "\n",
    "        print('\\n\\n\\n\\n\\n\\n')\n",
    "       \n",
    "    # Печать на экран лучшего алгоритам\n",
    "    if(generation == numberOfGenerations):\n",
    "        gboost.printBestAlgorithm(bestParents[0],\n",
    "                                scorePopulation['Gender ' + str(generation)][bestParents[0]],\n",
    "                                metricList, metric,\n",
    "                                list(sorted(hyperparameterSettings.keys())),\n",
    "                                population['Gender ' + str(generation)][bestParents[0]])\n",
    "\n",
    "\n",
    "print('\\n')\n",
    "time_genetic_cb = (time.time() - start) / 60.0\n",
    "print(f\"Time = {time_genetic_cb}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравнение работы алгоритмов (время работы при новом запуске может отличаться):\n",
    "\n",
    "|                                                  |**XGBoost**           |**LightGBM**                 |**Catboost**    |\n",
    "|:------------------------------------------------:|:--------------------:|:---------------------------:|:--------------:|\n",
    "|*ROC AUC Score Test*                              |$0.8772$              |$0.8765$                     |$0.8760$        |\n",
    "|*ROC AUC Score Train*                             |$0.8626$              |$0.8628$                     |$0.8626$        |\n",
    "|*Время работы*                                    |$0.87\\text{ min}$     |$0.69\\text{ min}$            |$4.2\\text{ min}$| \n",
    "|*Количество деревьев*                             |$45$                  |$105$                        |$115$           |\n",
    "|*Максимальная глубина деревьев*                   |$5$                   |$4$                          |$4$             |\n",
    "|*Шаг (скорость) обучения*                         |$0.183$               |$0.109$                      |$0.255$         |\n",
    "|*Доля объектов для обучения одного дерева*        |$0.8$                 |$0.6$                        |                |\n",
    "|*Доля признаков для обучения одного дерева*       |$0.4$                 |$0.4$                        |$0.5$           |\n",
    "|*L2 коэффициент регуляризации*                    |$0.05$                |$0.05$                       |$0.25$          |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 10. Сравнение алгоритмов поиска гиперпараметров"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравним методы поиска гиперпараметров на примере алгоритма **XGBoost**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 3240 candidates, totalling 6480 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:    4.5s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:   35.0s\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  1.5min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  2.7min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  4.2min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed:  6.2min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed:  8.7min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed: 12.0min\n",
      "[Parallel(n_jobs=-1)]: Done 4034 tasks      | elapsed: 15.9min\n",
      "[Parallel(n_jobs=-1)]: Done 4984 tasks      | elapsed: 20.7min\n",
      "[Parallel(n_jobs=-1)]: Done 6034 tasks      | elapsed: 26.5min\n",
      "[Parallel(n_jobs=-1)]: Done 6480 out of 6480 | elapsed: 28.9min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time = 28.93917124668757\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [20, 30, 50, 70, 100],\n",
    "    'max_depth': [5, 6, 7, 8],    \n",
    "    'learning_rate': [0.001, 0.005, 0.01, 0.05, 0.1, 0.2], \n",
    "    'subsample': [0.5, 0.8, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.8, 1.0],\n",
    "    'reg_lambda': [0.05, 0.1, 0.3],\n",
    "    'objective': ['binary:logistic'],\n",
    "}\n",
    "\n",
    "xgb_gs = GridSearchCV(xgb.XGBClassifier(random_state = 123), param_grid, n_jobs = -1, \n",
    "                   scoring = 'roc_auc', cv = 2, verbose = 1) \n",
    "xgb_gs.fit(X_train, Y_train)\n",
    "\n",
    "print(f\"Time = {(time.time() - start) / 60.0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.5,\n",
       " 'learning_rate': 0.05,\n",
       " 'max_depth': 6,\n",
       " 'n_estimators': 100,\n",
       " 'objective': 'binary:logistic',\n",
       " 'reg_lambda': 0.05,\n",
       " 'subsample': 1.0}"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8797772275853837\n",
      "0.8623772178329381\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "print(roc_auc_score(Y_train, xgb_gs.best_estimator_.predict_proba(X_train)[:, 1]))\n",
    "print(roc_auc_score(Y_test, xgb_gs.best_estimator_.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RandomizedSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 2 folds for each of 2000 candidates, totalling 4000 fits\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=-1)]: Using backend LokyBackend with 8 concurrent workers.\n",
      "[Parallel(n_jobs=-1)]: Done  34 tasks      | elapsed:   12.8s\n",
      "[Parallel(n_jobs=-1)]: Done 184 tasks      | elapsed:  1.2min\n",
      "[Parallel(n_jobs=-1)]: Done 434 tasks      | elapsed:  3.0min\n",
      "[Parallel(n_jobs=-1)]: Done 784 tasks      | elapsed:  5.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1234 tasks      | elapsed:  8.4min\n",
      "[Parallel(n_jobs=-1)]: Done 1784 tasks      | elapsed: 12.0min\n",
      "[Parallel(n_jobs=-1)]: Done 2434 tasks      | elapsed: 16.4min\n",
      "[Parallel(n_jobs=-1)]: Done 3184 tasks      | elapsed: 21.7min\n",
      "[Parallel(n_jobs=-1)]: Done 4000 out of 4000 | elapsed: 27.4min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time = 27.365946638584138\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "param_grid = {\n",
    "    'n_estimators': [20, 30, 50, 70, 85, 100, 125, 150],\n",
    "    'max_depth': [5, 6, 7, 8, 9],    \n",
    "    'learning_rate': [0.001, 0.005, 0.01, 0.05, 0.1, 0.2, 0.25, 0.3], \n",
    "    'subsample': [0.5, 0.7, 0.8, 0.9, 1.0],\n",
    "    'colsample_bytree': [0.5, 0.7, 0.8, 0.9, 1.0],\n",
    "    'reg_lambda': [0.05, 0.1, 0.2, 0.3, 0.4, 0.5],\n",
    "    'objective': ['binary:logistic'],\n",
    "}\n",
    "\n",
    "xgb_rs = RandomizedSearchCV(xgb.XGBClassifier(random_state = 23), param_grid, n_jobs = -1, \n",
    "                         scoring = 'roc_auc', cv = 2, verbose = 1, n_iter = 2000, random_state = 54) \n",
    "xgb_rs.fit(X_train, Y_train)\n",
    "\n",
    "print(f\"Time = {(time.time() - start) / 60.0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'subsample': 0.9,\n",
       " 'reg_lambda': 0.4,\n",
       " 'objective': 'binary:logistic',\n",
       " 'n_estimators': 50,\n",
       " 'max_depth': 6,\n",
       " 'learning_rate': 0.1,\n",
       " 'colsample_bytree': 0.5}"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xgb_rs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8792911680054667\n",
      "0.8622130777765075\n"
     ]
    }
   ],
   "source": [
    "print(roc_auc_score(Y_train, xgb_rs.best_estimator_.predict_proba(X_train)[:, 1]))\n",
    "print(roc_auc_score(Y_test, xgb_rs.best_estimator_.predict_proba(X_test)[:, 1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "HyperOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hyperopt import hp, tpe\n",
    "from hyperopt.fmin import fmin\n",
    "from hyperopt import Trials\n",
    "\n",
    "def hyperopt_xgb_score(params):\n",
    "    \n",
    "    model = xgb.XGBClassifier(n_jobs = -1)\n",
    "    model.set_params(**params, random_state = 121)\n",
    "    model.fit(X_train, Y_train)\n",
    "    \n",
    "    roc_test = roc_auc_score(Y_test, model.predict_proba(X_test)[:, 1])\n",
    "    \n",
    "    return -roc_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "  0%|                                                                           | 0/20 [00:00<?, ?trial/s, best loss=?]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "job exception: value -2 for Parameter max_depth should be greater equal to 0\n",
      "max_depth: Maximum depth of the tree; 0 indicates no limit; a limit is required for depthwise policy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  0%|                                                                           | 0/20 [00:00<?, ?trial/s, best loss=?]\n"
     ]
    },
    {
     "ename": "XGBoostError",
     "evalue": "value -2 for Parameter max_depth should be greater equal to 0\nmax_depth: Maximum depth of the tree; 0 indicates no limit; a limit is required for depthwise policy",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mXGBoostError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-64-54d63a198dbe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     19\u001b[0m best = fmin(fn = hyperopt_xgb_score, space = space_xgb2, algo = tpe.suggest, trials = tpe_trials,\n\u001b[1;32m---> 20\u001b[1;33m             max_evals = 20, show_progressbar = True)\n\u001b[0m\u001b[0;32m     21\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     22\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    520\u001b[0m             \u001b[0mshow_progressbar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    521\u001b[0m             \u001b[0mearly_stop_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mearly_stop_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 522\u001b[1;33m             \u001b[0mtrials_save_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrials_save_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    523\u001b[0m         )\n\u001b[0;32m    524\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(self, fn, space, algo, max_evals, timeout, loss_threshold, max_queue_len, rstate, verbose, pass_expr_memo_ctrl, catch_eval_exceptions, return_argmin, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    697\u001b[0m             \u001b[0mshow_progressbar\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mshow_progressbar\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    698\u001b[0m             \u001b[0mearly_stop_fn\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mearly_stop_fn\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 699\u001b[1;33m             \u001b[0mtrials_save_file\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrials_save_file\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    700\u001b[0m         )\n\u001b[0;32m    701\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mfmin\u001b[1;34m(fn, space, algo, max_evals, timeout, loss_threshold, trials, rstate, allow_trials_fmin, pass_expr_memo_ctrl, catch_eval_exceptions, verbose, return_argmin, points_to_evaluate, max_queue_len, show_progressbar, early_stop_fn, trials_save_file)\u001b[0m\n\u001b[0;32m    551\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    552\u001b[0m     \u001b[1;31m# next line is where the fmin is actually executed\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 553\u001b[1;33m     \u001b[0mrval\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    554\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    555\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mreturn_argmin\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mexhaust\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    354\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mexhaust\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m         \u001b[0mn_done\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 356\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_evals\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mn_done\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mblock_until_done\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0masynchronous\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    357\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    358\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mrun\u001b[1;34m(self, N, block_until_done)\u001b[0m\n\u001b[0;32m    290\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    291\u001b[0m                     \u001b[1;31m# -- loop over trials and do the jobs directly\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 292\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mserial_evaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    293\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    294\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrefresh\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperopt\\fmin.py\u001b[0m in \u001b[0;36mserial_evaluate\u001b[1;34m(self, N)\u001b[0m\n\u001b[0;32m    168\u001b[0m                 \u001b[0mctrl\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mbase\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mCtrl\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtrials\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcurrent_trial\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrial\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    169\u001b[0m                 \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 170\u001b[1;33m                     \u001b[0mresult\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdomain\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mspec\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mctrl\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    171\u001b[0m                 \u001b[1;32mexcept\u001b[0m \u001b[0mException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    172\u001b[0m                     \u001b[0mlogger\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0merror\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"job exception: %s\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0me\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\hyperopt\\base.py\u001b[0m in \u001b[0;36mevaluate\u001b[1;34m(self, config, ctrl, attach_attachments)\u001b[0m\n\u001b[0;32m    905\u001b[0m                 \u001b[0mprint_node_on_error\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mrec_eval_print_node_on_error\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    906\u001b[0m             )\n\u001b[1;32m--> 907\u001b[1;33m             \u001b[0mrval\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpyll_rval\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    908\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    909\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrval\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mfloat\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mint\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mnumber\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-49-c02af2c9d729>\u001b[0m in \u001b[0;36mhyperopt_xgb_score\u001b[1;34m(params)\u001b[0m\n\u001b[0;32m      7\u001b[0m     \u001b[0mmodel\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_jobs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_params\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m**\u001b[0m\u001b[0mparams\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrandom_state\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;36m121\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 9\u001b[1;33m     \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     10\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     11\u001b[0m     \u001b[0mroc_test\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mroc_auc_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_test\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_test\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\xgboost\\sklearn.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, sample_weight, base_margin, eval_set, eval_metric, early_stopping_rounds, verbose, xgb_model, sample_weight_eval_set, callbacks)\u001b[0m\n\u001b[0;32m    822\u001b[0m                               \u001b[0mevals_result\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals_result\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    823\u001b[0m                               \u001b[0mverbose_eval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mxgb_model\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mxgb_model\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 824\u001b[1;33m                               callbacks=callbacks)\n\u001b[0m\u001b[0;32m    825\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    826\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mobjective\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mxgb_options\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"objective\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36mtrain\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m    210\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    211\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 212\u001b[1;33m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[0;32m    213\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    214\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\xgboost\\training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[1;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[0;32m     73\u001b[0m         \u001b[1;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     74\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m2\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 75\u001b[1;33m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mi\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     76\u001b[0m             \u001b[0mbst\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     77\u001b[0m             \u001b[0mversion\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36mupdate\u001b[1;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[0;32m   1367\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle,\n\u001b[0;32m   1368\u001b[0m                                                     \u001b[0mctypes\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mc_int\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0miteration\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1369\u001b[1;33m                                                     dtrain.handle))\n\u001b[0m\u001b[0;32m   1370\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1371\u001b[0m             \u001b[0mpred\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0moutput_margin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtraining\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\Anaconda2\\envs\\py36\\lib\\site-packages\\xgboost\\core.py\u001b[0m in \u001b[0;36m_check_call\u001b[1;34m(ret)\u001b[0m\n\u001b[0;32m    188\u001b[0m     \"\"\"\n\u001b[0;32m    189\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mret\u001b[0m \u001b[1;33m!=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 190\u001b[1;33m         \u001b[1;32mraise\u001b[0m \u001b[0mXGBoostError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpy_str\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0m_LIB\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mXGBGetLastError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    191\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    192\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mXGBoostError\u001b[0m: value -2 for Parameter max_depth should be greater equal to 0\nmax_depth: Maximum depth of the tree; 0 indicates no limit; a limit is required for depthwise policy"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "\n",
    "tpe_trials = Trials()\n",
    "\n",
    "space_xgb = {\n",
    "            'n_estimators': hp.choice('n_estimators', np.arange(10, 121, dtype = int)),\n",
    "            'max_depth':  hp.choice('max_depth', np.arange(4, 11, dtype = int)),\n",
    "            'learning_rate': hp.quniform('learning_rate', 0.01, 0.5, 0.01),\n",
    "            'subsample': hp.quniform('subsample', 0.3, 1.0, 0.025),\n",
    "            'colsample_bytree': hp.quniform('colsample_bytree', 0.3, 1.0, 0.025),\n",
    "            'reg_lambda': hp.quniform('reg_lambda', 0.01, 0.5, 0.01),\n",
    "            'objective': 'binary:logistic',\n",
    "            'tree_method': 'hist',\n",
    "            'eval_metric': 'auc'\n",
    "            }\n",
    "\n",
    "\n",
    "\n",
    "best = fmin(fn = hyperopt_xgb_score, space = space_xgb, algo = tpe.suggest, trials = tpe_trials,\n",
    "            max_evals = 20, rstate = np.random.RandomState(17), show_progressbar = True)\n",
    "print()\n",
    "print()\n",
    "print('best:')\n",
    "print(best)\n",
    "print(f\"Time = {(time.time() - start) / 60.0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[4, 1, 4, 6, 0, 5, 1, 5, 3, 2, 4, 1, 4, 0, 5, 0, 1, 0, 2, 5]"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tpe_trials.idxs_vals[1]['max_depth']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Сравнение работы алгоритмов: (время работы при новом запуске может отличаться)\n",
    "\n",
    "|                                            |**Genetic**         |**GridSearch**    |**RandomSearch** |**HyperOpt**     |\n",
    "|:------------------------------------------:|:------------------:|:----------------:|:---------------:|:---------------:|\n",
    "|*ROC AUC Score Train*                       |$0.8772$            |$0.8798$          |$0.8793$         |                 |\n",
    "|*ROC AUC Score Test*                        |$0.8626$            |$0.8624$          |$0.8622$         |$0.8627$         |\n",
    "|*Время работы*                              |$0.87\\text{ min}$   |$27.9\\text{ min}$ |$26.7\\text{ min}$|$1.3\\text{ min}$ | \n",
    "|*Количество деревьев*                       |$45$                |$100$             |$50$             |$84$             |\n",
    "|*Максимальная глубина деревьев*             |$5$                 |$6$               |$6$              |$2$              |\n",
    "|*Шаг (скорость) обучения*                   |$0.183$             |$0.05$            |$0.1$            |$0.194$          |\n",
    "|*Доля объектов для обучения одного дерева*  |$0.8$               |$1.0$             |$0.9$            |$0.75$           |\n",
    "|*Доля признаков для обучения одного дерева* |$0.4$               |$0.5$             |$0.5$            |$0.9$            |\n",
    "|*L2 коэффициент регуляризации*              |$0.05$              |$0.05$            |$0.4$            |$0.05$           |"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
